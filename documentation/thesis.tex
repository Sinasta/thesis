\documentclass[a4paper, 12pt]{report}
\usepackage[top=2.0cm, bottom=2.0cm, left=2.5cm, right=2.5cm]{geometry}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{tikz}
\usepackage{subcaption}
\usepackage[bottom]{footmisc}
\usepackage{listings}
\usepackage[hidelinks]{hyperref}
\usepackage{parskip}
\usepackage{nonumonpart}
\usepackage{float}
\usepackage{appendix}
\usepackage{colortbl}
\usepackage[acronym, toc]{glossaries}
\usepackage[style=authoryear-comp]{biblatex}
\usepackage{xpatch}

\addbibresource{bibliography.bib}
\xapptobibmacro{cite}{\setunit{\nametitledelim}\printfield{title}}{}{}
\setcounter{tocdepth}{3}
\setcounter{secnumdepth}{3}
\setlength{\skip\footins}{20pt}
\hbadness=99999
\hfuzz=\maxdimen
\hypersetup{pdfstartpage=1, pdfauthor={Raban Ohlhoff}, pdftitle={Topological Graphs in Architecture}, pdfsubject={Topological Machine Learning}, pdfkeywords={Architecture, Graph-Theory, Topology, Machine learning, Energy performance}, pdfproducer={Latex}, pdfcreator={PdfLatex}}
\DeclareCiteCommand{\citeauthor}
{\boolfalse{citetracker}%
\boolfalse{pagetracker}%
\usebibmacro{prenote}}
{\ifciteindex
{\indexnames{labelname}}
{}%
\printtext[bibhyperref]{\printnames{labelname}}}
{\multicitedelim}
{\usebibmacro{postnote}}

\title{\fontsize{65}{60}\selectfont TOPO-\\LOGICAL GRAPHS IN ARCHITEC-\\TURE\vspace{-25pt}}
\author{\fontsize{28}{13}\selectfont Raban Ohlhoff}
\date{\vspace{-15pt}2023}

\makeglossaries
\newglossaryentry{python}{name=python, description={High-level programming language with a focus on simplicity and readability}}
\newglossaryentry{origin bias}{name=origin bias, description={The existence of patterns based on the place or source of origin}}
\newglossaryentry{open source}{name=open source, description={Software or technology with a freely accessible source code}}
\newglossaryentry{knowledge graph}{name=knowledge graph, description={Graphical representation of semantic information about real-world data}}
\newglossaryentry{binary tree}{name=binary tree, description={Hierarchical data structure with at most two subelements per node}}
\newglossaryentry{euler characteristic}{name=euler characteristic, description={Numerical measurement of topological properties}}
\newglossaryentry{axial map}{name=axial map, description={Graphical representation of spatial relationships}}
\newglossaryentry{centrality}{name=centrality, description={Measure of influence of an entity in a graph structure}}
\newglossaryentry{node degree}{name=node degree, description={The amount of edges connected to a node in a graph structure}}
\newglossaryentry{stochastic}{name=stochastic, description={Randomly determined processes or algorithms}}
\newglossaryentry{deterministic}{name=deterministic, description={Fully predictable set of rules or algorithms}}
\newglossaryentry{non-euclidean}{name=non-euclidean, description={Not adhering to \textit{Euclidean geometry}}}
\newglossaryentry{cartesian coordinates}{name=cartesian coordinates, description={System for representing points in a two- or three-dimensional space by their orthogonal distance from the origin}}
\newglossaryentry{convolution}{name=convolution, description={Mathematical operation that creates a single function based on the combination of two input functions}}
\newglossaryentry{continuous function}{name=continuous function, description={Function without sudden changes which is defined for all given points}}
\newglossaryentry{homotopy}{name=homotopy, description={Continuous transformation of two functions or topologically defined elements}}
\newglossaryentry{cohomology}{name=cohomology, description={Operation in algebraic topology which measures the discontinuity or higher-dimensional characteristics of an entity}}
\newglossaryentry{manifold}{name=manifold, description={Topological space that can be represented through \textit{Euclidean geometry} and thus can be described by \gls{cartesian coordinates}}}
\newglossaryentry{cellcomplex}{name=cellcomplex, description={Three dimensional entity describing a topological space composed by multiple cell objects}}
\newglossaryentry{boolean operation}{name=boolean operation, description={Mathematical concept that combines geometric or topological elements using set operations such as union, difference or intersection}}
\newglossaryentry{convex}{name=convex, description={Shape or set that remains above any line connecting its points}}
\newglossaryentry{greedy algorithm}{name=greedy algorithm, description={Algorithmic method that makes local choices at each iteration in order to find an optimal solution on a general level}}
\newglossaryentry{fitness function}{name=fitness function, description={Function for evaluating the performance of a solution in optimisation algorithms}}
\newglossaryentry{probability model}{name=probability model, description={Mathematical entity used to predict the likelihood of different events}}
\newglossaryentry{likelihood function}{name=likelihood function, description={Function which measures the fitness between input data and a statistical model}}
\newglossaryentry{quantile}{name=quantile, description={Statistical operation that divides a probability distribution into specified intervals thus identifying probability thresholds}}
\newglossaryentry{centroid}{name=centroid, description={Geometric center of a shape or point set}}
\newglossaryentry{lloyd algorithm}{name=lloyd algorithm, description={Iterative method based on Voronoi diagram computation, used to regularise the calculated Voronoi regions}}
\newglossaryentry{downhill simplex algorithm}{name=downhill simplex algorithm, description={Optimisation algorithm that iteratively computes a geometric shape in order to find the optimal solution}}
\newglossaryentry{degree of compactness}{name=degree of compactness, description={Measure quantifying the degree to which an object deviates from a perfectly circular shape}}
\newglossaryentry{aperture}{name=aperture, description={An opening, such as a window or door, which connects two mediums with each other}}
\newglossaryentry{glazing percentage}{name=glazing percentage, description={Ratio of the window area to the total facade area of an architectural object}}
\newglossaryentry{dictionary}{name=dictionary, description={Data structure in \Gls{python} that stores pairs of keys with their corresponding values}}
\newglossaryentry{site energy consumption}{name=site energy consumption, description={Total amount of energy consumed by a building at its location, thus excluding the consumption for excavation, transportation and material fabrication}}
\newglossaryentry{activation}{name=activation, description={Function that determines the output of an ANN layer based on the input combined with their corresponding weights}}
\newglossaryentry{pooling}{name=pooling, description={Operation in \acrshort{anns} that reduces the dimensionality of the input features through calculation of maximum or average values}}
\newglossaryentry{sampling}{name=sampling, description={Process of selecting a specific amount of datapoints from a dataset for analysis or model training purposes}}
\newglossaryentry{holdout}{name=holdout, description={Machine learning technique where a percentage of the data is used only for evaluating the model's performance rather then for training}}
\newglossaryentry{validation}{name=validation, description={Process of evaluating a model on a separate dataset in order to assess its accuracy}}
\newglossaryentry{subset random sampler}{name=subset random sampler, description={Method that randomly selects a set of datapoints from a dataset}}
\newglossaryentry{negative log-likelihood}{name=negative log-likelihood, description={\Gls{loss} function that measures the distance between prediction and actual values by computing the negative logarithm of the \textit{likelihood function}}}
\newglossaryentry{cross-entropy}{name=cross-entropy, description={\Gls{loss} function that measures the deviation between predictions and actual values}}
\newglossaryentry{loss}{name=loss, description={Measure of deviation between prediction and actual values in the context of machine learning}}
\newglossaryentry{epoch}{name=epoch, description={Amount of times a machine learning model passes through the entire dataset during the training process}}
\newglossaryentry{learning rate}{name=learning rate, description={\textit{Hyperparameter} that defines the step size and pace during deep learning processes}}
\newglossaryentry{bayesian optimisation}{name=bayesian optimisation, description={Technique for finding the global optimum of a function through construction of a probabilistic model}}
\newglossaryentry{precision}{name=precision, description={Proportion of \acrshort{tp} classes among all positive predicted classes}}
\newglossaryentry{recall}{name=recall, description={Proportion of correctly predicted positive classes among all \acrshort{tp} classes}}
\newglossaryentry{f1-score}{name=f1-score, description={Combination of \gls{precision} and \gls{recall} metrics into a single value}}
\newglossaryentry{10-fold cross-validation}{name=10-fold cross-validation, description={Evaluation metric of a machine learning model through division of the training data into 10 subsets, training on 9 of them and testing on the remaining one. After repetition of the process, the final performance is the average of all 10 performance values}}
\newglossaryentry{overfitting}{name=overfitting, description={When a model performs well on the training data but fails to generalise to unseen data}}
\newglossaryentry{underfitting}{name=underfitting, description={When a model lacks the capacity to capture the patterns in the data}}
\newglossaryentry{ensemble learning method}{name=ensemble learning method, description={Machine learning technique which combines predictions from multiple individual models for the sake of performance or generalisation}}
\newglossaryentry{decision tree}{name=decision tree, description={Model used to classify or predict based on binary decisions}}
\newglossaryentry{standard deviation}{name=standard deviation, description={Measure of the distance from the mean}}
\newglossaryentry{one-hot-encoded}{name=one-hot-encoded, description={Feature encoding technique where each individual category is converted into a list of zeros with only a one in the category's position}}
\newglossaryentry{isovist}{name=isovist, description={\textit{Space Syntax} concept that represents the visible area from a specific point in space}}

\newacronym{ifc}{IFC}{Industry Foundation Classes}
\newacronym{bim}{BIM}{Building Information Model}
\newacronym{cad}{CAD}{Computer Aided Design}
\newacronym{aec}{AEC}{Architectural Engineering and Construction}
\newacronym{mst}{MST}{Minimum spanning tree}
\newacronym{gnns}{GNNs}{Graph Neural Networks}
\newacronym{gcns}{GCNs}{Graph Convolutional Networks}
\newacronym{gats}{GATs}{Graph Attention Networks}
\newacronym{gaes}{GAEs}{Graph Autoencoder}
\newacronym{grnns}{GRNNs}{Graph Recurrent Neural Networks}
\newacronym{rnns}{RNNs}{Recurrent Neural Networks}
\newacronym{api}{API}{Application Programming Interface}
\newacronym{df}{DF}{Daylight Factor}
\newacronym{udi}{UDI}{Useful Daylight Illuminance}
\newacronym{da}{DA}{Daylight Autonomy}
\newacronym{gf}{GF}{Glare Factor}
\newacronym{json}{JSON}{JavaScript Object Notation}
\newacronym{brep}{Brep}{Boundary Representation}
\newacronym{ashrae}{ASHRAE}{American Society of Heating, Refrigerating and Air-Conditioning Engineers}
\newacronym{osm}{OSM}{OpenStudio Model}
\newacronym{sql}{SQL}{Structured Query Language}
\newacronym{dgl}{DGL}{Deep Graph Library}
\newacronym{dgcnn}{DGCNN}{Deep Graph Convolutional Neural Network}
\newacronym{tp}{TP}{True Positive}
\newacronym{fp}{FP}{False Positive}
\newacronym{fn}{FN}{False Negative}
\newacronym{mse}{MSE}{Mean Squared Error}
\newacronym{rmse}{RMSE}{Root Mean Squared Error}
\newacronym{mae}{MAE}{Mean Absolute Error}
\newacronym{mlp}{MLP}{Multi-layer Perceptron}
\newacronym{epw}{EPW}{EnergyPlus Weather Format}
\newacronym{src}{SRC}{source}
\newacronym{dst}{DST}{destination}
\newacronym{id}{ID}{identification}
\newacronym{bem}{BEM}{Building Energy Modelling}
\newacronym{oss}{OSS}{open source software}
\newacronym{osi}{OSI}{Open Source Initiative}
\newacronym{dxf}{DXF}{Drawing Interchange Format}
\newacronym{fea}{FEA}{Finite Element Analysis}
\newacronym{cfd}{CFD}{Computational Fluid Dynamics}
\newacronym{lca}{LCA}{Life Cycle Assessment}
\newacronym{hvac}{HVAC}{Heating, Ventilation and Air Conditioning}
\newacronym{nrel}{NREL}{National Renewable Energy Laboratory}
\newacronym{gis}{GIS}{Geographic Information System}
\newacronym{iot}{IoT}{Internet of Things}
\newacronym{vga}{VGA}{Visibility Graph Analysis}
\newacronym{gans}{GANs}{Generative Adversarial Networks}
\newacronym{anns}{ANNs}{Artificial Neural Networks}

\begin{document}

\pagenumbering{gobble}

\AddToHookNext{shipout/background}{\put (-8pt,-\paperheight-4pt){
\begin{tikzpicture}\node(a){\includegraphics[scale=1]{assets/cover/cover.pdf}};
\node at(a.center)[draw, fill=white,line width=1.5pt,circle, minimum height=450pt, yshift=25pt]{};
\end{tikzpicture}}}

\maketitle
\clearpage
\vspace*{\fill}
August 2023\\
Faculty of Architecture La Cambre Horta\\
Université Libre de Bruxelles\\\\
Thesis Supervisor : David Erkan\\
Second Supervisor : Gian Marco Paldino\\
Internal Jury Member : Gianluca Bontempi
\clearpage

\tableofcontents

\clearpage
\vspace*{\fill}
\textit{It is the fact of space that creates the special relation between function and social meaning in buildings. The ordering of space in buildings is really about the ordering of relations between people. Because this is so, society enters into the very nature and form of buildings. They are social objects through their very form as objects. Architecture is not a 'social art' simply because buildings are important visual symbols of society, but also because, through the ways in which buildings, individually and collectively, create and order space, we are able to recognise society: that it exists and has a certain form.} --- \cite{hillier1989social}
\vspace*{\fill}
\clearpage

\pagenumbering{roman}
\newcounter{abstractpage}
\setcounter{abstractpage}{\value{page}}
\begin{abstract}
\thispagestyle{plain}
\setcounter{page}{\value{abstractpage}}
\phantomsection
\addcontentsline{toc}{chapter}{\abstractname}

Traditional architectural practices often rely on creative intuition and experience rather than systematic analysis and data-driven decision making. With the increasing availability of computational tools and data science techniques, there is an opportunity to bring mathematical and computer science concepts and methods into the architectural context to challenge traditional practices and offer potential improvements. This thesis explores the application of graph theoretical and topological concepts in architecture and investigates the use of graph machine learning methods in the context of architectural analysis, with a particular focus on energy efficiency as a key performance metric. To this end, a synthetic architectural dataset containing geometric, categorical, dimensional, energetic, topological and relational information is generated by integrating various space partitioning algorithms combined with architectural control functions into an automated generation pipeline. Subsequently, a classification model and a regression model are trained on the generated \gls{knowledge graph} dataset to evaluate the prediction and classification accuracy in terms of energy efficiency. The resulting dataset and the code for generating and training the model will be made publicly available to further research in the field of graph machine learning in architectural applications. This research demonstrates the potential of a closer integration of various mathematical concepts and computer science methods into the architectural design and verification process, and shows the potential of applying \glspl{knowledge graph} for the abstraction, representation and analysis of architectural objects.

\small\textbf{\textit{Keywords ---}} Architecture, Graph Theory, Topology, Machine learning, Simulation

\setcounter{abstractpage}{\value{page}}
\end{abstract}\renewcommand{\abstractname}{Resumé}
\setcounter{page}{\value{abstractpage}}
\stepcounter{page}

\setcounter{abstractpage}{\value{page}}
\begin{abstract}
\thispagestyle{plain}
\setcounter{page}{\value{abstractpage}}

Les pratiques architecturales traditionnelles s'appuient souvent sur l'intuition créative et l'expérience plutôt que sur l'analyse systématique et la prise de décision fondée sur des données. Avec la disponibilité croissante d'outils informatiques et de techniques de science des données, il est possible d'introduire des concepts et des méthodes mathématiques et informatiques dans le contexte de l'architecture afin de remettre en question les pratiques traditionnelles et d'offrir des améliorations potentielles. Cette thèse explore l'application de la théorie des graphes et des concepts topologiques à l'architecture et étudie l'utilisation des méthodes d'apprentissage automatique des graphes dans le contexte de l'analyse architecturale, avec un accent particulier sur l'efficacité énergétique en tant que mesure clé de la performance. À cette fin, un ensemble de données architecturales synthétiques contenant des informations géométriques, catégorielles, dimensionnelles, énergétiques, topologiques et relationnelles est généré en intégrant divers algorithmes de partitionnement de l'espace combinés à des fonctions de contrôle architectural dans un système de génération automatisé. Ensuite, un modèle de classification et un modèle de régression sont entraînés sur l'ensemble de données graphique de connaissances généré afin d'évaluer la précision de la prédiction et de la classification en termes du rendement énergétique. L'ensemble de données résultant et le code de génération et d'entraînement du modèle seront mis à la disposition du public pour faire avancer la recherche dans le domaine de l'apprentissage automatique des graphes dans les applications architecturales. Cette recherche démontre le potentiel d'une intégration plus étroite de divers concepts mathématiques et de méthodes informatiques dans le processus de conception et de vérification architecturales, et montre le potentiel de l'application de graphes de connaissances pour l'abstraction, la représentation et l'analyse d'objets architecturaux.

\small\textbf{\textit{Mots-clés ---}} Architecture, Théorie des graphes, Topologie, Apprentissage automatique, Simulation

\setcounter{abstractpage}{\value{page}}
\end{abstract}
\setcounter{page}{\value{abstractpage}}
\stepcounter{page}

\clearpage
\vspace*{\fill}
\phantomsection
\addcontentsline{toc}{chapter}{Acknowledgements}
I would like to express my sincere gratitude to all those who have contributed to the completion of this thesis.\\

First and foremost, I would like to thank my supervisor, David Erkan, for providing invaluable advice, support and feedback throughout the research process. Your expertise, patience and willingness to challenge my ideas have been instrumental in the creation of this thesis.\\

I would also like to give special thanks to Gian Marco Paldino, Professor Wassim Jabi, Professor Gianluca Bontempi and all the participants who generously gave their time, insights and experience to this thesis. Their contributions have helped me to achieve my research goals.\\

I am grateful to my family and friends for their unwavering encouragement, understanding and motivation. Their love and support have carried me through the ups and downs of this journey.\\

Finally, I would like to acknowledge the contribution of this institution and the faculty members who have constantly challenged me throughout my academic journey. Thank you for providing me with the knowledge, skills and resources that have enabled me to undertake this research.\\

Without the support of these individuals and institutions, this thesis would not have been possible. Thank you.
\vspace*{\fill}
\clearpage

\cleardoublepage
\phantomsection
\addcontentsline{toc}{chapter}{\listfigurename}
\listoffigures

\cleardoublepage
\phantomsection
\addcontentsline{toc}{chapter}{\listtablename}
\listoftables

\printglossary
\printglossary[type=\acronymtype]
\cleardoublepage
\pagenumbering{arabic}

\AddToHookNext{shipout/background}{\put (-8pt,-\paperheight-4pt){
\begin{tikzpicture}\node(a){\includegraphics[scale=1]{assets/cover/overview_cover_1.pdf}};
\node at(a.center)[draw, fill=white,line width=1.2pt,circle, minimum height=200pt, yshift=80pt]{};
\end{tikzpicture}}}

\part{Overview}\label{part:overview}

\newpage \ \thispagestyle{empty}
\AddToHookNext{shipout/background}{\put (0,-\paperheight){\includegraphics[scale=1]{assets/cover/overview_cover_2.pdf}}}
\newpage\clearpage

\chapter{Introduction}\label{chap:introduction}

\section{Digitalisation in Architecture}\label{sec:digitalisation-in-architecture}

In recent decades, advances in technology and digitalisation have led to significant changes in a wide range of professions. Creative fields such as art and design have benefited significantly from the application and integration of various computer- or algorithm-based methods. Architecture, as a discipline at the intersection of technology and artistic practice, plays an interesting role in the adaptation of digital processes\footcite{chaillou2022artificial}. Whereas a few decades ago the profession of architecture was characterised by pencil and paper, today the mouse and computer screen constitute the principal working tools in the architectural design process.

This is only the visible surface of the digitisation of the architectural profession, as the entire working process of architects has been automated, optimised and structured by computer-based tools. With the development of computer-aided design methods such as \acrshort{bim} or more broadly \acrshort{cad}, the static, inflexible nature of representation methods in architecture has been remedied, opening up a wide range of new possibilities in all design phases. Today, a complete design process in a multi-dimensional environment is the norm, allowing the simulation, integration and manipulation of three-dimensional models in conjunction with information on materials, dimensions, construction details and many more. A constant back and forth between conceptual design decisions and construction details is thus largely seamless and is evident in many contemporary designs\footcite{pena2021artificial} based on the fusion of detail and creative or programmatic intent.

If we look at the history of architecture, it becomes clear how closely mathematics and architecture are intertwined, for example in the programmatic conception of projects or in the application of geometric rules at the plan level. However, it also becomes apparent that this interplay usually manifests itself at a highly theoretical level\footcite{baglivo1983incidence} and thus often has little tangible impact on the habitability of architectural objects, which can be explained by the lack of accessibility of fundamental mathematical concepts for architects. Therefore, in the design reality, there is unfortunately still a certain distance between useful mathematical concepts and their concrete, beneficial application in the conception phase. Concepts such as topology and graph theory are almost exclusively found at a very scientific level in academia and are therefore difficult to access for traditional or less privileged offices with limited scientific resources.

The application of computational scientific methods\footcite{caetano2020computational}, on the other hand, appears to have become more democratic with the widespread adoption of market-leading architectural software and the availability of third-party extensions. However, only a minority of traditional architectural practices have a clear overview of machine learning-based tools, and even fewer know how they work or how to make sense of them. In addition, such tools are often embedded in specific software, making interoperability difficult and thus becoming a software-specific feature. It is precisely this lack of clarity about the tools supported by artificial intelligence that hinders the general adaptation of such useful functionalities and contributes to the inequality between underprivileged architecture firms and the monopoly position of dominant proprietary architecture software. One possible reason for this situation is the need for essential resources for the development of such machine learning based models, such as time, economic resources such as budget, energy and computing power, but also access to adapted, evaluated and diverse training datasets.

\section{Current Challenges}\label{sec:current-challenges}

\paragraph{Need for a Universal Space Syntax Language}

The lack of a universal spatial syntax language in architectural design hinders the development of standardised guidelines and theoretical foundations. This results in arbitrary topological relationships between individual architectural elements, which can lead to inconsistencies and inefficiencies in design. A universal language would allow for better communication and understanding within the field and ultimately contribute to a more effective design practice. Furthermore, the skillful application of spatial syntax theories during the design process would lead to an increase in architectural quality during the occupancy of the constructed objects.

\paragraph{Limited Design Feedback During Early Project Stages}

Current design practices often lack feedback in the early stages\footcite{paterson2013real}, leading to arbitrary decisions and potential inconsistencies between initial drafts and detailed elaborations. Implementing a system that provides feedback throughout the design process on various parameters, such as performance or architectural feasibility, would allow architects to make more informed decisions and balance the creative and technical aspects of their projects.

\paragraph{Special Training and Computationally Intensive Simulations}

The setup and execution of complex physical simulations and intelligent \acrshort{cad} tools can be demanding and laborious, requiring special training and significant computational power\footcite{chatzivasileiadi2018effect}. This makes it difficult for non-specialist users to obtain accurate results and prevents continuous verification during the design process.

\paragraph{Need for Mathematical Foundations in Relationship Descriptions}

Understanding the spatial structuring and programmatic aspects of architecture requires research into the mathematical foundations of relational descriptions. By developing analysis methods based on such topological properties that can accurately represent and concretise programmatic intentions, architects can more effectively optimise spatial organisation, energy efficiency, natural lighting and load-bearing capacity.

\paragraph{Lack of Publicly Available Graph Datasets}

There is currently a significant gap in the availability of comprehensive graph datasets based on architectural objects\footcite{alymani2022graph}. This poses a challenge to researchers who need reliable data for algorithm development and testing. The creation and publication of such datasets would enable the academic community to make more substantial advances in the field and ultimately improve the traditional architecture practice.

\paragraph{Poor Application of Topological Concepts}

Conventional architectural projects often struggle to apply topological concepts effectively, making it difficult to automatically integrate and query rule-based information in \textit{Building Information Modelling}. Addressing this issue would help to ensure a coherent spatial design and improve overall project outcomes.

\paragraph{Data Form-Specific Challenges}

Architectural objects can be represented through various media, but reducing them to a single one often leads to a loss of information. Developing machine learning models that can efficiently manage diverse datasets and account for \textit{\gls{origin bias}} in architectural training data would help to ensure more meaningful and context relevant suggestions in the design processes.

\section{Contribution}\label{sec:contribution}

\paragraph{Feedback Loop in Early Design Stages}

This research contributes to the field by exploring methods and ways to implement feedback loops in the initial design stages. These feedback loops would allow architects to take decisions based on key parameters such as energy consumption, ultimately leading to more efficient and sustainable building designs and avoiding costly back and forth between detail drawing and conceptual design.

\paragraph{Exploration of Relational Datasets and Machine-Assisted Optimisation}

The thesis investigates the need for relational datasets in graphical form to enable machine-assisted optimisation of architectural parameters. It further explores the extraction of insights through analysis methods based on graph based input data, providing a foundation for further research and practical applications in this area.

\paragraph{Creation and Publication of an Architectural Graph Dataset}

To address the lack of publicly available graph datasets based on architectural objects, this work contributes by creating (chapter \ref{chap:synthetic-dataset-generation}) and publishing (section \ref{sec:summary-of-contribution}) a comprehensive graph dataset which can serve as a valuable resource for researchers and practitioners in architecture and related fields.

\paragraph{Meta-Analysis of Open Source Tools in Architecture}

Furthermore, this research includes a meta-analysis of \gls{open source} tools available to the architectural community (section \ref{sec:open-source-software-and-knowledge}). By investigating and promoting \gls{open source} file formats and software packages, the study aims to increase the interoperability of architectural data and encourage freedom in the \acrshort{aec} industry.

\paragraph{Application of Graph Theory in Architecture}

An essential part of this work focuses on the detailed analysis of possible applications of graph-theoretical concepts in the architectural context. For this purpose, the mathematical foundations are explained, graph-based analysis methods and their advantages are reviewed, and a concrete application of the theory is explored and evaluated in the experimental part of this thesis.

\paragraph{Analysis and Application of Topological Tools}

The thesis provides a thorough analysis of existing architectural applications of topological tools and their potential for graph-theoretic concepts. It explores the usefulness of topological transformations and analysis methods, ultimately demonstrating the value of an integration into the architectural practice.

\paragraph{Machine Learning Applications in Architecture}

The research further contributes by exploring the application of machine learning in the architectural profession. Specifically, graph machine learning methods are developed for predicting energy consumption and efficiency of architectural designs. This constitutes an essential step towards a closer connection between new technological, scientific methods and traditional architectural practices.

\paragraph{Generation of a Synthetic Architectural Dataset}

This work produces a synthetic architectural dataset, generated using parametric algorithms and respecting architectural rules while maintaining geometric variance. This dataset, published as an \gls{open source} resource, bridges the gap between different scientific fields, thus confirming the position of architecture as a polyvalent science. Furthermore, the documentation and publication of the generation pipeline is intended as a basis for subsequent work aiming at the generation of synthetic architectural data.

\section{Research Questions}\label{sec:research-questions}

Within the context described in the previous section, this manuscript identifies four main research questions related to graphs and topology, feedback in early design stages, architecture and machine learning and synthetic architectural datasets. By exploring these research questions, this manuscript aims to shed light on the potential benefits and challenges of integrating scientific and mathematical methods into architectural practice. These research topics are examined in detail to provide insights into how they can be applied to enhance creativity, coherence and efficiency in architectural design, as well as to explore new possibilities for the field of \acrshort{aec}. The questions are as follows:

\begin{itemize}

\item What is the role and potential benefits of integrating \textit{\glspl{knowledge graph}} and topological methods into everyday architectural practice? How can abstract relational information from graphs be meaningfully applied to project design, and what are the challenges and opportunities that arise from its use?

\item How can design feedback be provided in the early stages of the design process to enhance creativity and coherence between initial design and detailed elaboration in later project stages? What are the benefits and challenges of integrating indicative simulation variables in early design stages and what methods can be used to optimise this process?

\item What role can machine learning models play in architectural design and what are their current practical and academic research applications? What potential benefits can trained models provide for design feedback iteration, and how feasible is it to use annotated graphs as input to machine learning methods for the abstract representation of geometric information?

\item How can architectural datasets be created synthetically and to what extent can the information they contain be abstracted? What are the benefits, methods and potential issues associated with automatic floor plan generation and its application methods? Can synthetic dataset creation remedy \gls{origin bias} and how can the creative diversity and adaptability offered by automatic, regulated synthetic dataset creation be evaluated?

\end{itemize}

\section{Outline}\label{sec:outline}

This thesis is divided into two parts: Part \ref{part:overview}: \emph{Overview}, which deals with the explanation of the basic concepts as well as the state of the art, and part \ref{part:contribution}: \emph{Contribution}, which documents the experiments carried out along with their evaluation and conclusions. Finally, part \ref{part:appendix} of this thesis consists of the \emph{Appendix}, which contains further reading, additional information and code samples.

In \textbf{Chapter \ref{chap:introduction}: \emph{Introduction}}, the context for the study is set. Section \ref{sec:digitalisation-in-architecture}: \emph{Digitalisation in Architecture} discusses the increasing use of digital tools in everyday architectural practice, followed by an overview of the current challenges associated with the digitalisation process in section \ref{sec:current-challenges}. The contributions of this work are presented in section \ref{sec:contribution} and the research questions driving the study are outlined in section \ref{sec:research-questions}.

\textbf{Chapter \ref{chap:preliminaries}: \emph{Preliminaries}} delves into the basic concepts used throughout this study. Section \ref{sec:graph-theory} introduces the principal concepts of graph theory, and its applications in architecture are discussed in section \ref{subsec:graphs-in-architecture}. The topological analysis methods and tools used in this work are covered in section \ref{sec:topology}. The role of simulation in architectural design and the different possible applications are explored in section \ref{sec:simulation}, with particular emphasis on energy performance and optimisation methods in sections \ref{subsec:energy-performance} and \ref{subsec:optimisation}.

\textbf{Chapter \ref{chap:state-of-the-art}: \emph{State of the Art}} reviews the literature on graphs and topology in architecture in section \ref{sec:graphs-and-topology-in-architecture}, feedback in early design stages in section \ref{sec:feedback-in-early-design-stages}), architecture and machine learning in section \ref{sec:architecture-and-machine-learning}, and synthetic architecture datasets in section \ref{sec:synthetic-architecture-datasets}.

\textbf{Chapter \ref{chap:synthetic-dataset-generation}: \emph{Synthetic Dataset Generation}} details the process of generating a synthetic graph dataset based on architectural objects for subsequent use in this study. It includes discussions on different space partitioning algorithms in section \ref{sec:space-partitioning}, the integration into a parametric generation framework (section \ref{sec:parametric-framework}), the application of architectural rules (section \ref{sec:architectural-rules}) and post-processing techniques such as information annotation, energy performance simulation, data mapping and graph retrieval detailed in sections \ref{subsec:information-annotation} to \ref{subsec:graph-retrieval}. The chapter concludes with an evaluation of the results in section \ref{sec:outcome-synthetic-data-generation} and a discussion about synthetic data generation in section \ref{subsec:conclusion}.

\textbf{Chapter \ref{chap:graph-machine-learning}: \emph{Graph Machine Learning}} presents the structure and hyperparameters (sections \ref{sec:structure} and \ref{sec:hyperparameter}) of the proposed graph-based machine learning approach, as well as the evaluation metrics used for classification and regression tasks in sections \ref{subsec:classification-metrics} and \ref{subsec:regression-metrics}. This explanation is followed by the introduction of the comparative framework of both models in section \ref{subsec:comparison}. The results and comparison of the different methods are presented in section \ref{sec:outcome-machine-learning-for-energy-prediction}, where conclusions about the results of each model are drawn in sections \ref{subsubsec:conclusion-classification} and \ref{subsubsec:conclusion-regression}.

Finally, \textbf{Chapter \ref{chap:conclusion-and-future-perspectives}: \emph{Conclusion and Future Perspectives}} summarises the contributions and provides answers to the previously announced research questions in sections \ref{sec:summary-of-contribution} and \ref{sec:research-questions-conclusion}. This is followed by an outline of the learned lessons, future research directions, limitations, added value for architects and concluding remarks (sections \ref{sec:summary-of-contribution} to \ref{sec:concluding-remarks}). The manuscript concludes with a bibliography and two appendices: \ref{chap:additional-content} and \ref{chap:raw-data}, where the first part of the appendix provides additional content such as further literature, a proposal for a potential application of the experimental results, and a discussion on the topic of \gls{open source} in the \acrshort{aec} industry (sections \ref{sec:further-readings} to \ref{sec:open-source-software-and-knowledge}). The part \ref{chap:raw-data} of the appendix presents and explains the raw data, divided into geometrical, graphical and informational data in sections \ref{sec:geometry-data} to \ref{sec:information-data}.

\begin{figure}
\centering
\includegraphics[width=.98\textwidth]{assets/preliminaries/graph_theory/graphs/general.pdf}
\caption{Graph Structure}
\label{fig:graph_structure}
\end{figure}
			
\chapter{Preliminaries}\label{chap:preliminaries}

This chapter introduces the basics for understanding the essential concepts and terminology. First, a step-by-step description is given of what graph theory defines and what the properties and components of graph structures are. Then, the described concepts and characteristics are examined in their application in the architectural context, where the digital modelling process as such is considered, but also a variety of different graph-based analysis methods and their advantages are explained. Finally, in the context of graph theory, graph-based machine learning is introduced and its functionality, structure and application are explained.

The following section focuses on the introduction of the term \textit{topology} in a mathematical and architectural context. For this purpose, the topological analysis method is explained using the \textit{\Gls{python}} library \textit{TopologicPy} and its structure. Then the main concepts of \textit{space syntax} theory are recapitulated and illustrated with examples, followed by an explanation of the concept and use of \textit{shape grammar} implementations. This section concludes with a consideration of the basic concepts of different spatial partitioning methods with their respective advantages and disadvantages in the application of automatic floor plan generation.

The last section of the contribution chapter deals with the explanation of the concept of simulation and its different variants as well as possible applications. After a detailed description of the main simulation applications in architecture and engineering, the focus is on energy performance simulation. Finally, different optimisation families and their essential algorithms and functionalities are explained.

\section{Graph Theory}\label{sec:graph-theory}

Graph networks and their derivatives surround us in our everyday lives and influence us on a wide variety of levels. Yet this simple mathematical concept seems to be considered only rarely or at a highly academic level. When considering the fastest geographical route from point A to point B, or how many rooms in a Renaissance castle have to be traversed to get from the reception atrium to the noble chambers, these are graph-theoretical questions that can be solved through established mathematical methods.

Historically, the study of graph theory can be traced back to a problem known as the \textit{Königsberg Bridge} problem. It is an urban connection problem involving the city of \textit{Königsberg}, through which the river \textit{Pregel} flows (figure \ref{fig:königsberg-bridge-problem}). The city consisted of two main islands in the middle of the river and also extended on both sides of the riverbeds, being connected by exactly seven bridges as shown in figure \ref{fig:connection-graph}. The question that led to this famous problem\footcite{kantor2005tale} was whether it was feasible to cross each bridge exactly once and arrive back at the end of the path precisely at the starting point. On a mathematical level, it is a question of traversing the represented network consisting of points and their connections only once, but not repeatedly.

A closer look at this example reveals that the nodes represented are not defined by their geographical position, but only by their relationship or connection to the other points of the network (figure \ref{fig:simplified-representation}). This demonstrates one of the fundamental characteristics of graph structures.

\begin{figure}
\centering
\begin{subfigure}{.62\textwidth}
\centering
\includegraphics[width=\linewidth]{assets/preliminaries/graph_theory/graphs/koenigsberg.pdf}
\caption{Connection Graph}
\label{fig:connection-graph}
\end{subfigure}%
\begin{subfigure}{.38\textwidth}
\centering
\includegraphics[width=\linewidth]{assets/preliminaries/graph_theory/graphs/koenigsberg_graph.pdf}
\caption{Simplified Representation}
\label{fig:simplified-representation}
\end{subfigure}
\caption{Königsberg Bridge Problem}
\label{fig:königsberg-bridge-problem}
\end{figure}

Applications of graph theory can be found in a variety of ways in many fields, such as the analysis and representation of molecular structures in chemistry, where individual atoms represent the nodes and the edges represent the respective compounds to which the molecules owe, among other things, their chemical properties. Another well-known example is the representation of social networks, where the nodes represent single individuals or groups and the edges between them model their relationships. Areas such as transportation networks, traffic infrastructures, computer programs, data structures, financial markets, economic systems and ecological biospheres or phylogenetic trees are also well known and intuitive applications of practical graph theory.

\subsection{Graphs}\label{subsec:graphs}

But what makes a graph a graph? What is its basic structure and what are its properties? First of all, it is important to establish a general definition of a graph in order to explain and make intelligible its properties and functions in the following steps. A graph is a mathematical object from \textit{discrete mathematics} and \textit{combinatorics} which is composed of a finite, non-empty set of \textit{nodes}, also called \textit{vertices}, and a finite, unordered set of \textit{edges}. Vertices represent points in the graph structure which may be connected by edges.

\begin{figure}
\centering
\includegraphics[width=\textwidth]{assets/preliminaries/graph_theory/graphs/graph_object.pdf}
\caption{Graph Network}
\label{fig:graph-representation}
\end{figure}

Since graphs are dimensionally independent, a visualisation of their structure is often misleading, as edges may appear to intersect when in fact they exist completely independently, or vertices may be positioned locally adjacent to each other, but without having any commonality with each other. This abstraction and deceptive dimensionality becomes particularly important to internalise when graph structures are related to geometric objects. As can be seen in figure \ref{fig:graph-representation}, the complexity of the two-dimensional representation of graph structures increases significantly as soon as a certain number of nodes and edges is exceeded. Usually, vertices and edges are labelled with letters or numbers in order to refer to them individually.

In addition, edges can be \textit{unidirectional} or \textit{bidirectional}, meaning that there is a one-way or two-way relationship of the two connected vertices to each other. This is usually represented by arrows along or on the edges (figure \ref{fig:directed-graph}). Once a graph structure has one or more edges with a specific direction, it is called a \textit{directed graph}, which distinguishes it from an \textit{undirected graph}. Another special property of edges in a graph is that they can be weighted, which means that a hierarchy can be created between the edges and thus the designation of the graph changes from \textit{unweighted} to \textit{weighted graph} (figure \ref{fig:weighted-graphs}).

\begin{figure}
\centering
\includegraphics[width=.8\textwidth]{assets/preliminaries/graph_theory/graphs/directed_graph.pdf}
\caption{Directed Graph}
\label{fig:directed-graph}
\end{figure}

Regarding the naming convention\footcite{wilson1979introduction}, in mathematical terminology a graph is described by a $G$ and contains vertices $V(G)$ with their corresponding edge set $E(G)$, so $G = (V, E)$. An edge consists of a vertex pair $e = (u, v)$ and is usually abbreviated to $uv$. For undirected edges $e = (u, v)$ and $e = (v, u)$ holds, since an edge connecting vertex $u$ to vertex $v$ also connects vertex $v$ to $u$. The number of vertices in $G$ is notated as $|V|$ and the set of edges as $|E|$.

\subsubsection{Components}\label{subsec:components}

A graph consists of two basic components, which have already been introduced as nodes and edges. Nodes, or vertices, are the basic units of the graph and are usually labelled with a unique identifier, such as a letter or number. The total number of nodes $|V|$ in a graph is called the \textit{order of the graph} and is usually represented by the letter $n$.

Edges connect the nodes in the graph and provide information about which nodes are directly connected. An edge is therefore defined by an ordered pair of connected nodes and is often represented by the notation $(u, v)$, where $u$ and $v$ are the two nodes connected by the edge. If the graph is undirected, this means that the edge can be traversed in either direction and is often written as $(u, v)$ or $(v, u)$. If the graph is directed, the edge is represented by an arrow from $u$ to $v$ and is written as $(u, v)$. The total number of edges in a graph is called the \textit{size of the graph} and is usually represented by the letter $m$. For undirected graphs, the relation is \(m \leq n * (n - 1) / 2\), while for directed graphs it is \(m \leq n * (n - 1)\)\footcite{earl1979architectural}.

\begin{figure}
\centering
\begin{subfigure}{.5\textwidth}
\centering
\includegraphics[width=\linewidth]{assets/preliminaries/graph_theory/graphs/loop_graph.pdf}
\caption{Multigraph}
\label{fig:multigraph}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
\centering
\includegraphics[width=\linewidth]{assets/preliminaries/graph_theory/graphs/isolated_node.pdf}
\caption{Isolated Node}
\label{fig:isolated-node}
\end{subfigure}
\caption{Graph Variations}
\label{fig:graph-variations}
\end{figure}

There are several special cases for the two components of graph structures, such as \textit{isolated nodes} (figure \ref{fig:isolated-node}), which represent points that have no connection to other points of the graph and thus have no edges. A special case for edges are \textit{loops}, which have the same start and end vertex and thus connect a node to itself: $e = (u, u)$. Furthermore, two points $u$ and $v$ can be connected by multiple edges, which means that $E(G)$ contains either $(u, v)$ or $(v, u)$ more than once (figure \ref{fig:multigraph}). Multiple edges can occur in directed graphs, but also in undirected graphs.

As already described by the possibility of weighting edges, information can be added to the individual elements, since each element can be referenced precisely and independently. For example, nodes can contain the names of individuals in a social network, or edges can be classified into different categories, such as the type of relationship that different actors in a play have with each other, such as related, in love, married, and so on. Value notation is also possible, where individual nodes can be referenced by the age of the people, or edges can hold the number of months in the relationship. A common example of such graphical knowledge information is Zachary's karate club\footcite{sanchez2021gentle}, which serves as an example of how relational semantic information can be translated into graphical form.

\begin{figure}
\centering
\begin{subfigure}{.5\textwidth}
\centering
\includegraphics[width=\linewidth]{assets/preliminaries/graph_theory/graphs/weighted_edges.pdf}
\caption{Weighted Edges}
\label{fig:weighted-edges}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
\centering
\includegraphics[width=\linewidth]{assets/preliminaries/graph_theory/graphs/weighted_nodes.pdf}
\caption{Weighted Nodes}
\label{fig:weighted-nodes}
\end{subfigure}
\caption{Weighted Graphs}
\label{fig:weighted-graphs}
\end{figure}

Another component of a graph are \textit{subgraphs}, which form a new graph structure from a subset of the vertex set $V(G)$ of the original graph $G$ and contain the edges corresponding to the vertices. Similarly, \textit{supergraphs} are all graphs formed by adding vertices or edges to a graph structure. This implies that if $F$ is a subgraph of $G$, then $G$ must be a supergraph of $F$.

\subsubsection{Properties}\label{subsec:properties}

There exists a large variety of different forms and special cases in which graph networks can occur. Directed graphs, also called \textit{digraphs}, which are defined by the order of vertex pairs, have already been introduced. Similarly, the properties of weighted graphs, which are defined by adding a weight function $w$ and thus formulated as $G = (V, E, w)$, have been explained. However, these two cases are far from being the only special cases of graph structures. A graph can be called a \textit{complete graph} if every single vertex is connected to every other vertex by an edge (figure \ref{fig:complete-graph}), allowing the number of edges in a complete graph to be calculated by the formula \(K(n) = n * (n - 1) / 2\) for undirected graphs. Another subset is called a \textit{bipartite graph} when the set of vertices of the graph can be divided into two disjoint subsets such that each edge of the graph connects a vertex from the vertex subset $A$ to a vertex from the subset $B$ (figure \ref{fig:bipartite-graph}). In this case, the bipartite graph can be represented as $G = (A, B, E)$, where $A$ and $B$ represent the two vertex subsets and $E$ is the edge set connecting $A$ and $B$.

\begin{figure}
\centering
\begin{subfigure}{.5\textwidth}
\centering
\includegraphics[width=\linewidth]{assets/preliminaries/graph_theory/graphs/complete_graph.pdf}
\caption{Complete Graph}
\label{fig:complete-graph}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
\centering
\includegraphics[width=\linewidth]{assets/preliminaries/graph_theory/graphs/bipartite_graph.pdf}
\caption{Bipartite Graph}
\label{fig:bipartite-graph}
\end{subfigure}
\caption{Graph Properties}
\label{fig:graph-properties-1}
\end{figure}

Another essential concept in graph theory are \textit{trees}\footcite{wilson1979introduction} (figure \ref{fig:tree}). They are circle-free graphs in which there is no path from a vertex back to itself. Like conventional graphs, they consist of a subset of vertices and edges, but the edges are directed, and so trees are directed graphs. The analogy with biological trees is that there is only one vertex in a graph tree, called the root, which marks the starting point of the tree. All vertices of the object have exactly one parent node, except for the root node. However, each vertex can have several child nodes. A well known variant of trees is the so-called \textit{\glspl{binary tree}}, which differs from conventional trees in that each vertex has only two child nodes.

\begin{figure}
\centering
\begin{subfigure}{.5\textwidth}
\centering
\includegraphics[width=\linewidth]{assets/preliminaries/graph_theory/graphs/planar_graph.pdf}
\caption{Planar Graph}
\label{fig:planar-graph}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
\centering
\includegraphics[width=\linewidth]{assets/preliminaries/graph_theory/graphs/tree_graph.pdf}
\caption{Tree}
\label{fig:tree}
\end{subfigure}
\caption{Graph Properties}
\label{fig:graph-properties}
\end{figure}

Of interest in geometric representations of graphs are certain graph structures where, by definition, no edge intersects when represented on a plane. This particular property of graphs is called \textit{planar} (figure \ref{fig:planar-graph}). In this case, the faces formed between the edges can be considered, and it holds that the \textit{\Gls{euler characteristic}} described by \(|V| - |E| + |F| \)\footcite{dawes2013applications} always gives 2 for planar graphs, where $|F|$ is the number of faces formed by the edges and $|V|$ and $|E|$ represent the number of vertices and edges. Planar graphs offer interesting geometric interpretations and allow for an easy understanding once represented in the two-dimensional space.

\subsubsection{Representation}\label{subsec:representation}

As previously mentioned, graphs are space-independent representations since the position of vertices per se is not bound to local information and edges, contrary to intuitive interpretation, do not represent a geometric connection between pairs of vertices, but only their relationship to each other. Since we as individuals can only realise visual representations in three dimensions, and in the case of immobile representations are generally limited to two dimensions, a visual representation of spatially independent mathematical concepts and objects, such as graphs, always involves a certain degree of abstraction. In order to avoid misinterpretation of the information to be conveyed, it is important to remain conscious of this abstraction when considering visualised graphical information, especially when graphs are derived from geometric figures.

\begin{figure}
\centering
\begin{subfigure}{.5\textwidth}
\centering
\includegraphics[width=\linewidth]{assets/preliminaries/graph_theory/graphs/same_graph_1.pdf}
\caption{Unordered Representation}
\label{fig:unordered-representation}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
\centering
\includegraphics[width=\linewidth]{assets/preliminaries/graph_theory/graphs/same_graph_2.pdf}
\caption{Ordered Representation}
\label{fig:ordered-representation}
\end{subfigure}
\caption{Ordering of Graph Nodes}
\label{fig:ordering-of-graph-nodes}
\end{figure}

Numerical, tabular representations can counteract this risk of confusion by their inherently abstract form of representation. However, this level of abstraction is a major drawback of tabular representations of graph structures, since relationships between nodes are more difficult to perceive and a greater amount of representative information is generally required. Whereas in a visual representation a non-existent edge is simply not drawn, in a tabular representation a value is required to indicate its non-existence.

The main visual representations of simple graph structures are node diagrams (figure \ref{fig:2d-representation}) and their individual special cases such as \glspl{binary tree} or directed graphs. This type of representation is particularly suitable for planar graphs and when it is necessary to quickly convey an overview of the entire graph structure and the relationship of individual nodes. There are no explicit visual rules for representing the various components of graphs, but it is common to represent vertices as points or circles of the same size, and edge connections as straight lines between points if possible, or curves if straight lines are not possible. For directed edges and graphs, the orientation is usually symbolised by an arrow on the edge\footcite{wilson1979introduction}. As a general convention, there should be as few crossings between graph edges as possible to avoid dimensional confusion (figure \ref{fig:ordering-of-graph-nodes}).

\begin{figure}
\centering
\includegraphics[width=0.6\textwidth]{assets/preliminaries/graph_theory/graphs/representation_graph.pdf}
\caption{Graph Representation}
\label{fig:2d-representation}
\end{figure}

The most common numerical and tabular representations of graphs are the \textit{adjacency matrix} (table \ref{tab:adjacency-matrix}), the \textit{adjacency list} (table \ref{tab:adjacency-list}), the \textit{incidence matrix} (table \ref{tab:incidence-matrix}) or the \textit{edge list} (table \ref{tab:edge-list}). Here, edge lists provide the most intuitive representation, since it concerns simply a listing of the references to the individual vertex pairs connected through edges.

\begin{table}
\begin{subtable}{0.33\textwidth}\centering
\begin{tabular}{ c c c c c c c }
& \textbf{1} & \textbf{2} & \textbf{3} & \textbf{4} & \textbf{5} & \textbf{6} \\
\textbf{1} & 0 & 1 & 1 & 1 & 0 & 0 \\
\textbf{2} & 1 & 0 & 0 & 1 & 0 & 0 \\
\textbf{3} & 0 & 0 & 0 & 0 & 1 & 0 \\
\textbf{4} & 0 & 0 & 0 & 0 & 0 & 1 \\
\textbf{5} & 1 & 0 & 1 & 0 & 0 & 0 \\
\textbf{6} & 0 & 0 & 1 & 1 & 0 & 0 \\
\end{tabular}
\subcaption{Adjacency Matrix}
\label{tab:adjacency-matrix}
\end{subtable}
\begin{subtable}{0.41\textwidth}\centering
\begin{tabular}{ c c c c c c c c c }
& \textbf{a} & \textbf{b} & \textbf{c} & \textbf{d} & \textbf{e} & \textbf{f} & \textbf{g} & \textbf{h} \\
\textbf{1} & -1 & 1 & 0 & 1 & 1 & 0 & 0 & 0 \\
\textbf{2} & 0 & 0 & 0 & 1 & 0 & 0 & 1 & 0 \\
\textbf{3} & 0 & -1 & 1 & 0 & 0 & -1 & 0 & 0 \\
\textbf{4} & 0 & 0 & 0 & 0 & -1 & 0 & -1 & 1 \\
\textbf{5} & 1 & 0 & 1 & 0 & 0 & 0 & 0 & 0 \\
\textbf{6} & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 1 \\
\end{tabular}
\subcaption{Incidence Matrix}
\label{tab:incidence-matrix}
\end{subtable}
\begin{subtable}{0.24\textwidth}\centering
\begin{tabular}{ c c c c c c c }
\textbf{1} & 2 & 4 & 3 \\
\textbf{2} & 1 & 4 & \\
\textbf{3} & 5 & & \\
\textbf{4} & 6 & & \\
\textbf{5} & 3 & 1 & \\
\textbf{6} & 4 & 3 & \\
\end{tabular}
\subcaption{Adjacency List}
\label{tab:adjacency-list}
\end{subtable}
\newline
\vspace*{2pt}
\newline
\begin{subtable}{\textwidth}\centering
\begin{tabular}{ c c c c c c c c c c c }
(1,2) & (1,4) & (1,3) & (2,1) & (2,4) & (3,5) & (4,6) & (5,3) & (5,1) & (6,3) & (6,4) \\
\end{tabular}
\subcaption{Edge List}
\label{tab:edge-list}
\end{subtable}
\caption{Numerical Representation Methods}
\label{tab:numerical-representation-methods}
\end{table}

A slightly modified form of this listing is the adjacency list, where for each vertex in the graph the references of the vertices connected by edges are listed. Incidence and adjacency matrices are two tabular forms of representing relational networks and differ mainly in that in the case of the adjacency matrix the rows and columns represent the same vertices and thus always result in square tables, which are mirrored diagonally in the case of undirected graphs. The values of the table are binary and indicate whether an edge exists between the two reference vertices or not.

In the case of the incidence matrix, one row or column is the reference to the edges and the other remains the reference to the individual vertices. If an edge j is connected to vertex i, then the entry in the i-th row and j-th column of the incidence matrix is a 1 if the edge is outgoing from vertex i, otherwise it is a -1 if the edge is incoming to vertex i. All other entries are 0. An incidence matrix is usually used for directed graphs, but can also be used for undirected graphs.

\subsection{Graphs in Architecture}\label{subsec:graphs-in-architecture}

The application of graph theoretic methods of analysis has been an integral part of architectural research since the beginning of the last century\footcite{earl1979architectural}. The discrete mathematical concepts can be beneficially applied at various levels of design practice, providing insight into spatial and relational structures that would be difficult to relate without their intervention.

In concrete applications, graph theory can be used to analyse, optimise and plan designs and buildings\footcite{dawes2013applications}. The use of graphs as a modelling tool also makes it possible to represent and analyse complex structures\footcite{lakshmi2017graph} such as road networks, public spaces or buildings in a simplified form (figure \ref{fig:la-rotonda} and \ref{fig:connectivity-graphs-for-architectural-examples}). Both quantitative and qualitative aspects of the structure can be considered, such as the length and temporal duration of paths, the number and type of connections, or the visual relationships between individual architectural elements.

In academic architectural research, graph theory is also used to study the spatial properties of buildings and urban spaces\footcite{napong2004graph}. In doing so, it can help to study and understand the structure and organisation of spaces as well as the relationships between them. It can equally be used to analyse and evaluate urban design concepts or to develop new urban designs.

The implementation of graph-theoretical methods in project elaboration at the conceptual level can also play an important role, since the abstract and relational information of the mathematical object can form an essential function in the development of programmatic and structural organisation. In some projects, it may also be of great value to construct a detailed social or biological network in order to understand, analyse and optimise existing systems or desired relationships between individuals, groups of actors or families.

\begin{figure}
\centering
\includegraphics[width=0.8\linewidth]{assets/preliminaries/graph_theory/architecture/rotonda.pdf}
\caption{Connectivity Graph of Palladio's Villa La Rotonda}
\label{fig:la-rotonda}
\end{figure}

\begin{figure}
\centering
\begin{subfigure}{.5\textwidth}
\centering
\includegraphics[width=\linewidth]{assets/preliminaries/graph_theory/architecture/mahal.pdf}
\caption{Taj Mahal}
\label{fig:taj-mahal}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
\centering
\includegraphics[width=\linewidth]{assets/preliminaries/graph_theory/architecture/savoye.pdf}
\caption{Villa Savoye}
\label{fig:villa-savoye}
\end{subfigure}
\caption{Connectivity Graphs of Architectural Examples}
\label{fig:connectivity-graphs-for-architectural-examples}
\end{figure}

The potential of graphs to carry additional information at their nodes and edges is equally shown to be of great value in the architectural context, as it becomes possible to add geometric, dimensional, physical or social information to the network, thus creating as representative an image of reality as possible. In subsequent steps, this allows the simulation and analysis of the effects of architectural interventions.

Graph theory can furthermore be usefully applied to the development of new structures. By modelling the structure as a graph, the designer can test and evaluate different layouts and configurations. For example, the number of connections or the length of paths can be empirically evaluated to maximise the efficiency of the structure while satisfying aesthetic and functional requirements.

A key aspect explored in this thesis in relation to graph structures in the architectural design process is the analysis of spatial and elemental relationships. By defining spaces and architectural components as nodes and their relationships to each other as edges, designers can explore and understand these interconnections. This can be used, for example, to represent and evaluate the visibility between different spaces, or to measure the efficiency of the connections between them in order to achieve optimal spatial organisation.

\subsubsection{Graphs in BIM}\label{subsec:graphs-in-bim}

\textit{\acrlong{bim}}, or \acrshort{bim}, refers to the process of creating, managing and using digital architectural models of buildings and architectural elements, in which all relevant data, such as dimensions, materials, components, installations and technical systems, are stored and linked in a central digital model. The aim of \acrshort{bim} is to optimise the planning and construction of projects by bundling and coordinating the flow of information. By using \acrshort{bim}, stakeholders involved in design and construction, such as architects, engineers and contractors, can work more efficiently and collaboratively, reducing errors and costs through centralised data exchange and reconciliation.

An essential aspect of the concept described is interoperability between the parties involved, but also data exchange between established architecture and engineering software solutions. To ensure this, open standards such as the \acrshort{ifc} file format are often used, which, due to its \gls{open source} characteristics, allows data to be shared between different \acrshort{bim} software tools and to be read and processed optimally. Developed and maintained by the \textit{BuildingSMART International} organisation, the \acrshort{ifc} file format, which stands for \textit{\acrlong{ifc}}, is a human-readable text file containing structured and hierarchically categorised information about the given architectural object. The information contained and stored is object-oriented and can therefore assign an unlimited number of attributes to a wide range of constructive entities.

Elements in \acrshort{ifc} projects can be of diverse nature, such as project sites, levels, walls, ceilings, windows and doors, and attributes can be attached as information about these individual elements, such as a description of their materials, dimensions and relationships to each other\footcite{schultz2011toward}. The direct implementation of graphical relationship descriptions in \acrshort{bim} modelling processes has not yet been established, partly due to the aforementioned representation difficulties caused by their abstract structure. However, looking at the compositional logic of \acrshort{ifc} file types and their application in everyday architecture, it becomes clear that the integration of graphical representations could be of great benefit for collaboration and analysis capabilities.

Due to their hierarchical structure of interrelated objects, \acrlong{ifc} lend themselves as \acrshort{bim} models to representing the objects as nodes and the relationships between them as edges. This graphical representation allows additional relational information, such as local distances or conceptual links, to be embedded in the basic hierarchical structure of the file format. The integration of such graph-based representations could be then used to analyse and optimise aspects of building design such as space planning, energy and resource optimisation. Such integration would provide a variety of new automatable analysis capabilities and enable additional levels of abstraction in the architectural design process.

\subsubsection{Graph Analysis}\label{subsec:graph-analysis}

In addition to their organisational and hierarchical capabilities, graph elements offer a variety of different analytical methods that can be of significant use in the design process as well as in the optimisation phase of architectural, urban or landscape projects. In order to provide an overview of these analysis methods, the most important ones are presented and demonstrated in order to evaluate the usefulness of graphs in the field of \textit{\acrlong{aec}}.

\paragraph{Space Syntax Analysis}\label{par:space-syntax-analysis}

\textit{Space syntax} analysis methods\footcite{hillier1989social} are a set of tools that allow the structure of an architectural object to be considered in terms of its social role. This means observing how and through what interactions the structure of a building influences the human use of a space. This allows architects and designers to optimise the connections between spaces, as well as the arrangement of doors and passages, to make the pathways and connections within a building more effective in their use of space and more user-friendly.

\paragraph{Visibility Graph Analysis}\label{par:visibility-graph-analysis}

By analysing \textit{visibility graphs}\footcite{lee2017measuring} (figure \ref{fig:visibility-graphs}), conclusions can be drawn about spatial perception at the human level. In this way, certain aspects such as privacy, openness towards public spaces and accessibility can be analysed and optimised in order to provide the most appropriate spatial perception. The \textit{\acrfull{vga}} method is used to create visibility maps that assign values to the entire surface of the objects being analysed, providing information on how observable they are from all other points in the space.

\begin{figure}
\centering
\begin{subfigure}{.5\textwidth}
\centering
\includegraphics[width=\linewidth]{assets/preliminaries/graph_theory/architecture/visibility_graph.pdf}
\caption{Visibility Between Nodes}
\label{fig:visibility-between-nodes}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
\centering
\includegraphics[width=\linewidth]{assets/preliminaries/graph_theory/architecture/visibility_graph_obstacles.pdf}
\caption{Visibility with Obstacles}
\label{fig:visibility-with-obstacles}
\end{subfigure}
\caption{Visibility Graphs}
\label{fig:visibility-graphs}
\end{figure}

\paragraph{Graph Clustering}\label{par:graph-clustering}

Unlike the previous methods, \textit{graph clustering} (figure \ref{fig:graph-clustering}) is an abstract form of analysis in architectural applications. Once a network has been created based on geometric or topological concepts, it can be divided into individual subgroups by observing the \textit{clustering coefficient} of the nodes. This allows the designer to abstract complex projects and environments and understand their interrelationships as well as the distinctions between them in order to make specific design decisions in a meaningful way. This method unfolds its full potential when dealing with complex buildings or urban structures.

\begin{figure}
\centering
\begin{subfigure}{.5\textwidth}
\centering
\includegraphics[width=\linewidth]{assets/preliminaries/graph_theory/architecture/graph_clustering.pdf}
\caption{Network}
\label{fig:network}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
\centering
\includegraphics[width=\linewidth]{assets/preliminaries/graph_theory/architecture/clustered.pdf}
\caption{Clustering}
\label{fig:clustering}
\end{subfigure}
\caption{Graph Clustering}
\label{fig:graph-clustering}
\end{figure}

\paragraph{Multiscale Graph Analysis}\label{par:multiscale-graph-analysis}

This method allows the analysis of a system represented by points and edges at multiple levels or scales. By being able to analyse at multiple scales, significant insights can be obtained about the interactions between individual elements of different layers, allowing for the design of more effective and, above all, more integrative design concepts. A meticulous elaboration of the graph structure to be analysed is necessary to successfully represent this interaction between the individual subsystems.

\begin{figure}
\centering
\includegraphics[width=.85\textwidth]{assets/preliminaries/graph_theory/architecture/congestion.pdf}
\caption{Congestion of Floor Plan}
\label{fig:congestion-of-floor-plan}
\end{figure}

\paragraph{Congestion Analysis}\label{par:congestion-analysis}

\textit{Congestion analysis} methods originate from urban applications, where the risk of congestion (figure \ref{fig:congestion-of-floor-plan}) is calculated as a function of the geometric properties of the infrastructure network. However, this method of analysis has also found useful applications in building architecture, where the set of possible paths from one point to an opposite point is computed. Values are thus calculated for the entirety of the plan being analysed, which can provide information about the overlap of these paths and describe the risk of congestion within the building at all points. This method therefore represents an essential tool that can lead to the optimisation of circulation within a building through morphological changes.

\paragraph{Centrality Analysis}\label{par:centrality-analysis}

Using graph-based representations of complex systems and geometric or topological structures, the \textit{\gls{centrality}} of individual elements within the system can be inferred. By comparing and summing the number of edges of each node, conclusions about the degree of \gls{centrality} can be drawn. In addition, examining properties that influence \gls{centrality}, such as the \textit{\gls{node degree}} along certain paths, provides insight into the hierarchy and structure of the system. A practical application of this method of analysis is to observe the \textit{\gls{axial map}} of a building complex to verify whether particular rooms meet or exceed the desired level of \gls{centrality}. There are two main types of \gls{centrality} measures: \textit{closeness} (figure \ref{fig:closeness-centrality}) and \textit{betweeness} (figure \ref{fig:betweeness-centrality}). While the former quantifies how close each node is to all other nodes in the network, the latter measures the degree to which a node is located on the shortest paths between pairs of other nodes in the graph structure.

\begin{figure}
\centering
\begin{subfigure}{.5\textwidth}
\centering
\includegraphics[width=\linewidth]{assets/preliminaries/graph_theory/architecture/savoye_betweeness_centrality.pdf}
\caption{Villa Savoye}
\label{fig:villa-savoye-1}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
\centering
\includegraphics[width=\linewidth]{assets/preliminaries/graph_theory/architecture/rotonda_betweeness_centrality.pdf}
\caption{La Rotonda}
\label{fig:la-rotonda-1}
\end{subfigure}
\caption{Betweeness Centrality}
\label{fig:betweeness-centrality}
\end{figure}

\begin{figure}
\centering
\begin{subfigure}{.5\textwidth}
\centering
\includegraphics[width=\linewidth]{assets/preliminaries/graph_theory/architecture/savoye_closeness_centrality.pdf}
\caption{Villa Savoye}
\label{fig:villa-savoye-2}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
\centering
\includegraphics[width=\linewidth]{assets/preliminaries/graph_theory/architecture/rotonda_closeness_centrality.pdf}
\caption{La Rotonda}
\label{fig:la-rotonda-2}
\end{subfigure}
\caption{Closeness Centrality}
\label{fig:closeness-centrality}
\end{figure}

\paragraph{Frequent Pattern Analysis}\label{par:frequent-pattern-analysis}

Another method of graph analysis more commonly used in urbanism is called \textit{frequent pattern mining}, which uses specific algorithms to identify repeating patterns within the graph network, and can be used to examine certain urban constellations for possible problems or even advantages. In architectural applications, this method can also be used to classify complex building structures, for example to identify all the vertical staircases in a building complex or to categorise the individual corridors.

\paragraph{Connectivity Analysis}\label{par:connectivity-analysis}

The most important information conveyed by graphs is the connectivity of the nodes. In fact, the existence of an edge between individual elements in a network effectively indicates a relationship between them. It is therefore not surprising that a common method of graph analysis in both urbanism and architecture is \textit{connectivity analysis}\footcite{dawes2021examining}. This method involves taking individual elements of the system as starting points, trying to analyse their connection to other elements, and ultimately understanding the full interconnectedness of objects and making design proposals accordingly.

\paragraph{Community Detection}\label{par:community-detection}

Similar to graph clustering, \textit{community detection} is used to identify groups of elements in a system that share similar properties or characteristics. However, this method looks at both the information carried by each node and the edge properties of each link to identify individual groups. In this respect, this method can be used by the designer to understand existing systems, such as ecological systems, or to visualise the grouping of building elements in a designed building based on their attributes and connections to each other.

\paragraph{Morphological Analysis}\label{par:morphological-analysis}

In the example of \textit{morphological analysis} of graph networks, the general formal aspects (figure \ref{fig:graph-morphologies}) of the network are considered. Depending on the level of abstraction of the represented data, information about the morphology of the structure can be obtained. In an architectural application, for example, this allows the identification of the ground relationship\footcite{alymani2023classifying} of a particular building or other formal aspects such as strong elongations or circular structures on a geometric-formal level.

\begin{figure}
\centering
\begin{subfigure}{.5\textwidth}
\centering
\includegraphics[width=\linewidth]{assets/preliminaries/graph_theory/architecture/circular_morphology.pdf}
\caption{Circle}
\label{fig:circle}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
\centering
\includegraphics[width=\linewidth]{assets/preliminaries/graph_theory/architecture/star_morphology.pdf}
\caption{Star}
\label{fig:star}
\end{subfigure}
\caption{Graph Morphologies}
\label{fig:graph-morphologies}
\end{figure}

\paragraph{Random Walks}\label{par:random-walks}

A graph entity can be tested for possible design related issues by applying the \textit{random walk} method, which consists of starting iteratively or in parallel at randomly chosen nodes of the network and walking along an equally randomly chosen path. This initially purely \textit{\gls{stochastic}} method makes it possible to reveal paths of movement that might have remained unknown to the designer without the use of this method. A concrete application could be an automatic analysis of the architectural usability of the designed object, which can reveal unwanted dead ends or missing connections between rooms.

\paragraph{Minimum Spanning Trees}\label{par:minimum-spanning-trees}

An elementary method of mathematical graph theory is to create a subgraph of the original graph such that it connects all vertices. The particularity of this subgraph, however, is that each pair of nodes of this graph is connected by only a single edge, thus representing the mathematical concept of a tree. The formation of this \textit{spanning tree} is fundamental to a number of graph analysis methods, including the computation of a \textit{\acrlong{mst}}, which describes the spanning tree of a graph that has the lowest total edge weight (figure \ref{fig:mst-of-base-graph}). For unweighted graphs, this is equivalent to the lowest number of edges. It becomes apparent from this definition that the application of such minimal spanning trees in a complex building can provide essential information about the shortest overall connection of all represented elements on a two- or even three-dimensional level.

\begin{figure}
\centering
\begin{subfigure}{.5\textwidth}
\centering
\includegraphics[width=\linewidth]{assets/preliminaries/graph_theory/architecture/mst_graph.pdf}
\caption{Base Graph}
\label{fig:base-graph}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
\centering
\includegraphics[width=\linewidth]{assets/preliminaries/graph_theory/architecture/mst.pdf}
\caption{Minimum Spanning Tree}
\label{fig:minimum-spanning-tree}
\end{subfigure}
\caption{MST Computation}
\label{fig:mst-of-base-graph}
\end{figure}

\paragraph{Network Flow Analysis}\label{par:network-flow-analysis}

Originating in engineering and computer science, \textit{network flow analysis} can provide important information about the performance of a defined oriented system where a particular capacity or resource is to be optimised. It involves the construction of a directed graph with a starting point and one or more end points. The individual edges and nodes can receive information about a maximum allowable quantity of the flowing medium, representing the individual constraining instances of the flow network. This method is most commonly used to test the performance and capacity of systems such as plumbing, power grids, sewer systems or even ventilation systems. However, the flowing medium can also consist of crowds and can therefore be used to test the capacity of architectural designs, particularly in public buildings such as concert halls, conference centers or hospitals, to accommodate a certain number of visitors.

\paragraph{Node Degree Analysis}\label{par:node-degree-analysis}

A relatively intuitive method of graph analysis is to look at the \gls{node degree} of each node (figure \ref{fig:node-degree}). This can provide information about the degree of integration of the particular element under consideration into the system at hand, and thus provide conceptual support in the pursuit of a well-connected architectural object.

\begin{figure}
\centering
\begin{subfigure}{.5\textwidth}
\centering
\includegraphics[width=\linewidth]{assets/preliminaries/graph_theory/architecture/savoye_degree.pdf}
\caption{Villa Savoye}
\label{fig:villa-savoye-3}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
\centering
\includegraphics[width=\linewidth]{assets/preliminaries/graph_theory/architecture/rotonda_degree.pdf}
\caption{La Rotonda}
\label{fig:la-rotonda-3}
\end{subfigure}
\caption{Node Degree}
\label{fig:node-degree}
\end{figure}

\paragraph{Geometric Graph Theory}\label{par:geometric-graph-theory}

The methodology of \textit{geometric graph theory} is concerned with the analysis of spatial networks which, unlike \textit{\gls{non-euclidean}} graphs, describe a geometric representation of objects represented in graph form. Thus, edges in these geometric graphs describe exact metric distances between individual vertices, and the individual vertices refer to exact two- or three-dimensional \textit{\gls{cartesian coordinates}}. In an architectural application, this allows a purely geometric view of the represented system, as opposed to the analysis of topological or abstract programmatic information. This enables the designer to calculate the total distance in metric values between two nodes or, for example, the exact length of the longest paths from certain rooms in a building complex to the nearest emergency exit.

\paragraph{Shortest Paths Analysis}\label{par:shortest-paths-analysis}

The \textit{shortest path} problem (figure \ref{fig:shortest-path-analysis}) in graph theory describes the search for the shortest connection between any two arbitrarily chosen vertices of a graph. In other words, it is about finding the shortest sequence of different vertices connected by edges, starting with an arbitrary node of the graph and ending with another arbitrary node. There are several different algorithms for finding this shortest path with different computational speeds. As a method of analysis in architecture, the search for the shortest path is an essential concept to topologically analyse the architectural object and to identify widely separated spaces or to strategically position important elements such as emergency exits or lifts.

\begin{figure}
\centering
\begin{subfigure}{.5\textwidth}
\centering
\includegraphics[width=\linewidth]{assets/preliminaries/graph_theory/architecture/shortest_path1.pdf}
\caption{Shortest Path I}
\label{fig:shortest-path-i}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
\centering
\includegraphics[width=\linewidth]{assets/preliminaries/graph_theory/architecture/shortest_path2.pdf}
\caption{Shortest Path II}
\label{fig:shortest-path-ii}
\end{subfigure}
\caption{Shortest Path Analysis}
\label{fig:shortest-path-analysis}
\end{figure}

\subsubsection{Graph Machine Learning}\label{subsec:graph-machine-learning}

In the field of architecture, the application of machine learning has rapidly gained importance in the recent years\footcite{belem2019impact}. These computer science methods make it possible to analyse large amounts of data and use established algorithms to identify patterns or make predictions that are relevant to the design and planning of architectural objects. Machine learning can thus help to optimise architectural projects and improve the performance of these objects in a variety of ways, for example by analysing the behaviour of spatialities and materials under different conditions. For instance, machine learning can be used to optimise certain parameters or dimensions that improve a building's energy requirements or load-bearing capacity. Similarly, data science methods can be of great benefit to architects in analysing user behaviour and designing spaces. However, there are also some challenges in applying machine learning to architecture\footcite{pena2021artificial}, such as the need for high quality, evaluated and maintained datasets, and the sometimes high complexity of the algorithms.

In principle, there are two main types of machine learning tasks, called \textit{classification} and \textit{regression}. Classification involves dividing data into predefined classes or categories, while regression involves making continuous predictions based on numerical values. For example, categorising building elements into windows, doors and walls would be a classification task, while predicting the number of people in an office building at a given time would be a regression problem. In addition, there are different types of training procedures in data science, which come with their own set of benefits and challenges. The two main methods, \textit{supervised} and \textit{unsupervised learning}, are basic categories of machine learning that differ in the type of data provided. In supervised learning, algorithms are trained on labelled data, where the desired value or class is known in advance. In unsupervised learning, the models are trained on unlabelled data where no clear answers are given. There is also \textit{semi-supervised learning}, a hybrid of supervised and unsupervised learning, where both labelled and unlabelled data are used for training.

\textit{Graph-based machine learning} is the term used to describe the relatively new branch of data science that relies on datasets in graphical form and therefore involves a number of special procedures. Training machine learning on graph data, as opposed to the more common tabular data, has the significant advantage that the trained models receive correlations and relationships as input data, making it possible to identify graph-specific qualities or problem sets. In the context of graph-based machine learning, both classification and regression tasks can arise. For example, the task may be to classify nodes in a graph based on certain features, or to make predictions about the weights of certain edges in a weighted graph. Similarly, the training methods can vary, so that both supervised and unsupervised learning can be applied in the graph context. For example, the goal can be to learn the classification of nodes in a graph by training on labelled data, or the objective may be to discover hidden patterns or commonalities in the graph by clustering or dimension reduction without the need for labelled data. Accordingly, the hybrid of the two training variants, semi-supervised learning, can be used to deal with a limited amount of labelled graph data as well as a large number of unlabelled graphs.

\begin{figure}
\centering
\includegraphics[width=\textwidth]{assets/preliminaries/graph_theory/graph_machine_learning/gnn.pdf}
\caption{Graph Neural Network Scheme}
\label{fig:graph-neural-network-scheme}
\end{figure}

The discipline of graph-based machine learning in data science has seen a considerable amount of new research\footcite{velivckovic2023everything} in recent years and is in constant flux. Nevertheless, four main groups of \textit{neural network} methods have emerged as useful \textit{deep learning} approaches.

\paragraph{Graph Neural Networks}\label{par:graph-neural-networks}

A broad family of machine learning models that focus on graph data processing are \textit{\acrfull{gnns}}. By propagating information within the graph and aggregating features from neighbouring nodes, they produce more meaningful representations of each vertex. Through this iterative process, \acrshort{gnns} can perform tasks such as \textit{node classification,} \textit{link prediction}, and \textit{graph-level prediction}. \acrshort{gnns} have proven to be extremely powerful in a number of domains, including architecture, for a variety of tasks. There are several variants of \acrshort{gnns}, each with its own specific techniques and applications. The principal groups are presented below.

\paragraph{Graph Convolutional Networks}\label{par:graph-convolutional-networks}

\acrshort{gnns} architectures called \textit{\acrfull{gcns}} have been developed specifically for processing data represented in graph structures. They perform \textit{\glspl{convolution}} on the graph by aggregating local information from adjacent nodes to create new representations or features that capture both the local structure and the properties of the nodes (figure \ref{fig:graph-neural-network-scheme}). The \gls{convolution} process uses both the features of the nodes and the topology of the graph to capture complex patterns and relationships. These networks can be used for tasks such as \textit{graph clustering}, \textit{node and edge classification} or \textit{edge prediction}. By harnessing the topological information within graph structures, \acrshort{gcns} can outperform conventional \acrshort{gnns}.

\paragraph{Graph Attention Networks}\label{par:graph-attention-networks}

A special type of \acrshort{gnns} called \textit{\acrfull{gats}} introduces an attention mechanism to evaluate the relative importance of neighbouring nodes in aggregating information, such as evaluating the influence of different architectural components within a building design. By using attention, \acrshort{gats} can assign different weights to different nodes in the graph, focusing on the most relevant nodes in a given context. This selective attention not only improves the efficiency of the model, but also allows it to capture more fine-grained, contextual relationships within the graph.

\paragraph{Graph Autoencoder}\label{par:graph-autoencoder}

Unsupervised learning models called \textit{\acrfull{gaes}} encode graph data, such as architectural layouts, into a compact \textit{latent representation} that can be subsequently decoded into the original graph structure. \acrshort{gaes} consist of two primary \acrlong{gnns}: an \textit{encoder}, which transforms the input graph into a low-dimensional representation, and a \textit{decoder}, which reconstructs the graph from the compressed representation. This latent representation can be used for tasks such as dimension reduction or graph generation. \acrshort{gaes} are able to efficiently learn expressive representations and provide insights into the underlying patterns and relationships within the data.

\paragraph{Graph Recurrent Neural Networks}\label{par:graph-recurrent-neural-networks}

\textit{\acrfull{grnns}} are another class of \acrshort{gnns} that incorporate \textit{\acrfull{rnns}} components to model dynamic, \textit{sequential data} on graphs, such as evolving architectural designs or construction processes. \acrshort{grnns} are particularly useful for problems where the graph structure or node attributes change over time, such as in temporal networks or dynamic structures. By combining the expressiveness of \acrshort{gnns} with the sequential modelling capabilities of \acrshort{rnns}, \acrshort{grnns} can capture both spatial and temporal dependencies within the data, enabling accurate prediction and in-depth analysis of temporally evolving graph structures.

\section{Topology}\label{sec:topology}

Architectural thinking is usually closely linked to the understanding and analysis of geometric methods, since dimensions, surfaces and angles constitute an elementary part of the discipline. However, if we consider only the space generated by surfaces and boundaries, the metric components can be disregarded for the time being. In fact, the space defined by the domain of a sphere has the same properties, regardless of size or distortion transformations, as long as we do not consider the interior as a metric unit of volume.

This becomes interesting as soon as we compare the interior of a sphere with the interior of a cube. In both cases it is a space enclosed by a continuous surface without openings to the surrounding environment. To illustrate this theoretical understanding of space, the example of a doughnut and a coffee cup is often used, since although the two shapes have little in common at first sight, they are indistinguishable on a purely spatial-formal level. In fact, a coffee cup can be transformed into a doughnut by certain matrix transformations without breaking or joining the faces that make up the object. This approach is called \textit{topology} because it deals with the study, \textit{logos}, of spaces, \textit{topos}. In other words, topology is the branch of mathematics that deals with the analysis of solids undergoing continuous deformation without being opened, closed, torn, joined or self-overlapping\footcite{kantor2005tale} (figure \ref{fig:topological-deformation-of-a-cube}).

\begin{figure}
\centering
\begin{subfigure}{.25\textwidth}
\centering
\includegraphics[width=\linewidth]{assets/preliminaries/topology/topology1.pdf}
\label{fig:topological-shape-i}
\end{subfigure}%
\begin{subfigure}{.25\textwidth}
\centering
\includegraphics[width=\linewidth]{assets/preliminaries/topology/topology2.pdf}
\label{fig:topological-shape-ii}
\end{subfigure}%
\begin{subfigure}{.25\textwidth}
\centering
\includegraphics[width=\linewidth]{assets/preliminaries/topology/topology3.pdf}
\label{fig:topological-shape-iii}
\end{subfigure}%
\begin{subfigure}{.25\textwidth}
\centering
\includegraphics[width=\linewidth]{assets/preliminaries/topology/topology4.pdf}
\label{fig:topological-shape-iv}
\end{subfigure}
\caption{Topological Deformations of a Cube}
\label{fig:topological-deformation-of-a-cube}
\end{figure}

Topology is based on a formal definition of the space concept, in which space is a set of points associated with certain properties and relations. The basic concept is to consider open and closed sets of points in space, where the dimensionality of the space can be unbounded. An \textit{open set} is a set in which each point has a certain radius around it in which there are no other points. A \textit{closed set} is a set that contains all its boundary values, which are the boundaries between the set and the outside world. Based on these basic concepts, other terminologies such as \textit{\glspl{continuous function}}, \textit{\gls{homotopy}} and \textit{\gls{cohomology}} have been developed. A \gls{continuous function} is a mapping between two spaces that preserves the structure of the space. A \gls{homotopy} is a continuous transformation between two functions that change the space in an equivalent way. \Gls{cohomology}, on the other hand, deals with the study of geometric objects by analysing their characteristic classes.

Topological studies are divided into three main areas\footcite{kelley1955general}: \textit{algebraic topology}, \textit{differential topology} and \textit{geometric topology}. Algebraic topology studies space by means of algebraic methods and focuses on the analysis of \gls{homotopy} groups, \gls{cohomology} groups and other algebraic instances. Differential topology, on the other hand, studies space by analysing its \textit{smooth functions} and \textit{differentiable structures}. Geometric topology studies space using geometric methods and investigates questions of shape and structure, such as the study of \textit{\glspl{manifold}} and the curvature of spaces.

Through the concept of topology, new tools for architectural consideration of spatial structuring become apparent and their formal application via transformation and analysis in the design process demonstrates their usefulness.

\subsection{Topological Analysis}\label{subsec:topological-analysis}

Topological methods of analysis are integral to the discipline of architecture, although they are often not clearly recognisable as such to the observer or even the designer. In fact, architecture can in a sense be considered part of the topological discipline, since architecture, according to certain definitions, is about the elaborate division and enclosure of the three-dimensional space that surrounds us, which is achieved by connecting and assembling various constructive components. In this respect, topological methods of analysis in architecture offer a range of possibilities for investigating the properties and relationships between different elements in an architectural system. This involves the analysis of spatial or elemental structures that cannot be reduced to geometric properties such as length, width or height, but rather the topology of space, in other words how the elements are connected to each other or how the respective forms are constructed. In particular, the spatial language tool \textit{Topologic}\footcite{aish2018topologic} and its \Gls{python} \acrshort{api} \textit{TopologicPy}, as well as its analytical capabilities, are considered here as a concrete demonstration of topological methods on architectural objects and elements. The software tool allows n-dimensional bodies to be decomposed into their constituent parts and higher-dimensional structures to be created from n-dimensional basic parts, thus enabling a topological understanding of the formation of boundaries and space.

The hierarchical-categorical structure of elements into topological categories (figure \ref{fig:topologic-components}) such as \textit{vertex}, \textit{edge}, \textit{wire}, \textit{face}, \textit{shell}, \textit{cell}, \textit{\gls{cellcomplex}} and \textit{cluster} allows formal transformations without changing the basic topology of the geometric bodies or even performing specific topological deformations. The functionality is based on the decomposition of shapes into \textit{non-\glspl{manifold}}, so that, for example, a cell body is composed of three or more contiguous closed faces, which become a two-dimensional shell object once this shape is opened through a whole in the boundary of the body.

\begin{figure}
\centering
\begin{subfigure}{.25\textwidth}
\centering
\includegraphics[width=\linewidth]{assets/preliminaries/topology/cluster.pdf}
\caption{Cluster}
\label{fig:cluster}
\end{subfigure}%
\begin{subfigure}{.25\textwidth}
\centering
\includegraphics[width=\linewidth]{assets/preliminaries/topology/cellcomplex.pdf}
\caption{Cellcomplex}
\label{fig:cellcomplex}
\end{subfigure}%
\begin{subfigure}{.25\textwidth}
\centering
\includegraphics[width=\linewidth]{assets/preliminaries/topology/cell.pdf}
\caption{Cell}
\label{fig:cell}
\end{subfigure}%
\begin{subfigure}{.25\textwidth}
\centering
\includegraphics[width=\linewidth]{assets/preliminaries/topology/shell.pdf}
\caption{Shell}
\label{fig:shell}
\end{subfigure}
\begin{subfigure}{.25\textwidth}
\centering
\includegraphics[width=0.5\linewidth]{assets/preliminaries/topology/face.pdf}
\caption{Face}
\label{fig:face}
\end{subfigure}%
\begin{subfigure}{.25\textwidth}
\centering
\includegraphics[width=0.5\linewidth]{assets/preliminaries/topology/wire.pdf}
\caption{Wire}
\label{fig:wire}
\end{subfigure}%
\begin{subfigure}{.25\textwidth}
\centering
\includegraphics[width=.17\linewidth]{assets/preliminaries/topology/edge.pdf}
\caption{Edge}
\label{fig:edge}
\end{subfigure}%
\begin{subfigure}{.25\textwidth}
\centering
\includegraphics[width=.17\linewidth]{assets/preliminaries/topology/vertex.pdf}
\caption{Vertex}
\label{fig:vertex}
\end{subfigure}
\caption{Topologic Components}
\label{fig:topologic-components}
\end{figure}

Using this setup, three main topological relationships can be queried\footcite{jabi2018topologic} in an object-specific manner. The \textit{hierarchical relationship} (figure \ref{fig:hierarchical}) of topological elements refers to the composition of objects by their subelements. For example, the edge of a cell object is a \textit{subtopology} of that same cell object and is referenced as such in the data structure. This also works the other way round: if three vertex elements are connected by edges, their \textit{supertopology} is a wire object.

Another type of relationship, which can be easily queried thanks to the structure of the \textit{Topologic} library, is the \textit{lateral relationship} between individual elements (figure \ref{fig:lateral}). Here, the relation of two n-dimensional elements to each other is queried by looking at the subtopologies they share. For example, a three-dimensional apartment model with rooms and corridors as \gls{cellcomplex} objects can be seen as a collection of spaces, each of which shares at least four edges and correspondingly at least four vertices at the one-dimensional level.

The third relational property is the \textit{connectivity} of the individual elements (figure \ref{fig:connection}), which can be queried through defined methods by describing the topological connection of element A to element B. The connection between the two elements is described in terms of the topological connection between the two elements and can pass along edges, through faces or volumes depending on the defined dimensionality. The connection created in this way can be represented by a graph structure, thus allowing the application of graphical analysis methods.

\begin{figure}
\centering
\begin{subfigure}{.33\textwidth}
\centering
\includegraphics[width=\linewidth]{assets/preliminaries/topology/relation_hierarchy.pdf}
\caption{Hierarchical}
\label{fig:hierarchical}
\end{subfigure}%
\begin{subfigure}{.33\textwidth}
\centering
\includegraphics[width=\linewidth]{assets/preliminaries/topology/relation_lateral.pdf}
\caption{Lateral}
\label{fig:lateral}
\end{subfigure}%
\begin{subfigure}{.33\textwidth}
\centering
\includegraphics[width=\linewidth]{assets/preliminaries/topology/relation_conectivity.pdf}
\caption{Connection}
\label{fig:connection}
\end{subfigure}
\caption{Relation Query Methods}
\label{fig:relation-query-methods}
\end{figure}

Another fundamental feature of the software described is the implementation of \textit{\Glspl{boolean operation}} that can be applied to any n-dimensional pairs of elements, thus creating new topologies. The main \Glspl{boolean operation} are \textit{Union} (figure \ref{fig:union}), \textit{Difference} (figure \ref{fig:cut}), \textit{Intersection} (figure \ref{fig:intersection}), \textit{Symmetric Difference}, \textit{Merge}, \textit{Slice}, \textit{Impose} and \textit{Imprint}, where the resulting objects are always non-\gls{manifold}.

\begin{figure}
\centering
\begin{subfigure}{.25\textwidth}
\centering
\includegraphics[width=\linewidth]{assets/preliminaries/topology/boolean.pdf}
\caption{Base Configuration}
\label{fig:base-configuration}
\end{subfigure}%
\begin{subfigure}{.25\textwidth}
\centering
\includegraphics[width=\linewidth]{assets/preliminaries/topology/boolean_cut.pdf}
\caption{Cut}
\label{fig:cut}
\end{subfigure}%
\begin{subfigure}{.25\textwidth}
\centering
\includegraphics[width=\linewidth]{assets/preliminaries/topology/boolean_union.pdf}
\caption{Union}
\label{fig:union}
\end{subfigure}%
\begin{subfigure}{.25\textwidth}
\centering
\includegraphics[width=\linewidth]{assets/preliminaries/topology/boolean_intersection.pdf}
\caption{Intersection}
\label{fig:intersection}
\end{subfigure}
\caption{Boolean Operations}
\label{fig:boolean-operations}
\end{figure}

The main advantage of using non-\gls{manifold} geometries is that elements can contain subelements that do not necessarily form a closed body. Thus it is possible for a face to contain an \textit{\gls{aperture}} which could represent a window or a door, so that the face becomes the domain of the \gls{aperture} and can accordingly be examined by the topological methods mentioned.

\subsection{Space Syntax}\label{subsec:space-syntax}

When first introduced\footcite{hillier1989social}, the study of \textit{space syntax} was seen as a parallel current to the formal preoccupations of the architectural profession because, as a theory, it is primarily concerned with understanding spatial relationships and their social effects on people. It thus serves to metrically analyse the social performance of architectural objects \footcite{nourian2013designing} and is primarily concerned with the socio-spatial organisation of buildings. Particular attention is paid to the local status of spatiality and its organisational position between private and public space. However, the theory of spatial syntax is by no means limited to buildings, as it can develop its full potential in larger networks such as urban structures. Therefore, this theory is concerned with studying the interaction and behaviour of social and spatial structures in order to achieve the best possible combination and interaction\footcite{franz2005graph}. As a discipline, it combines topological, geometric and social information to test specific spatial configurations through analysis and simulation to evaluate their performance before they are built. Three principal methods of analysis exist in spatial syntax theory, which can describe the relationships and interactions of spatialities at the urban or building scale through cartographic and graphical representation.

\begin{figure}
\centering
\begin{subfigure}{.5\textwidth}
\centering
\includegraphics[width=\linewidth]{assets/preliminaries/topology/space_syntax/isovist1.pdf}
\caption{Isovist Point I}
\label{fig:isovist-point-i}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
\centering
\includegraphics[width=\linewidth]{assets/preliminaries/topology/space_syntax/isovist2.pdf}
\caption{Isovist Point II}
\label{fig:isovist-point-ii}
\end{subfigure}
\caption{Isovist Analysis of Villa Savoye}
\label{fig:isovist-analysis-of-villa-savoye}
\end{figure}

\paragraph{Isovist Analysis}\label{par:isovist-analysis}

An\textit{ \gls{isovist} analysis} (figure \ref{fig:isovist-analysis-of-villa-savoye}) describes a method that allows the visualisation of all elements of a spatial body that are visible from a given point in space. The creation of an \gls{isovist} graph is based on the tracing of lines of sight and fields of vision, and makes it possible to understand and optimise the effect of light, space and materials on the perception of spaces.

\paragraph{Axial Analysis}\label{par:axial-analysis}

\begin{figure}
\centering
\begin{subfigure}{.5\textwidth}
\centering
\includegraphics[width=.98\linewidth]{assets/preliminaries/topology/space_syntax/axial_closeness_centrality.pdf}
\caption{Closeness Centrality}
\label{fig:closeness-centrality-1}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
\centering
\includegraphics[width=.98\linewidth]{assets/preliminaries/topology/space_syntax/axial_length.pdf}
\caption{Path Length}
\label{fig:path-length}
\end{subfigure}
\caption{Axial Analysis of Street Network}
\label{fig:axial-analysis-of-street-network}
\end{figure}

The creation of \glspl{axial map} (figure \ref{fig:axial-graph-of-kreuzberg-district}) is another fundamental tool from the field of spatial syntax analysis and consists in representing spatial structures on an architectural or urban level through a network structure, thus making spatial connections clear. It forms the basis for graph-theoretic analysis methods that build on its framework, but certain conclusions can already be drawn without deeper analysis. For example, the spatial organisation of a building can be better understood and communicated by looking at the connections between rooms of an architectural object and the course of the generated axes.

\begin{figure}
\centering
\includegraphics[height=.966\textheight]{assets/preliminaries/topology/space_syntax/axial_footprint.pdf}
\caption{Axial Graph of the Kreuzberg District}
\label{fig:axial-graph-of-kreuzberg-district}
\end{figure}

\paragraph{Convex Space}\label{par:convex-space}

The \textit{convex space} method (figure \ref{fig:convex-space-analysis}) considers architectural space as a collection of \textit{\gls{convex}} surfaces or volumes separated by walls or other solid elements. The individual \gls{convex} bodies describe the number of respective point pairs that can be connected by an edge without intersecting or leaving the boundaries of the \gls{convex} body. The analysis of convex space is therefore based on looking at the space created by the individual shapes and observing how these areas are connected. This method of analysis provides essential spatial information, as by placing an individual in such a convex space, the totality of the environment can be accessed by the observer, which strongly influences the cognitive perception of such spaces.

\begin{figure}
\centering
\begin{subfigure}{.24\textwidth}
\centering
\includegraphics[width=0.9\linewidth]{assets/preliminaries/topology/space_syntax/convex_space1.pdf}
\caption{Villa Savoye}
\label{fig:villa-savoye-4}
\end{subfigure}%
\begin{subfigure}{.38\textwidth}
\centering
\includegraphics[width=0.9\linewidth]{assets/preliminaries/topology/space_syntax/convex_space2.pdf}
\caption{Convex Space}
\label{fig:convex-space}
\end{subfigure}%
\begin{subfigure}{.38\textwidth}
\centering
\includegraphics[width=0.9\linewidth]{assets/preliminaries/topology/space_syntax/convex_space3.pdf}
\caption{Convex Connection}
\label{fig:convex-connection}
\end{subfigure}
\caption{Convex Space Analysis}
\label{fig:convex-space-analysis}
\end{figure}

With the increased adaptation\footcite{li2009design} of space syntax in architecture and urbanism, other concepts have emerged that allow for more concrete spatial analyses, a more accurate understanding of social structures and their interaction with the environment, and accessibility analyses of designed elements.

\paragraph{Depth Analysis}\label{par:depth-analysis}

\textit{Depth analysis} (figure \ref{fig:depth-map-analysis}) in the context of spatial syntax theory is concerned with the topological depth of urban structures or architectural objects. The depth is described by the number of elements that must be traversed to reach a given point in the described system. In this framework, it is therefore a topological distance as opposed to a geometric one. The method is usually applied to topological graphs, but can also be used to create a general depth map of the body to be described, in which case it can also be used for geometric depth evaluations.

\begin{figure}
\centering
\begin{subfigure}{.5\textwidth}
\centering
\includegraphics[width=\linewidth]{assets/preliminaries/topology/space_syntax/rotonda_depthmap.pdf}
\caption{La Rotonda}
\label{fig:la-rotonda-5}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
\centering
\includegraphics[width=\linewidth]{assets/preliminaries/topology/space_syntax/savoye_depthmap.pdf}
\caption{Villa Savoye}
\label{fig:villa-savoye-5}
\end{subfigure}
\caption{Depth Map Analysis}
\label{fig:depth-map-analysis}
\end{figure}

\paragraph{Space Integration}\label{par:space-integration}

Based on the creation of an \gls{axial map}, the accessibility of certain paths or elements (figure \ref{fig:closeness-centrality-1}) can be detected, calculated and visualised by applying the concept of \textit{space integration}. This method consists in calculating the degree of accessibility at each element of an axial graph by considering how many elements must be crossed to reach the analysed entity. Again, different units of distance can be considered, such as topological distance, metric distance (figure \ref{fig:path-length}), or even angular distance, which considers how many rotations an individual goes through to get from any point to the point or element being investigated.

\paragraph{Patterns}\label{par:patterns}

The discussion of the role of recurring \textit{patterns} in architecture, introduced by Christopher Alexander\footcite{alexander1977pattern}, has attracted considerable attention in architectural theory and has occurred concurrently with the development of spatial syntax theory. In this respect, it is hardly surprising that his description and meticulous definition of certain patterns, which follow clear and predictable rules, have also found their way into space syntax methods. In this context, these patterns can refer to the spatial structure, the social organisation or the functions of an environment. In particular, the so-called social patterns are considered, which refer to the patterns resulting from the social interaction and behaviour of people in certain spatial contexts. Each social pattern comes with its own characteristics and needs and, once identified, can help the designer to shape the space accordingly.

\subsection{Shape Grammar}\label{subsec:shape-grammar}

The so-called \textit{shape grammar} (figure \ref{fig:shape-grammar-process}) is a formal methodology for the automated generation of architectural bodies in mainly two- and three-dimensional spaces, and has gained importance in architectural theory since the 1970s\footcite{hong2022five}. The basic operation of the method is based on the assumption that certain elements can be combined through established rules to generate various architectural objects and their derivatives (figure \ref{fig:variations}). The theory thus assumes that the formal aspects of architecture follow certain regularities which are defined by a finite number of rules and thus become part of a formal design language. By applying this grammatical syntax to specific architectural forms, a multitude of different variants of a morphologically similar aggregate of bodies can be generated, all following the same generative rules, as long as these rules allow for a certain degree of variation.

As the terminology of the two concepts, shape grammar and space syntax, suggests, their combination is complementary at a theoretical level and, when skilfully applied, allows a qualitative architecture to emerge. The architectural rules needed to make the algorithm work can be quite diverse, but should always start with an initial geometric \textit{ground rule} (figure \ref{fig:initial-shape}) and be terminated by a \textit{final rule}. Typically, the initial rule is the definition of a simple geometric shape, and subsequent rules (figure \ref{fig:rule}) are either \textit{transformation rules} or \textit{parametric rules}. Examples of transformation rules are rotation of certain added or initial bodies, translations, reflections, scaling or Boolean transformations. Parametric rules allow a greater relationship to the geometric context of the transformations, as parameters can be defined and varied depending on other subfigures or constellations.

\begin{figure}
\centering
\begin{subfigure}{.2\textwidth}
\centering
\includegraphics[width=\linewidth]{assets/preliminaries/topology/initial_shape.pdf}
\caption{Initial Shape}
\label{fig:initial-shape}
\end{subfigure}%
\begin{subfigure}{.2\textwidth}
\centering
\includegraphics[width=\linewidth]{assets/preliminaries/topology/rule.pdf}
\caption{Rule}
\label{fig:rule}
\end{subfigure}%
\begin{subfigure}{.6\textwidth}
\centering
\includegraphics[width=\linewidth]{assets/preliminaries/topology/result.pdf}
\caption{Variations}
\label{fig:variations}
\end{subfigure}
\caption{Shape Grammar Process}
\label{fig:shape-grammar-process}
\end{figure}

\subsection{Space Partitioning}\label{subsec:space-partitioning}

For nearly a century, a subfield of architecture and design theory has been concerned with the research and development of algorithms that enable the generation of a two- or three-dimensional spatial layout by partitioning or accumulation that follows desired regularities. This area of research is often referred to as automated floor plan generation\footcite{nisztuk2019hybrid}. The shape grammar language mentioned above is part of this search for algorithm-based spatial partitioning. In the context of this work, a close look at each of the established and less common methods, as well as their combination, is essential, since a basic geometric space partitioning is elementary for the generation of an architecturally qualitative dataset, as demonstrated in the chapter \ref{chap:synthetic-dataset-generation}.

The initial conditions, the input, for the correct functioning of the various methods can be very different and are therefore an elementary part of the consideration, evaluation and comparison of the presented algorithms. The rule-based structure of the shape grammar differs significantly from the simpler geometric algorithms, which require as a starting point a defined two- to three-dimensional body, percentages of space or even a random seed. In addition, there are a number of methods that start with certain geometric bodies with given dimensions and then proceed to combine them in various ways to form an agglomerate.

\paragraph{Grid Planning}\label{par:grid-planning}

A relatively simple way to partition a desired space under certain conditions is the grid-based planning method (\textit{Grid Planning}). In this method, the initial surface or body to be partitioned is covered with a uniform or varying grid (figure \ref{fig:grid}) that is more or less  closely meshed with respect to the total area and the desired number of partitions. This method further requires information on the programmatic and topological relationships of the individual units in order to make compositional decisions and compare different arrangements.

In the case of automatic floor planning, the individual rooms to be arranged are assigned different values for their connections to each other, similar to a weighted graph, introduced in figure \ref{fig:weighted-graph}. In order to place the individual spaces in the defined grid in the best possible way, the desired connection between them is respected as much as possible, be it Boolean values in the case of topological relationships or local distance values. This defines the basic framework of the grid-based planning method\footcite{lopes2010constrained}, to which different algorithms for optimised space placement can now be applied.

\begin{figure}
\centering
\begin{subfigure}{.25\textwidth}
\centering
\includegraphics[width=\linewidth]{assets/preliminaries/topology/space_partitioning/grid_graph.pdf}
\caption{Weighted Graph}
\label{fig:weighted-graph}
\end{subfigure}%
\begin{subfigure}{.25\textwidth}
\centering
\includegraphics[width=\linewidth]{assets/preliminaries/topology/space_partitioning/grid.pdf}
\caption{Grid}
\label{fig:grid}
\end{subfigure}%
\begin{subfigure}{.25\textwidth}
\centering
\includegraphics[width=\linewidth]{assets/preliminaries/topology/space_partitioning/grid_assign1.pdf}
\caption{Iteration}
\label{fig:iteration}
\end{subfigure}%
\begin{subfigure}{.25\textwidth}
\centering
\includegraphics[width=\linewidth]{assets/preliminaries/topology/space_partitioning/grid_assign2.pdf}
\caption{Final Placement}
\label{fig:final-placement}
\end{subfigure}
\caption{Grid Planning}
\label{fig:grid-planning}
\end{figure}

First, spaces can be placed through an iterative process starting with a randomly selected or predetermined space (figure \ref{fig:iteration}). In the following steps, the rooms are gradually placed according to their relationship priority. This continues until finally all rooms have been placed (figure \ref{fig:final-placement}). Although this method has a higher probability of finding an optimal space allocation than a random placement method, there is a risk that certain decisions during the process may prevent an optimal solution from being found.

The \textit{random swap} algorithm\footcite{nagy2021ai} has proven to be a better alternative to the \textit{iterative assignment} method. Starting from a random placement of all rooms, this algorithm swaps two room positions in iterative steps and checks whether the general \textit{\gls{fitness function}}, which is the sum of all distances, is minimised. This method represents a \textit{\gls{greedy algorithm}}\footcite{nisztuk2019hybrid} because it only evaluates per iteration whether the action just performed optimises the overall result, but without having a general overview over several iterations. Using the random swap algorithm, optimised results can be obtained compared to the iterative assignment method. However, the latter is significantly more time-consuming and computationally expensive.

\paragraph{Subdivision}\label{par:subdivision}

While the grid-based planning method locks the final layout to the position and dimensions of the defined grid, a subdivision method can be used to iteratively structure a space by freely placing walls within a defined boundary, similar to the manual architecture method. This family of partitioning methods consists mainly of relatively simple algorithms that differ only in the way the walls are placed. However, they all have as a basic requirement the definition of an initial shape which, in this concrete context, represents the outline of the floor plan to be subdivided.

The first and most intuitive method is to divide the basic body (figure \ref{fig:rectangle}) in an arbitrary or defined direction, creating two partial surfaces (figure \ref{fig:first-iteration}). This process is repeated until the desired number of faces is obtained (figure \ref{fig:second-iteration}). The basic concept of \textit{recursive subdivision} is a \gls{binary tree} which has a starting point, the base body, and two child leaves representing the two resulting faces. This is a fast and effective algorithm, taking only linearly more time as the number of spaces increases. However, this method of space division is very limited as it does not take into account any definition of room, size or percentage.

\begin{figure}
\centering
\begin{subfigure}{.33\textwidth}
\centering
\includegraphics[width=\linewidth]{assets/preliminaries/topology/space_partitioning/subdivision1.pdf}
\caption{Base}
\label{fig:rectangle}
\end{subfigure}%
\begin{subfigure}{.33\textwidth}
\centering
\includegraphics[width=\linewidth]{assets/preliminaries/topology/space_partitioning/subdivision2.pdf}
\caption{First Iteration}
\label{fig:first-iteration}
\end{subfigure}%
\begin{subfigure}{.33\textwidth}
\centering
\includegraphics[width=\linewidth]{assets/preliminaries/topology/space_partitioning/subdivision3.pdf}
\caption{Second Iteration}
\label{fig:second-iteration}
\end{subfigure}
\caption{Recursive Subdivision of a Rectangle}
\label{fig:recursive-subdivision-of-a-rectangle}
\end{figure}

A slightly modified form of this recursive subdivision are the so-called \textit{k-dimensional tree} algorithms\footcite{knecht2010generating} (K-D tree algorithms), which subdivide the space by iteratively placing walls in a similar way to the previous algorithm. However, the placement of the intersections is not arbitrary. First, points are placed on or in the two- or three-dimensional body representing the position of each room. These sets of points are then bisected by a wall along their larger extension in the middle, so that the same number of points are in both subspaces. This is repeated iteratively until there is only a single point in each subdivided space. The direction of the wall placed per iteration is variable and is determined by the geometric depth of the set of points. Due to its logic, this algorithm is only minimally slower than the previous one, requires only negligibly more computing power, and is more suitable for automated floor planning than the recursive subdivision method due to the possibility of controlling room sizes and placement through the position of the aforementioned points. Examples of the k-d tree method are given in the section \ref{par:k-dimensional-tree}.

Another method, also based on the \gls{binary tree} construction, is the \textit{squarified treemap} algorithm\footcite{marson2010automatic}, which has the significant advantage of being able to define the percentages of the room sizes in advance. However, unlike the recursive subdivision and k-dimensional tree algorithms, this method is only suitable for rectangular initial shapes. Further explanations can be found in the section \ref{par:squarified-treemap}. 

Methods that are not based on a \gls{binary tree} structure include so-called \textit{Voronoi diagrams}\footcite{coates2005generating}. Similar to the k-dimensional tree algorithms, points are projected onto the surfaces to be subdivided, representing the position of each space. By creating a \textit{Delaunay triangulation} of these points and then dividing the centers of the edges of the mesh by orthogonal lines, Voronoi cells can be constructed around the origin points. These denote the region that is geometrically closer to the Voronoi seed than to any other point on the shape. This method of subdivision has the significant advantage that the initial body can be of any shape, thus providing great variability in layout generation. This method is described in detail in section \ref{par:voronoi-diagram}. However, a disadvantage of conventional Voronoi diagrams is the lack of control over the area of each Voronoi cell. To overcome this drawback, \textit{weighted Voronoi diagrams}, also called \textit{Laguerre Voronoi diagrams}\footcite{anuradha2008voronoi}, can be adopted. They allow a weight to be assigned to each Voronoi cell, thus influencing the size of the area of each cell in relation to the other cells.

\paragraph{Aggregation}\label{par:aggregation}

A striking characteristic of both the grid-planning and the subdivision methods is that both require a predetermined initial shape. This is advantageous for certain automatic floor planning tasks, but it can also be limiting as it restricts design variation. The family of \textit{aggregation methods} overcome this limitation by being structured in such a way that only the shapes of the individual rooms need to be defined in advance (figure \ref{fig:initial-configuration}). As the name suggests, aggregation methods\footcite{as2021routledge} attempt to create an agglomerate (figure \ref{fig:final-layout}) of the predefined room shapes by applying different agglomeration functions.

To achieve the best possible result, evaluation formulas and conditions have to be created, which can be optimised by shifting the individual spaces (figure \ref{fig:iteration-1}) using various algorithms. For example, a \gls{fitness function} can be created that tries to minimise the distance between the individual areas without overlapping them. This can be done by specifying different strengths of relationships between the rooms, if appropriate. This \gls{fitness function} can then be used as a constraint for an \textit{evolutionary algorithm}, which, similar to the biological concept, creates populations that influence different parameters and go through natural selection processes, mutations and crossovers per iteration to find the best possible position of each room, represented by the highest degree of optimisation of the \gls{fitness function}.

\begin{figure}
\centering
\begin{subfigure}{.33\textwidth}
\centering
\includegraphics[width=\linewidth]{assets/preliminaries/topology/space_partitioning/aggregation1.pdf}
\caption{Initial Configuration}
\label{fig:initial-configuration}
\end{subfigure}%
\begin{subfigure}{.33\textwidth}
\centering
\includegraphics[width=\linewidth]{assets/preliminaries/topology/space_partitioning/aggregation2.pdf}
\caption{Iteration}
\label{fig:iteration-1}
\end{subfigure}%
\begin{subfigure}{.33\textwidth}
\centering
\includegraphics[width=\linewidth]{assets/preliminaries/topology/space_partitioning/aggregation3.pdf}
\caption{Final Aggregation}
\label{fig:final-layout}
\end{subfigure}
\caption{Aggregation Process}
\label{fig:aggregation-method}
\end{figure}

Similar to the evolutionary algorithm, \textit{simulated annealing} can also be used as an optimisation strategy, as it can likewise find an optimal arrangement by iteratively evaluating the initially randomly selected solution of the parameters of the function to be optimised. This process, like the evolutionary approach, does not guarantee that the optimal spatial arrangement will be found. However, both methods have a higher probability of finding the global maximum of the functions to be optimised.

Another method that allows for some autonomy in the room arrangement is the use of \textit{agent-based optimisation}\footcite{guo2017evolutionary}. Here, the individual rooms are initialised as agents acting independently of each other, so that each individual room evaluates its own position and orientation in relation to its environment. This evaluation takes place iteratively and in parallel until the desired final condition is reached. More information on agent-based optimisation can be found in the section \ref{par:agent-based}.

Despite their advantages, the methods mentioned so far all have the common drawback of being time-consuming and particularly computationally intensive due to their iterative computation structure. Another possibility for aggregation is the creation of a physical attraction model by \textit{spring force}, where the rooms are connected to each other in a predetermined way by physical forces and thus a desired space aggregation is generated at the end of the simulation, as long as they were determined to be fixed, impenetrable rooms. However, the main disadvantage of this method remains the high requirement of computing power and that there is no guarantee that the final result will be free of gaps. It must therefore be complemented by a displacement algorithm that tries to minimise the gaps in subsequent steps.

Last but not least, methods from the family of \textit{shape-packing} algorithms can also be used in the aggregation strategy. Here, the individual rooms are shifted from a randomly chosen initial arrangement in such a way that the area they occupy is minimised. However, even in this case, gapless layout generation is far from being guaranteed.

\section{Simulation}\label{sec:simulation}

During the design process of any architectural concept, the attentive designer is concerned with the interaction between inanimate matter and its environment. In order to evaluate this interaction in a sound and correct way, a number of different methods have been developed that allow the final or preliminary design to be tested in real scenarios in order to evaluate its performance. This discipline is summarised under the concept of \textit{building simulation} in architectural design.

In terms of this definition, the best known simulation technique, which is inevitably familiar to any designer or observer, is the representation of architectural projects in perspective space. Here, the abstract constructive element adapts to the perspective perception conditioned by the human eye, thus attempting to represent a simulation of the physical situation. However, when talking about simulations in the architectural context, one usually refers to more complex methods that imply either physical simulations, numerical simulations, virtual, real-time or even agent-based simulations. In the context of this thesis, the focus lies on physical and agent-based simulations, where the results can be either \gls{stochastic} or \textit{\gls{deterministic}}. A variety of various simulation techniques with different variables can be carried out to provide detailed insights into the respective environmental behaviour. A list and description of available \gls{open source} simulation software can be found in section \ref{subsec:aec-software-and-efforts}.

\paragraph{Light Simulations}\label{par:light-simulations}

This simulation method is primarily concerned with simulating the photons of natural light emitted by the sun, which manifest their interaction with our physical world as solar radiation and can be measured in \textit{lux} (figure \ref{fig:light-simulation-lux-villa-savoye}). A variety of different values and measurements can be calculated, such as the \textit{\acrfull{df}}, the \textit{\acrfull{udi}}, the \textit{\acrfull{da}} or even the \textit{\acrfull{gf}}. In this way, the positioning and dimensioning of windows and the choice of solar shading systems can be optimised\footcite{aksin2021use}. However, light simulations also allow the simulation of illumination and its interaction with matter from artificial light sources. In addition, the absence of light or the shadow cast by a particular building shape at a particular time and day of the year can be simulated in a similar way (figure \ref{fig:shadow-simulation}).

\begin{figure}
\centering
\includegraphics[width=0.93\textwidth]{assets/preliminaries/simulation/light_lux.pdf}
\caption{Light Simulation Villa Savoye in lux}
\label{fig:light-simulation-lux-villa-savoye}
\end{figure}

\begin{figure}
\centering
\begin{subfigure}{.5\textwidth}
\centering
\includegraphics[width=\linewidth]{assets/preliminaries/simulation/shadow_1_may_13h30.pdf}
\caption{1 May 13h30}
\label{fig:1-may-13h30}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
\centering
\includegraphics[width=\linewidth]{assets/preliminaries/simulation/shadow_1_nov_13h30.pdf}
\caption{1 November 13h30}
\label{fig:1-november-13h30}
\end{subfigure}
\caption{Shadow Simulation}
\label{fig:shadow-simulation}
\end{figure}

\paragraph{Acoustic Simulations}\label{par:acoustic-simulations}

This method of physically simulating the acoustic properties of defined spaces involves calculating the interaction of sound waves with the media in which they propagate. Either a sender object is defined in advance whose emitted sound waves are to be analysed, such as a professor in an auditorium, or artificial sender objects are created to simulate sound transmission through certain materials, such as a neighbour's footsteps and their sound transmission through the floor to the apartment below.

\paragraph{Vibration and Earthquake Simulations}\label{par:vibration-and-earthquake-simulations}

Material properties in the presence of vibration are not only of interest in the context of sound propagation, but can also provide information about general physical properties of the overall structure. Vibration and seismic simulations are an essential method of physical simulation for evaluating design decisions, as resistance to different levels of vibration can be calculated by changing the structure of the static frame or its material composition. For example, in geographical regions where earthquakes occur regularly, this type of simulation can provide answers to important questions about the stability and load-bearing capacity of the proposed structure.

\paragraph{Fire Simulations}\label{par:fire-simulations}

The propagation of fires and the changes in essential material properties under extreme heat are difficult to calculate mathematically, because the interaction of many different factors, such as the geometric structure of the building, the air density, the material composition and the connection of individual elements and their spatial proximity, significantly determine how the potential fire would affect the architectural object. Fire simulations are therefore the most reliable way of analysing a building from a fire safety point of view.

\paragraph{Rain and Wind Load Simulations}\label{par:rain-and-wind-load-simulations}

As buildings are inevitably exposed to natural forces, thorough calculations of the interactions between these forces and architectural objects are required. Although rain and snow loads are often calculated using mathematical equations rather than complex simulations, wind forces are difficult to calculate in a simple mathematical way. For particularly high buildings, the horizontal wind load can even be considered a limiting factor and must therefore be simulated and analysed as accurately as possible according to the environment (figure \ref{fig:wind-simulation}). In almost all wind and rain simulations, it is necessary to take into account and model the immediate urban or natural environment, as certain wind corridors can form, which can cause unexpected horizontal forces.

\begin{figure}
\centering
\includegraphics[width=\textwidth]{assets/preliminaries/simulation/airflow.pdf}
\caption{Wind Simulation}
\label{fig:wind-simulation}
\end{figure}

\paragraph{Airflow Simulations}\label{par:airflow-simulations}

In order to provide a comfortable microclimate within a designed building, or to prevent possible health risks due to reduced fresh air supply, general airflow simulations within the building need to be carried out. This involves simulating the medium of air and its movement through each room of the building, calculating the rate at which a certain percentage of existing air in a room is replaced by new incoming air, or even which method of ventilating the whole building requires the least amount of energy. Of course, the heating energy lost to incoming cold air also plays a role in the simulation, but it is primarily an important aspect of energy performance simulations.

\paragraph{Load-Bearing Simulations}\label{par:load-bearing-simulations}

Almost every architectural creation is subject to forces based on Newton's laws of physics, essentially influenced by the earth's gravitational pull. In this respect, architecture, as an engineering profession, is more closely related to the constant consideration of the forces at work than any other activity. In this context, it is not surprising that methods for simulating the load-bearing capacity of individual materials, elements and even the entire structure are among the elementary tools of architectural practice.

\paragraph{Building Condition Simulations}\label{par:building-condition-simulations}

This family of physical simulation methods has a special place in simulation methodology because it is one of the few methods that takes a largely four-dimensional approach. Building condition simulations look at how individual materials change and possibly degrade over time to make predictions about the life cycle of individual components or even the entire structure. Furthermore, due to its temporal aspect, this simulation method can be integrated into the analysis of the material cycle, which provides information on the possibility of recycling certain materials once they have reached the end of their life.

This list of different physical simulations represents the principal methods for simulations in everyday architecture and engineering, but can be complemented by a variety of other simulations. Agent-based methods are less common simulation techniques, but can also provide valuable information about the performance of designed architectural objects.

\paragraph{Building Occupancy Simulations}\label{par:building-occupancy-simulations}

A specific simulation that can have significant application in architecture is the creation of agents that represent the occupants of a building or the visitors to a public building. The individual agents are given defined behavioural patterns and simulated using \gls{stochastic} methods in an architectural context. The interesting aspect of such agent-based methods is that the individual actors can exhibit new, unexpected behaviours among themselves and in interaction with the architectural framework, which can indicate design shortcomings or confirm or refute certain spatial and programmatic design intentions.

\paragraph{Network Flow Simulations}\label{par:network-flow-simulations}

This method simulates the interaction of specific actors in a network in order to observe different patterns of behaviour in their interaction within the context of the given network topology. A widely used application in the urban context is traffic flow simulation, where the behaviour of vehicles and other road users is observed and analysed in the context of the given road infrastructure. In architectural applications, it can be the graphical modelling of public buildings, where certain agents go about their daily activities in and around the building, thus revealing possible bottlenecks in the spatial configuration.

\paragraph{Safety and Evacuation Simulations}\label{par:safety-and-evacuation-simulations}

Another important application of agent-based simulations is to project the behaviour of crowds under specific circumstances. In this context, safety and evacuation simulations assign general behaviours to individuals that correspond to emergency situations in the simulated scenario. As those type of situations are influenced by a large number of different variables, the use of \gls{stochastic} methods is necessary. The results of such simulations can therefore provide important insights into the optimal circulation within a building in evacuation situations.

\paragraph{Crowd Simulations}\label{par:crowd-simulations}

The behaviour of large crowds differs in some respects from the way individuals respond, as some patterns of human behaviour are only exhibited when a certain number of people are present in and around the space. By defining the individuals of a crowd as distinct agents, the difficult-to-predict behaviour due to the interaction of the single agents can be represented and analysed. This provides formal and topological information about the performance of the designed architecture in relation to its use of space.

Typically, the aforementioned simulations are performed in advanced design phases of the project, or even after the design process has been completed, since in most cases they are elaborate processes which, in their conventional form, are not economically suitable for integration into an iterative process. This implies that, in reality, any shortcomings discovered during the simulation stages are often costly and difficult to correct in the architectural design\footcite{ali2023architectural}.

\subsection{Energy Performance}\label{subsec:energy-performance}

Energy simulation is a key tool in architecture for investigating and optimising the energy efficiency of buildings (figure \ref{fig:energy-simulation}). With growing global energy challenges due to climate change and limited resources, such simulations are becoming increasingly important and are commonly referred to as \textit{\acrfull{bem}}. They use complex mathematical models and algorithms to analyse the energy performance of buildings, taking into account factors such as climate, geometry, material properties, use and energy transfer. The data collected is converted into \textit{energy balances} to predict the behaviour of the building and identify weak points. By optimising the energy performance of buildings, significant energy savings can be achieved, bringing financial benefits and helping to reduce energy consumption and CO2 emissions. This type of simulation allows architects and designers to test different scenarios and optimise the energy performance of buildings before they are actually built. In practice, they are increasingly being used to make buildings more sustainable and energy efficient, both for new buildings and for the renovation or conversion of existing buildings.

\begin{figure}
\centering
\begin{subfigure}{.5\textwidth}
\centering
\includegraphics[width=\linewidth]{assets/preliminaries/simulation/sensible_cooling.pdf}
\caption{Sensible Cooling}
\label{fig:sensible-cooling}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
\centering
\includegraphics[width=\linewidth]{assets/preliminaries/simulation/sensible_heating.pdf}
\caption{Sensible Heating}
\label{fig:sensible-heating}
\end{subfigure}
\caption{Energy Simulation}
\label{fig:energy-simulation}
\end{figure}

Energy simulation in architecture can be performed in a variety of ways, including dynamic, thermal and structural simulations. Each type of simulation requires different input data to obtain accurate results. Typically, data such as building geometry, material properties, climate data and building occupancy data are required. The resulting values vary according to the type of simulation. For example, dynamic simulations can produce energy consumption projections for the building on a daily, weekly or annual basis. Thermal simulations, on the other hand, can map the temperature distribution in the building and the heat output from heating and air conditioning systems. Structural simulations can calculate loads on structural elements and the load-bearing capacity of materials. Despite the benefits of energy simulation, there are a number of drawbacks and limitations.

One of the biggest challenges is obtaining accurate input data\footcite{hauck2017energy}, as material properties and actual building use are often unclear or difficult to obtain. In addition, simulations can be time-consuming and require powerful computers, specialised software and trained personnel to operate them\footcite{chatzivasileiadi2018effect}.

The results of energy simulations are usually presented in numerical form, such as charts, graphs or tables and may have different units depending on the values simulated: for example, energy consumption projections are usually measured in kilowatt-hours ($kWh$) or even megajoules ($MJ$), while temperatures are expressed in degrees Celsius ($^{\circ}C$), Fahrenheit ($^{\circ}F$) or Kelvin ($K$).

\subsection{Optimisation}\label{subsec:optimisation}

Simulation-based optimisation is a relatively recent but growing area of architectural design tools\footcite{sebestyen2020machine}, as it allows practitioners to test and manually or automatically optimise different design scenarios before they are realised. The aim of this simulation-based optimisation is to adjust and vary certain parameters, such as dimensions, materials, shapes, rotations and general transformations, so that the simulated evaluation function achieves its optimal performance\footcite{aksin2021use}. An intuitive approach to optimisation through simulation in architecture is manual parametric testing, which allows architects to evaluate the overall design through simulations by varying specified parameters. Iterative processes and parameter adjustments are then used to achieve the best possible result.

The advantage of this optimisation strategy is that once the parametric model has been defined, little expertise is required on the part of the designer to optimise it. However, this approach is extremely time and computation intensive and does not guarantee optimal results due to its intuitive logic. As a result, over the years many automatic optimisation algorithms and methods, most of which have their origins in mathematics or computer science, have proved useful in architectural optimisation applications.

\paragraph{Deterministic Optimisation Methods}\label{par:deterministic-optimisation-methods}

\Gls{deterministic} optimisation methods have become an indispensable part of today's scientific world. They are based on the application of mathematical models to find an optimal solution to a given problem (figure \ref{fig:gradient-descent-optimisation}). These methods offer a variety of techniques, such as the \textit{gradient descent method}, the \textit{Newton-Raphson method}, the \textit{conjugate gradient method} or even the \textit{quasi-Newton method}. The gradient descent method is a well-known algorithm\footcite{zhang2021dive} used in many optimisation applications. It is particularly effective in finding a solution that minimises the value of a given function. It can converge quickly even in complex cases and is easy to implement. The Newton-Raphson and Conjugate Gradient methods are useful for large and complex problems. They can find solutions accurately in a short amount of time, using \textit{local Hessian matrix} information. The Conjugate Gradient method is especially useful for \textit{symmetric matrices}, while the Newton-Raphson method is good for non-linear functions. The quasi-Newton method is similarly an effective method for optimising non-linear functions, but is based on the approximation of the Hessian matrix by a \textit{positive definite matrix}. The method is known for its fast convergence and ability to work well in practice.

\begin{figure}
\centering
\begin{subfigure}{.33\textwidth}
\centering
\includegraphics[width=\linewidth]{assets/preliminaries/simulation/optimization1.pdf}
\label{fig:optimisation-1}
\end{subfigure}%
\begin{subfigure}{.33\textwidth}
\centering
\includegraphics[width=\linewidth]{assets/preliminaries/simulation/optimization2.pdf}
\label{fig:optimisation-2}
\end{subfigure}%
\begin{subfigure}{.33\textwidth}
\centering
\includegraphics[width=\linewidth]{assets/preliminaries/simulation/optimization3.pdf}
\label{fig:optimisation-3}
\end{subfigure}
\caption{Gradient Descent Optimisation}
\label{fig:gradient-descent-optimisation}
\end{figure}

\paragraph{Stochastic Optimisation Methods}\label{par:stochastic-optimisation-methods}

\Gls{stochastic} optimisation methods provide an alternative approach to solving optimisation problems, and are particularly useful for solving complex tasks. Unlike \gls{deterministic} methods, they use probabilistic models to find an optimal solution. Random processes are used to search for an optimal solution. There are several methods of \gls{stochastic} optimisation, such as simulated annealing, \textit{particle swarm optimisation}, genetic algorithms and \textit{\Gls{bayesian optimisation}}. Simulated annealing is a method that uses random search to find an optimal solution. In this process, an energy level is calculated and reduced as the optimal solution is gradually approached. Particle Swarm Optimisation and Genetic Algorithms use populations and evolution to find an optimal solution. Particle Swarm Optimisation simulates the behaviour of swarms or groups of particles to collectively find an optimal solution. The position of each particle is adjusted at each iteration to find the function maximizing variables. Genetic algorithms are based on the idea of natural selection and evolution. Solutions are represented in the form of \textit{chromosomes} and evolve through \textit{crossover} and \textit{mutation}. Through continuous evolution of the population, a better solution is gradually found. \Gls{bayesian optimisation} uses Bayesian statistics to find an optimal solution. It involves building a probability model that maps the relationship between the input and output variables. By combining \textit{prior} and \textit{likelihood functions}, the optimal solution is calculated.

\paragraph{Machine Learning Optimisation Methods}\label{par:machine-learning-optimisation-methods}

Machine learning based optimisation methods represent another approach to optimisation and use algorithms and models from computer science to find an optimal solution for a defined or even undefined function\footcite{sebestyen2020machine}. These methods usually use the largest possible amount of data to train the model to make the best decisions. There are several machine learning methods for optimisation, such as \textit{stochastic gradient descent}, \textit{reinforcement learning} and \textit{artificial neural networks}. Stochastic Gradient Descent is a method that uses gradient descent techniques in combination with random processes to find an optimal solution. In this process, a \textit{cost function} is minimised step by step until the optimal solution is reached. Reinforcement learning is a learning technique in which the system learns how to optimise its actions by interacting with the environment. The system receives feedback in the form of rewards or punishments for its actions and adjusts its strategy accordingly. In this way, the system learns how to optimise its actions and achieve its goals. Artificial neural networks are a well-known method based on modelling the neuronal structure of the human brain, which makes them particularly suitable for processing large amounts of data. Like other methods, neural networks can be used for optimisation by creating a model of the function to be maximised or minimised. A particular feature of neural networks is the creation of a \textit{predictive function} by training the neural network to find the optimal solution.

\chapter{State of the Art}\label{chap:state-of-the-art}

The following overview of significant academic research, studies and projects related to the topic of this thesis is divided into four main areas: Graphs and Topology in Architecture, Feedback in Early Design Stages, Architecture and Machine Learning, and Synthetic Architecture Datasets. Each of these topics is further subdivided into several subtopics in order to simplify and structure the reading. It is important to note, however, that a large number of the references described do not easily fit into a single category, as most of the works listed demonstrate a methodology that combines several topics, such as machine learning, topology and automatic floor planning for instance. The literature listed below represents essential sources for the elaboration of the topics covered in this thesis. Further studies and research on the subjects covered are listed and evaluated in the section \ref{sec:further-readings} in the appendix.

\section{Graphs and Topology in Architecture}\label{sec:graphs-and-topology-in-architecture}

A renewed interest in topological analysis approaches in architecture, as well as an intensive engagement and research in the field of graph-theoretic methodology in data and computer science in the last decades, has led to a variety of different research and projects at the interface of both disciplines. The application of topological graphs and their inherent methods to the \acrshort{aec} industry has led to new design approaches and analysis techniques that have the potential to significantly enrich architectural thinking. This section reviews the state of the art of graphs and topology in the architectural context by referring to the most relevant articles, books and experiments on and around these subjects.

\subsection{Graph Theory in Architecture}\label{subsec:graph-theory-in-architecture}

The theoretical field of graph theory has found diverse applications in architecture, particularly in the analysis of spatial structure, layout and organisation. The beginning of the study of graphical relations and mathematics can be traced back to Euler in the year 1736. In the subsequent centuries multiple fundamental research has been conducted around the topic. One of these contributions is the book \citetitle{wilson1979introduction} by \citeauthor{wilson1979introduction} in which he introduces the concepts of graph theory through examples that define the structure and components of graph networks and explain special cases and methods such as paths, cycles, trees, planarity, digraphs and \textit{colouring}. This lecture proved to be a valuable source of information for an introduction to theorems and methods of graph analysis. In the same year \citeauthor{earl1979architectural} in their book \citetitle{earl1979architectural} deal with the application of graphs to two-dimensional architectural floor plans in order to analyse and evaluate the connection of spaces and their arrangement in the spatial layout design. Here, graphs are applied to the architectural example in two different ways: on the one hand, they take the form of programmatic-organisational information and, on the other hand, they represent the structure and connection of the wall elements represented by the edges of the graphs, whereby the empty area thus created within the graph describes the rooms of the architectural object.

On a technical level, starting from a \acrshort{bim} model, the study \citetitle{van2004analysis} by \citeauthor{van2004analysis} derives four different types of graphs: a structural element graph, a graph connecting room surfaces, a room adjacency graph and a graph representing object relations. The relationship between them is explained and the use of these networks as input for successful energy simulations is demonstrated. Due to the interest raised by the combination of graph theory and architectural practice, parallel research was carried out in several areas at the beginning of this century. In this context and with a more interdisciplinary approach, \citeauthor{franz2005graph} in \citetitle{franz2005graph} connect the broad field of psychology with architecture by applying specific graph-based computational methods. The research includes spatial cognition-based graphs such as the \textit{occupancy grid}, \textit{place graph} and \textit{view graph}, as well as architecture analysis-based graphs such as the \textit{access graph}, \glspl{axial map}, \glspl{isovist} and \acrlong{vga}. The aim of this work is to demonstrate the application of spatial cognition methods as design tools in architecture. Furthermore, the paper \citetitle{dawes2013applications} by \citeauthor{dawes2013applications} adds to this psychologically oriented approach by critically examining three essential methods of spatial syntax, namely axial line, convex space and \glspl{isovist}, through architectural examples. He suggests ways to extend the established, abstract and geometry-independent methods of analysis. The analysis tools developed aim to reconcile space syntax theory with geometric and formal information.

The technical approach to incorporating graph-theoretic concepts into \acrshort{aec} was further explored by \citeauthor{isaac2013analyzing} who, with \citetitle{isaac2013analyzing}, provided a basis for integrating graph-theoretic analysis methods into \acrshort{bim} models through a thorough study of \acrshort{ifc} files and their ability to be synthesised into semantic graphs that could in turn describe the hierarchical relationships of architectural elements in the model. The authors further discuss significant advantages such as the representation and mathematical analysis of abstract information through computational graph algorithms. Another technical implementation of graph concepts is documented in \citetitle{boguslawski2016two} by \citeauthor{boguslawski2016two}. This paper develops \acrshort{bim} graph retrieval methods for the coordinated emergency response of rescue teams and calculates safe routes, shortest paths and fire spread within the model using graph analysis methods. The different types of graphs used in this study are summarised under the concept of '\textit{variable density networks}'.

To conclude the general subject of the synergy between graph theory and architecture, \citeauthor{nourian2016configraphics} provides a general research on the application of graph theoretical concepts in architectural design practice entitled \citetitle{nourian2016configraphics}. The motivation of this work was to provide designers with a spatial configuration-based design practice that complements traditional formal research. Within this framework, the configurative design toolkit '\textit{Syntactic}' was developed for the application of graph analysis in the field of \acrshort{aec}.

\subsection{Topology and Space Syntax}\label{subsec:topology-in-architecture}

Similarly, topological analysis methods have found numerous applications in architecture over the last century, particularly in relation to form finding, design optimisation and spatial analysis. One of the fundamental literature in this field is the research of \citeauthor{kelley1955general} published in his book \citetitle{kelley1955general} of \citeyear{kelley1955general}. This document represents an essential resource in the field of mathematical topology as it thoroughly explains the fundamentals and advanced methods related to the concept of topology. Of particular importance were the explanations of elementary topics and algebraic concepts such as sets, relations and \Glspl{boolean operation} such as union and intersection. Thirty years later, \citeauthor{baglivo1983incidence} in their work, \citetitle{baglivo1983incidence} extensively discuss the connection of graph-theoretical concepts in the design context through topological methods. The book begins by examining geometric symmetry through the application of graph-theoretic methods, then progressively delves into topological transformations, ultimately relating them back to graph-theoretic concepts.

Space significantly influences social structures and behavioural patterns, and society in turn has a considerable impact on its physical environment. This interaction has been intensively analysed in the foundational book \citetitle{hillier1989social} by \citeauthor{hillier1989social}, which explores the influence of buildings and urban structures on the formation of social patterns. It further develops tools that combine social and cultural approaches with constructive ones, such as the methods of analysis collectively known as \textit{space syntax}. Following this publication, technical implementations of the theory introduced were carried out in \citeyear{li2009design} by \citeauthor{li2009design}, among others. The paper, \citetitle{li2009design}, presents and discusses the implementation of a space syntax analysis tool during the initial design phases. The developed feedback and analysis software, called '\textit{Archispace}', can analyse and evaluate \acrshort{bim} models in terms of various space syntax methods such as connectivity, integration and \textit{intelligibility}. During this study, the authors identify several challenges in applying space syntax concepts to \acrshort{bim} models, such as the lack of a unified topological syntax or interoperability with the \acrshort{ifc} file format. The preoccupation with combining digital analysis tools and spatial syntax concepts was further explored by \citeauthor{nourian2013designing} in the study \citetitle{nourian2013designing}. In the course of this work, a complete design pipeline with integrated analysis metrics was developed. The user defines the program using a graph and has the additional possibility to impose certain geometric requirements. The algorithm then provides different floor plan suggestions with individual room syntax evaluations, thus enriching the design process.

In \citeyear{aish2018topologic}, \citeauthor{aish2018topologic} document the developed topological software toolkit called '\textit{Topologic}', which establishes the connection between architecture and discrete mathematical operations by dividing the architectural model into elements of different hierarchies and dimensions. This ultimately enables topological analysis and \Glspl{boolean operation}, including neighbourhood and lateral relationship queries. In addition to presenting the functionality of the software toolkit, a subsequent publication in the same year, \citetitle{jabi2018topologic}, describes two applications of the software in the \acrshort{aec} domain. Firstly, the generation of non-\gls{manifold} topological models used as input for energy simulations is considered, and secondly, the possibility of structural analysis and simulations through topological models is explained.

Finally, in the year \citeyear{dawes2021examining}, a concrete analytical application of space syntax methods using three villas designed by Palladio is demonstrated in \citetitle{dawes2021examining} by \citeauthor{dawes2021examining}. The authors focus in particular on evaluating the importance of the main salon and the overall flexibility of the spatial layout through the creation of so-called \textit{justified plan graphs}.

The overview of the current state of research in graph theory and topological approaches in the context of architecture demonstrates the diversity and wide range of different research and application areas within the fields under consideration. However, it becomes clear that research into topological graphs in architecture is still in its infancy. This is partly due to the short time that has elapsed since the two subjects were brought together and applied in the architectural design process, and partly due to the lack of uniform design languages and processes in the \acrshort{aec} industry. It becomes thus apparent that, despite remarkable progress in the respective fields, the integration of graphs and topology in architecture is still an emerging area of research with room for further study and experimentation in order to apply the full potential of these mathematical concepts in the context of conventional architectural practice.

\section{Feedback in Early Design-Stages}\label{sec:feedback-in-early-design-stages}

Early design stages are critical to the success of an architectural project, as they form the solid basis for later decisions and developments. Feedback about the design intentions and individual decisions during these preliminary steps is therefore essential to potentially optimise the design, identify problems and develop solutions. Likewise, performative feedback in early project phases can have significant economic and environmental benefits and is therefore of great value during the initial design process.

\subsection{Decisionmaking and Feedback-Tools}\label{subsec:decisionmaking-and-feedback-tools}

In recent years, several techniques, tools and prototypes have been developed to provide feedback to designers during the initial design iterations. In particular, the application of computational methods has allowed a diverse exploration of different tools and algorithms to provide optimisation and support the decision-making process.

In this regard, the development and training of a neural network model that predicts the heating and electricity energy consumption of school buildings is described by \citeauthor{paterson2013real} in their publication, \citetitle{paterson2013real}. The resulting toolkit, \textit{SEED}, could provide energy performance during the design process based on specific design parameters such as glazing ratio, floor area orientation and others. However, the achieved accuracy of 40\% still leaves room for improvement. In order to successfully perform conventional energy simulations, \acrshort{bim} models often require substantial simplification at the geometry level. The research, \citetitle{chatzivasileiadi2018effect} by \citeauthor{chatzivasileiadi2018effect}, investigates the extent to which this geometric abstraction affects the simulation results and to what point geometric simplification is possible without causing significant issues and loss of accuracy during the simulations.

Addressing the ambiguity between the need for early energy consumption simulations and the significant time and resource investment, \citeauthor{singh2020applying} apply in \citeyear{singh2020applying}, neural networks to predict energy consumption of buildings. The developed methodology allows the prediction of both embodied and operational energy demand during the initial design phases, providing a feedback function for the overall energy performance without any special effort or knowledge required from the designer. One year later, to provide continuous feedback on the performance of designed floor plans in terms of daylighting and energy performance, \citeauthor{yousif2021deep} similarly trains a neural network with labelled floor plans. The resulting modified \textit{Pix2Pix} generative adversarial network (GAN) is capable of predicting heat maps for new floor plans with a high degree of accuracy. However, this method is limited to the second dimension and requires a pixel-based representation of the floor plan as input.

\subsection{Optimisation in Early Design-Stages}\label{subsec:optimisation-in-early-design-stages}

A particular type of feedback function in design processes is the analysis of information about the performance of the designed layout, allowing the application of optimisation algorithms with the aim of automatically improving the design with respect to the chosen performance parameter at an early stage. In this context, a wide range of algorithms such as evolutionary approaches or simpler function optimisation computations have found their application in the architectural context. However, meta-heuristic or multivariable approaches have the significant disadvantage of being very time consuming and resource intensive. To address this issue, the study \citetitle{schaffranek2015parallel} by \citeauthor{schaffranek2015parallel} introduces methods for optimising multiple parameters through \textit{spectral graph matching} based on graphically encoded information. The main advantage of this method is that, unlike conventional methods, it does not rely on iterative processes, thus avoiding significant time overhead. However, the algorithm can only take into account information that can be represented in graphical form, which significantly limits its applicability. In addition to multivariable optimisation methods in the design process, the research carried out by \citeauthor{canestrino2020generating} in the year \citeyear{canestrino2020generating} provides applications in constructive detail design using a complete \acrshort{bim} model. This study emphasises the importance of optimisation methods in the automatic exploration of a variety of different design and construction solutions.

The attempt to provide designers with a set of tools during the design process to achieve an optimised adjacency of programs and spatial functions through the application of graph \gls{centrality}-based design heuristics is documented in \citetitle{fuchkina2022space} by \citeauthor{fuchkina2022space}. Topological-graphic analysis methods such as \textit{distance mapping} and shortest path calculation, in conjunction with \textit{fuzzy logic evaluation}, are used in the course of this research to find the best-optimised adjacency variant of the rooms in the floor plan. 

Finally, to achieve parallel optimisation of spatial configuration and function, \citeauthor{muslimin2023experience} discusses in \citetitle{muslimin2023experience} his experimental combination of shape grammar and spatial syntax. The author was able to extend the syntax of the shape grammar with topological rules based on the computation and analysis of graph-theoretical methods in order to create an optimisation tool that can generate shapes and layouts according to space syntactic metrics.

\subsection{Performance-Based Design}\label{subsec:performance-based-design}

The designer is always concerned with performance in the broadest sense of the word in architecture, which means that automatic feedback on these very evaluation metrics can bring significant benefits. As an umbrella term, these approaches can be described as performance-based design, where the primary goal is to develop tools and methodologies for continuous performance evaluation that can complement traditional modelling and conceptualisation processes. As this is a relatively recent research topic, the progress made over the last decade will be presented in chronological order:

The \textit{Generative design system}, developed by \citeauthor{caldas2012generation} and documented in \citetitle{caldas2012generation}, combines shape grammar with an energy simulation engine to optimise houses with a patio typology using evolutionary algorithms. The choice of energy performance as the optimisation metric allows for an automated performance-based design process. Furthermore, in order to clarify the requirements for performance analysis, \citeauthor{jabi2015potential} in \citeyear{jabi2015potential} investigates the importance of non-\gls{manifold} topology-based models for building performance simulation in the early stages of the design process. It is shown that this type of geometry representation, with its division into hierarchical subelements and high level of abstraction with limited loss of information, is well suited as input geometry for subsequent energy simulations. This implies the importance of a tool for extracting non-\gls{manifold} geometry from existing \acrshort{bim} models at different levels of detail and project stages.

The meta-analysis of the application of evolutionary optimisation algorithms in the field of \acrshort{aec}, carried out in \citeyear{canestrino2021influence} by \citeauthor{canestrino2021influence}, demonstrates that there will always be limitations to the application of optimisation algorithms during the design process. Not every architectural criterion can be abstracted to act as an evaluation metric for optimisation methods. The most common application of such algorithms is therefore limited to clearly defined elements and fitness criteria, such as the performance of shading systems or the formal appearance of a building in terms of its \acrlong{df}. In this context, \citeauthor{rogers2022running} developed a simple performance-oriented simulation technique in \citeyear{rogers2022running} that does not rely on such optimisation algorithms. Rather, it integrates graph-theoretic analysis with congestion path finding and building usage data such as schedules, walking speed, origin and destination, allowing for agent-based simulation along specific paths to identify potential bottlenecks in the overall circulation system.

The contributions presented show that feedback in early design stages can be of significant importance in optimising the design process and improving the architectural and technical quality of the outcome. In recent years, numerous methodologies and tools have been developed to assist architects, mainly through decision support systems such as computer-aided and algorithm-based technologies. However, this area of research is far from exhausted, and new studies and experiments are published on a weekly basis, with new results showing novel applications and potentials for the architectural design workflow. This is particularly relevant in the context of the recent resurgence of interest in machine learning and artificial intelligence.

\section{Architecture and Machine Learning}\label{sec:architecture-and-machine-learning}

As a discipline that has been increasingly applied to more and more different fields for some time now, machine learning methods have also taken on various roles in architecture. In most cases, however, these applications are primarily experimental or artistic, and are accompanied by a wide range of different research, studies and experiments. The following section presents research on the role of machine learning in the \acrshort{aec} industry, categorised into different subtopics, ranging from literature on the impact on the field to specific research on graph machine learning applications.

\subsection{Impact of Machine Learning}\label{subsec:impact-of-machine-learning}

To begin with, the general implications of integrating machine learning methods into architectural practice are considered, as well as the new questions and issues raised by this combination.

In the context of an experimental application of \textit{\acrlong{gans} (\acrshort{gans})} for architectural plan generation purposes, the work \citetitle{campo2020how} by \citeauthor{campo2020how} discusses the potential role that machine learning can play in the conventional design process and to what extent this might already be possible. Important aspects such as creativity and stylistic aspects are examined and discussed in detail from an architectural perspective. On the other hand, a much more technical look at the application of computational methods in the \acrshort{aec} industry is provided by the anthology \citetitle{as2021routledge} in \citeyear{as2021routledge}. Structured around subthemes ranging from theoretical foundations to artificial intelligence in architectural practice, a selection of authors provide insights into the various benefits, challenges and implications of adopting machine learning concepts in the field of architecture and construction. Particularly helpful for this work was the section entitled \citetitle{nagy2021ai} by \citeauthor{nagy2021ai}.

On a broader level, the publication \citetitle{joyce2021limits} by \citeauthor{joyce2021limits} presents a cautious and realistic meta-analysis of the limitations of integrating machine learning-based tools in the architectural domain. Various topics such as scale, representation, input data, resolvability, creativity, autonomy and design quality are identified and discussed. In strong contrast to this literature, \citeauthor{chaillou2022artificial} in \citetitle{chaillou2022artificial} offers an optimistic outlook on the future synergy between architecture and artificial intelligence. Current applications such as floor plan design, urban planning, automatic facade conception, structural design and predictive simulations through \textit{DaylightGAN} and \textit{ComfortGAN} are examined. Finally, potential application areas are presented which, according to the author, could benefit from the use of artificial intelligence.

\subsection{Design and Optimisation Applications}\label{subsec:design-and-optimisation-applications}

Looking at some concrete implementations of machine learning in design practice, the primary use can be seen as a support and optimisation system during the design process in architecture. For example, in the study conducted by \citeauthor{hauck2017energy} in \citeyear{hauck2017energy}, a neural network is trained to directly predict the energy performance of a designed architectural object, avoiding the costly detour via thermodynamic and energy simulations. The input features, collected in tabular form for training, consist of essential characteristics related to geometric and energetic parameters of the designed three-dimensional layout. Another type of machine learning based optimisation of architectural objects is presented in the publication \citetitle{jabi2019synergy} by \citeauthor{jabi2019synergy}. This work combines topological building graphs with \textit{reinforcement learning} methods to find optimal escape routes in the event of a fire in the building being studied. What makes this experiment interesting is that the simulated fire sources have both a three-dimensional spread and a temporal component, justifying the application of unsupervised reinforcement learning. The work demonstrates the potential of using abstract geometric representations through non-\gls{manifold} topology and graphs in conjunction with advanced machine learning methods.

The term \textit{surrogate model} refers to the method of replacing a variable that is difficult to compute with an approximated function. This concept is explored in \citetitle{tarabishy2020deep} by \citeauthor{tarabishy2020deep} using a trained neural network to accelerate the prediction of spatial syntax metrics such as spatial and visual connectivity. The pixel-based generation and training pipeline is thoroughly documented and explained throughout the paper.

Finally, in the topic of design and optimisation applications of artificial intelligence in the context of architecture, \citeauthor{chaillou2021ai} takes a creative and experimental approach to the application of \acrshort{gans} in the architectural floor plan generation process, demonstrating an interesting use of deep learning concepts in the form and style finding process. The technical documentation of the training and evaluation processes is complemented by extensive research on learning and combining different architectural styles from different periods, recently published in \citeyear{chaillou2021ai}.

\subsection{Graph Machine Learning in Architecture}\label{subsec:graph-machine-learning-in-architecture}

The concept of graph machine learning is relatively new in computer science, and thus has only been applied to architectural research and practice since around 2020. However, the fundamental importance of graph theory in the discipline of architecture makes it clear that there is great potential for such deep learning methods in this field. Some of the main contributions to this topic are listed below.

\textit{Semantic enrichment} refers to the process of adding descriptive information to an existing model. In the study \citetitle{wang2021room} by \citeauthor{wang2021room}, \acrlong{gnns}, specifically a \textit{GraphSAGE} model, was trained to predict the function of individual rooms in a \acrshort{bim} model with high accuracy. This application could potentially be extended to other elements of the architectural model, enabling interoperability through augmented \acrshort{bim} data. Extending the research topic of room classification methods, the \citetitle{kiavarz2021room} experiment published in \citeyear{kiavarz2021room} experimented with classifying the energy consumption of individual rooms in a \acrshort{bim} model using supervised graph deep learning algorithms based on the GraphSAGE method. The model is thus able to predict the labels of individual nodes in a graph object.

Using \textit{transfer learning} methods, the publication \citetitle{eisenstadt2022autocompletion} by \citeauthor{eisenstadt2022autocompletion} developed and evaluated the prediction of connections between individual programs in an architectural project. The trained graph neural network can autocomplete partial layout graphs and predict the probability of edges between nodes with high accuracy. This approach can be combined with node or edge classification to predict room functions and the types of connections between them. In terms of graph-wide prediction, \citeauthor{alymani2022graph} explored the application of a \textit{\acrlong{dgcnn}} to synthetically generated building graph datasets with different ground relations in two publications between \citeyear{alymani2022graph} and \citeyear{alymani2023classifying}. The trained graph neural network is thus able to accurately predict the ground relation of new graph objects. The framework of this research included supervised learning methods such as graph classification and unsupervised learning methods such as clustering.

Taken as a whole, this research demonstrates the versatility of machine learning in the field of \acrlong{aec}. It is possible to optimise topological arrangements, improve selected performance characteristics such as energy efficiency, and provide a detailed understanding of spatial structures. It should be noted, however, that the field of machine and deep learning in architecture is in constant evolution and there are still many open questions and challenges. In particular, questions remain about the interaction between human designers and machine learning systems, the \textit{interpretability} of machine learning models, and the adaptability of these techniques to different contexts and requirements.

\section{Synthetic Architecture-Datasets}\label{sec:synthetic-architecture-datasets}

With the increasing adoption of machine learning methods in the design process, there is a growing need for qualitative architectural datasets that represent specific aspects of the project as well as possible. Questions about the state of the art in the generation and development of these datasets, their different forms and their multiple applications are explored in the following sections.

\subsection{Parametric Design and Algorithms}\label{subsec:parametric-design-and-algorithms}

The creation of such data requires the application of parametric design principles as well as the definition and formulation of certain architectural rules and conditions. This has led to extensive research and experimentation on these concepts and their adaptation to allow the construction of appropriate and qualitative frameworks for architectural generation pipelines.

The algorithm developed in \citetitle{galle1981algorithm} is able to generate optimised rectangular floor plans based on variable input parameters such as total area, room areas, wall lengths, room adjacencies and room orientations. The research conducted by \citeauthor{galle1981algorithm} in \citeyear{galle1981algorithm} laid the groundwork for subsequent experiments by introducing geometric partitioning methods, iterative division and graph-theoretic adjacency calculations.

Regarding the purely algorithmic tools for automatic architecture generation, several different methods and approaches have been identified in the course of the research. \textit{Treemaps}, which are primarily used to visualise related quantities and offer interesting properties for constraint-based space allocation, form the first advanced space partitioning algorithm. However, they are limited to dividing purely rectangular areas. In their study from \citeyear{balzer2005voronoi}, \citeauthor{balzer2005voronoi} present a Voronoi diagram-based form of treemaps that can allocate desired percentages to a broader set of shapes, including irregular polygons.

An extension of the Voronoi diagram presented here is the weighted Voronoi diagram or Laguerre Voronoi diagram, which is explored in the context of urban planning by \citeauthor{anuradha2008voronoi} in \citeyear{anuradha2008voronoi}. The regulation of Voronoi cell sizes represents a significant advantage in the field of automated space subdivision. The influence of individual Voronoi seeds could thus be determined by adding individual weight values and corresponding weight-distance functions to the diagram points. In addition to the weight function, the morphology of the computed regions can also be regulated by this approach. In this context, \citeauthor{wang2019orthogonal}, with a publication entitled \citetitle{anuradha2008voronoi}, introduces a new algorithm based on an \textit{orthogonal Voronoi diagram} and an adapted distance and neighbourhood function, as well as a modified \textit{sweep-line algorithm} for computing \textit{orthogonal Voronoi treemaps}. This method combines the advantages of each technique, providing accurate percentage control through treemap calculation, flexibility through Voronoi diagram structure, and avoidance of irregular space generation through orthogonal shape generation.

\subsection{Automatic Floor Plan Generation}\label{subsec:automatic-floor-plan-generation}

When dealing with geometric floor plan data based on buildings or even apartment complexes, the application of automatic floor plan generation methods is almost inevitable. This topic comes with its own set of implications and challenges, partly caused by the high degree of freedom inherent in the conceptual architecture process. The main experiments of the last decade are listed and explained here in chronological order:

In \citeyear{knecht2010generating}, \citeauthor{knecht2010generating} discuss the application of K-dimensional trees using evolutionary optimisation algorithms for the automated generation of floor plan layouts. The \gls{binary tree} structure of K-d trees proved advantageous in the context of regulated space allocation, as a certain degree of controllability is provided through the initial definition of points in two or three dimensional space. Another type of partitioning algorithm is the so-called squarified treemap, which constitutes a variant of the regular treemap algorithms with the advantage of generating regions with high aspect ratios, resulting in more square-like spaces. This algorithm is introduced in \citetitle{marson2010automatic} by \citeauthor{marson2010automatic}, who explain the structure and functionality of such methods and test them on an architectural example. The published paper additionally discusses approaches to automatic corridor generation.

Staying within the topic of automatic floor plan generation, the paper \citetitle{shekhawat2019graph}, by \citeauthor{shekhawat2019graph} explains the development of an algorithm that employs graph-theoretic methods to generate a \textit{rectangular floor plan} (RFP) based on a given adjacency graph. If this is not possible, an \textit{orthogonal floor plan} (OFP) is constructed. This research is essential as most automatic floor plan algorithms generate rectangular floor plans exclusively, thus limiting the variety and design possibilities.

After several years of research in this area, \citeauthor{egor2020computer} in the year \citeyear{egor2020computer} provide a collection and analysis of existing room layout generators, combined with newly developed methods and extensions of existing research. This analysis led to the implementation of a so-called '\textit{Magnetizing Floor Plan Generator}', which respects the desired adjacency of individual rooms and their functions through iterative placement. An interesting aspect of this methodology is the introduction of corridor elements to address the issue of gaps between the generated room geometries.

\subsection{Architectural Datasets}\label{subsec:architectural-datasets}

What forms can these datasets take and what information do they contain? What are the most common datasets and what are potential issues in their application? To explore these questions in more detail, it is necessary to take a look at the main research around the topic of dataset generation and to examine some example datasets.

The research carried out by \citeauthor{kalervo2019cubicasa5k} and published under the title \citetitle{kalervo2019cubicasa5k} in \citeyear{kalervo2019cubicasa5k} resulted in the creation of a dataset containing 5 000 annotated, cleaned and vectorised floor plans. The dataset was generated using neural networks for architectural element recognition. It further provides an organised structure with polygonal room divisions and categorisation of individual elements and spaces, serving as a qualitative basis for training various deep learning methods in the architectural context. Building upon this vector-based \textit{Cubicasa5k} dataset, two years later \citeauthor{lu2021organizational} succeeded in synthesising a high-quality architectural graph dataset by applying relation detection methods to the Cubicasa5k data. A major advantage of this new set is the annotated graphs, which include node labels corresponding to room functions and edge labels describing the nature of room relationships. Furthermore, there is potential for combining the Cubicasa and the newly created \textit{Cubigraph} datasets, as their datapoints correspond and their information is complementary.

Finally, the approach described in the work of \citeauthor{chen2022ro} and published in a paper named \citetitle{chen2022ro} in \citeyear{chen2022ro} demonstrates a method for the extraction of \textit{attributed adjacency graphs} from pixel-based floor plan representations using image segmentation algorithms. The resulting graph dataset, generated using \textit{\glspl{ensemble learning method}}, includes node labels that provide information about the nature of the architectural element. The dataset has been released under an \gls{open source} licence and is therefore well suited for further research on the topic.

From the literature cited, it is apparent that the creation of qualitative datasets is an important step in enabling the application of machine learning to the field of architecture. By employing \acrlong{bim} and floor plan generation algorithms, as well as appropriate optimisation methods, researchers and architects can generate a variety of synthetic datasets that can be used to train and validate machine learning models. These synthetic datasets, in combination with data science methods, can help to improve the design process, optimise the performance of buildings or even develop new design concepts. The challenges and open questions identified during the literature review on synthetic architectural datasets mainly concern the quality and representativeness of the architectural data, as well as the \gls{origin bias} that may be present in the information.

\newpage
\AddToHookNext{shipout/background}{\put (-8pt,-\paperheight-4pt){
\begin{tikzpicture}\node(a){\includegraphics[scale=1]{assets/cover/contribution_cover_1.pdf}};
\node at(a.center)[draw, fill=white,line width=1.2pt,circle, minimum height=200pt, yshift=80pt]{};
\end{tikzpicture}}}

\part{Contribution}\label{part:contribution}

\newpage \ \thispagestyle{empty}
\AddToHookNext{shipout/background}{\put (0,-\paperheight){\includegraphics[scale=1]{assets/cover/contribution_cover_2.pdf}}}
\newpage\clearpage

The methodological part of this thesis serves to document the experiments carried out, which include the creation of a synthetic architectural graph dataset and the training and evaluation of different graph machine learning models. These experiments are the synthesis and application of the methods introduced in the theoretical framework, which come from different fields such as mathematics, geometry, urban studies, sociology and computer science, in order to propose answers to the stated research questions. To ensure a clear structure of the methodology, the essential steps and results of the experiments have been documented. However, it is important to mention in advance that the experiments presented are the result of a series of iterative processes, in which a multitude of different methods were tested in order to select the most appropriate approach.

The first part of the methodology deals with the application of different space partitioning algorithms that, integrated in a parametric framework and combined with architectural rules and conditions, initially allowed the creation of an architectural three-dimensional dataset. The resulting apartment geometries could be enriched with various information using topological methods to finally generate semantic topological graph networks corresponding to the individual apartment layouts. The added information includes categorical descriptions of each room, such as living room, bedroom, toilet, bathroom or utility room, but also typical descriptions such as apartment, window or room on a general level. Additionally, dimensional and geometric attributes have been included, providing information on the orientation and dimensions of each element.

Essential for the usefulness of the dataset is any information that allows statements to be made about the energy efficiency of the individual apartment layouts and is stored at the \gls{cellcomplex} level. For this purpose it was necessary to subject each individual three-dimensional \acrshort{bim} model to accurate energy simulations and to evaluate the results. Accordingly, the resulting dataset contains 40 000 graph objects describing the topologies of the architectural objects, enriched with information about the dimensions and orientations of the individual elements. Each of these graph objects is assigned an energy efficiency value and a class, which are used as training data in the subsequent model training phase. The parametric, automatic generation of a synthetic dataset is in stark contrast to the conventional method, which is usually based on the accumulation, adjustment and annotation of reality-based architectural plans.

The second part of the experimental phase builds on the results of the first part and allows to evaluate the relevance of the dataset. This involves the application of graph machine learning to the generated \gls{knowledge graph} objects in order to learn the relationship between the topology of the apartments and their corresponding energy efficiency. Two different supervised learning approaches are demonstrated, which differ to some extent in their possible applications.

\begin{figure}
\centering
\includegraphics[width=\textwidth]{assets/contribution/mindmap.pdf}
\caption{Experimentation Overview}
\label{fig:process-overview}
\end{figure}

The first step of the machine learning experiment is to design a classification model capable of categorising the unseen apartment graphs into their corresponding energy efficiency classes. In order to be able to define these energy classes, it is necessary to create a relative energy consumption scale due to the high level of abstraction in the structure of the energy simulation framework parameters as well as in the geometric composition of the individual apartment layouts. This measure takes into account the entire dataset and creates a new scale by dividing it into \textit{\glspl{quantile}} (table \ref{tab:energy-class-calculation-quantiles}). The accuracy of the trained model is then compared to other models, trained on tabular information rather than graphical data, in order to evaluate the benefits of graph machine learning methods in the context of energy efficiency classification compared to conventional machine learning algorithms.

The second deep learning method is to build a regression model that allows accurate prediction of energy consumption in $MJ/m^2$ from unseen topological \glspl{knowledge graph}. This involved modifying and adapting the training setup of the \textit{\acrlong{dgcnn}} and its model architecture to enable the regression task. The resulting model is then likewise compared with conventional regression models to make statements about its \textit{\gls{precision}}.

Figure \ref{fig:process-overview} shows the overall process combining the two steps of dataset generation and graph machine learning. The continuous arrows show the principal workflow, while the dotted lines provide additional information about the steps required to accomplish the principal tasks. The diagram is divided into three colours, where green represents the dataset generation process and its subtasks, blue provides information about the composition of the data enhancement and the information generation through simulation, and red indicates the graph machine learning framework, which forms the final step of the experimentation.

\chapter{Synthetic Dataset Generation}\label{chap:synthetic-dataset-generation}

Datasets form the elementary basis of data science methods such as machine learning, as this discipline is concerned with identifying regularities and correlations in the available information using large amounts of data. The patterns and trends in the dataset are recognised and used by a machine learning algorithm to perform the predefined task of the model. As the task of the experimentation involves supervised learning, it is essential that the dataset contains labels corresponding to the desired outcomes, in addition to the training information. This implies that the energy consumption value and energy class in the dataset must be assigned as a value to each corresponding datapoint.

One of the main requirements for the synthetic architectural dataset in this thesis is to have as much variation in the design layout as possible, so that a large number of different room arrangements and apartment morphologies can be associated as training data with their respective energy efficiencies. This allows for some generalisation in the application of the trained model. Typically, datasets in computer science have a tabular form and are represented by numerical values. Accordingly, architecture, as an artistic and geometric discipline, poses a challenge for the creation of datasets, because architectural geometric objects such as rooms, houses and apartment complexes, as well as their individual programs and structures, cannot be represented in an intuitive way as tabular, numerical data.

To solve this problem, several methods for representing architectural data in a machine-understandable form have emerged in recent years\footcite{kalervo2019cubicasa5k}. It is always important to distinguish what the desired output of the trained model is, as this has a significant impact on the requirements on the training dataset. The most common data representation is the one that has been used in architectural practice since the beginning of the profession, namely the representation by pictorial cross-sections of the architectural object, such as plans and sections.

The advantage of an image-based data representation is that it forms an integral part of the architectural representation, and thus a wide range of real data derived from scanned or digitally created plans is available. Furthermore, plan representation is deeply rooted in architectural practice, with many design processes traditionally focused on the two-dimensional plan level. As a result, plan representation has become a synthetic form of presentation that provides insight into unmapped elements of the architectural object by adding semantic and descriptive information. In addition, there are usually several levels of detail in the two-dimensional plan representation of a project, as different instances in everyday architectural practice have different requirements for architectural plans. This enables the aggregation of multiple layers of information, all relating to the same object.

As with conventional machine learning based on image data, the bitmap pixels are tabulated with their colour values, resulting in a machine-readable table of values as a dataset. However, a major drawback of this conventional way of creating architectural datasets is the significant loss of information at multiple levels due to the pixel representation of architectural objects. Conceptual and programmatic information, such as the use of individual spaces, their topological relationship to each other, their extension into the third dimension, as well as information on materials and morphological properties, are difficult and, above all, inconsistent to represent in architectural plans. This means that this information cannot be taken into account during the training phases, even though it may represent elementary information.

Furthermore, pictorial datasets in architecture are associated with high memory and computational requirements, as they usually contain a large amount of non-informative areas, such as the white framing around the project. Architectural plans are often annotated with handwritten or computer generated annotations, which do not follow any uniform rules and therefore have to be removed in several successive filtering processes in order not to contribute to unwanted noise in the dataset\footcite{newton2019deep}. In this work, an attempt is made to counteract the aforementioned drawbacks of traditional datasets by representing architectural objects not in pictorial form, but through topological graphs enriched with additional information, which can thus be called a \gls{knowledge graph} dataset.

\section{Space Partitioning}\label{sec:space-partitioning}

Since the synthetic data generation approach requires an automated algorithm, it is first necessary to consider the appropriate application of a geometry generation method. Basically, the objective is to develop a pipeline that takes specified architectural guidelines as input and generates a floor plan as output, which can then be automatically transformed into a full three-dimensional \acrshort{bim} model in subsequent steps.

It is essential that these methods have a degree of randomness to allow for the greatest possible morphological diversity in geometry generation. The process of choosing the appropriate space partitioning method is intended to compare as many different algorithms as possible in order to make a selection of suitable methods that could be integrated into the generation pipeline. As the aim was to develop as large a dataset as possible, properties such as the speed of the methods and the computational requirements were considered more carefully. However, the quality and especially the variety of the generated geometries was deemed to be the most important property for the selection of the algorithms.

\subsection{Algorithms}\label{subsec:algorithms}

As presented in the theoretical framework, a variety of different automatic floor plan generation methods exist\footcites{egor2020computer}{lopes2010constrained}{nisztuk2019hybrid}, some of which can be combined with different optimisation algorithms, and can be roughly divided into grid-based, subdivision or aggregation methods. In general, aggregation methods are much more computationally intensive and complex than subdivision or grid methods. However, this increased complexity also leads to increased variety between each generated layout, which is a significant added value in the demonstrated methodology. Finally, due to resource constraints, it was decided to include only three algorithms, coming from the subdivision family, in the final generation pipeline. However, in order to present a comprehensive picture of the experiments and to provide a basis for further research, methods that were ultimately deemed unsuitable for the generative pipeline for various reasons are equally presented.

\subsubsection{Subdivision}\label{subsubsec:subdivision}

The simplest methods for automatic space segmentation come from the family of subdivision algorithms, which, as the name suggests, subdivide a previously defined shape into a desired number of polygons according to defined rules. As they are mostly simple algorithms, they all share the advantage of being computationally inexpensive and therefore relatively fast in their execution. However, in order to integrate any method from this family of algorithms into an automatic pipeline, an parametric generation of the initial shape to be divided must first be considered.

\paragraph{Initial Shape}\label{par:initial-shape}

A common method of automatically generating irregular \gls{convex} polygons is to apply a \textit{convex hull algorithm}\footcite{anuradha2008voronoi} to a two-dimensional set of points. This involves first randomly placing a defined or arbitrary number of points in two-dimensional space. The amount of points is not important as long as there are more than two; however, a larger number of points increases the probability of generating a polygon with an equally larger number of sides. Once these points have been placed, the convex hull can be calculated and thus create the initial shape for the generation pipeline. However, a major drawback of this method is that it is only suitable for generating \gls{convex} figures, making concave plans impossible.

In order to remain true to architectural reality, it is equally important to generate rectangular plans as well as their derivatives, which can be generated by shifting individual vertices of the rectangles. This is possible through the application of simple algebraic operations and does not require the use of established algorithms. Furthermore, by using \textit{orthogonal convex hull algorithms}\footcite{an2021modified} (figure \ref{fig:orthogonal-convex-hull}), the variety of basic shapes to be generated can be increased, since this is a method that generates rectangular polygons, analogous to ordinary convex hull methods, based on a randomly placed set of points in the two-dimensional space.

\begin{figure}
\centering
\begin{subfigure}{.5\textwidth}
\centering
\includegraphics[width=\linewidth]{assets/contribution/dataset/orthogonal_convex_hull1.pdf}
\label{fig:orthogonal-convex-hull-1}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
\centering
\includegraphics[width=\linewidth]{assets/contribution/dataset/orthogonal_convex_hull2.pdf}
\label{fig:orthogonal-convex-hull-2}
\end{subfigure}
\caption{Orthogonal Convex Hulls}
\label{fig:orthogonal-convex-hull}
\end{figure}

\paragraph{Recursive Subdivision}\label{par:recursive-subdivision}

Using a simple recursive subdivision method, the initial shape is iteratively sliced so that the parent shape is split into two equally sized child shapes. This method works for rectangular bases, but can cause problems when \gls{convex} or concave polygons are involved. One way to have more influence on the position of the cutting planes is to define percentages for the positioning of the slicing faces in relation to the two-dimensional depth of the shape being divided. This can be re-evaluated from iteration to iteration (figure \ref{fig:recursive-subdivision}), allowing it to adapt to irregular shapes.

Another derivative of the recursive subdivision method can be \textit{recursive quad-division}, which, unlike the conventional method, does not divide the parent surface by a straight line in the X or Y direction, but instead performs a division from the \textit{\gls{centroid}} of the geometric shape to the center of the edges of the polygon, resulting in at least three quad-edged child polygons. This subdivision method is particularly suitable for irregular \gls{convex} shapes, as it avoids unwanted intersections. Also of interest is the \textit{recursive bent bisection method}, which instead of straight cutting surfaces introduces a bend in the cutting surface at a random or defined point. This simple modification can lead to a great variety in the resulting design, but does not solve the problems encountered with the simple recursive subdivision method.

\begin{figure}
\centering
\begin{subfigure}{.33\textwidth}
\centering
\includegraphics[width=0.93\linewidth]{assets/contribution/dataset/recursive1.pdf}
\caption{Second Iteration}
\label{fig:second-iteration-1}
\end{subfigure}%
\begin{subfigure}{.33\textwidth}
\centering
\includegraphics[width=0.93\linewidth]{assets/contribution/dataset/recursive2.pdf}
\caption{4th Iteration}
\label{fig:4th-iteration}
\end{subfigure}%
\begin{subfigure}{.33\textwidth}
\centering
\includegraphics[width=0.93\linewidth]{assets/contribution/dataset/recursive3.pdf}
\caption{6th Iteration}
\label{fig:6th-iteration}
\end{subfigure}
\caption{Recursive Subdivision}
\label{fig:recursive-subdivision}
\end{figure}

\paragraph{Squarified Treemap}\label{par:squarified-treemap}

The squarified treemap algorithm\footcite{marson2010automatic} originally comes from the family of data visualisation algorithms. It is based on the creation and ordering of the spatial quantities to be created in a \gls{binary tree} structure, where the values are ordered in advance to produce the most optimal subdivision. The cutting method is not much different from a conventional recursive subdivision, but in a squarified treemap the resulting spaces in the generated layout are calculated to be as quadratic as possible, which means that the quotient of two adjacent edges is as close to 1 as possible. A major advantage of this method is that the percentage size of the spaces can be defined in advance (figure \ref{fig:treemap-subdivision}), allowing finer architectural control over the generation process. However, this method only works for rectangular solids and is therefore not suitable for a generation pipeline where polygons are also desired as initial shapes.

\begin{figure}
\centering
\begin{subfigure}{.3\textwidth}
\centering
\includegraphics[width=\linewidth]{assets/contribution/dataset/treemap1.pdf}
\caption{4 Spaces}
\label{fig:4-spaces}
\end{subfigure}%
\begin{subfigure}{.34\textwidth}
\centering
\includegraphics[width=\linewidth]{assets/contribution/dataset/treemap2.pdf}
\caption{6 Spaces}
\label{fig:6-spaces}
\end{subfigure}%
\begin{subfigure}{.36\textwidth}
\centering
\includegraphics[width=\linewidth]{assets/contribution/dataset/treemap3.pdf}
\caption{7 Spaces}
\label{fig:7-spaces}
\end{subfigure}
\caption{Treemap Subdivision}
\label{fig:treemap-subdivision}
\end{figure}

\paragraph{K-Dimensional Tree}\label{par:k-dimensional-tree}

When using a k-dimensional tree\footcites{knecht2010generating}{das2016space} for space partitioning, new requirements are placed on the input. In order to perform a successful space subdivision with a satisfactory level of control, a two-dimensional set of points is required to be located on the initial shape. These points will be inside the respective spaces of the fully partitioned surfaces and can thus serve as a basis for a rough local determination of the position of the individual spaces in advance (figure \ref{fig:k-d-tree-subdivision}). This ability to predetermine the approximate division of space by placing points on the two-dimensional body is an advantage over other methods. This can be achieved by simple random or rule-based point placement methods. Furthermore, the algorithm works not just on rectangular solids but also on \gls{convex} polygons (figure \ref{fig:k-d-tree-subdivision-of-irregular-polygons}). This algorithm allows seamless subdivision of rectangles with a particularly low edge ratio (figure \ref{fig:k-d-tree-subdivision-of-stretched-rectangle}) but does however not allow to define the desired percentage areas of the individual spaces in advance, as in the example of the squarified treemap algorithm.

\begin{figure}
\centering
\begin{subfigure}{.25\textwidth}
\centering
\includegraphics[width=.94\linewidth]{assets/contribution/dataset/kdtree1.pdf}
\caption{Iteration 1}
\label{fig:iteration-2}
\end{subfigure}%
\begin{subfigure}{.25\textwidth}
\centering
\includegraphics[width=.94\linewidth]{assets/contribution/dataset/kdtree2.pdf}
\caption{Iteration 2}
\label{fig:iteration-3}
\end{subfigure}%
\begin{subfigure}{.25\textwidth}
\centering
\includegraphics[width=.94\linewidth]{assets/contribution/dataset/kdtree3.pdf}
\caption{Iteration 3}
\label{fig:iteration-4}
\end{subfigure}%
\begin{subfigure}{.25\textwidth}
\centering
\includegraphics[width=.94\linewidth]{assets/contribution/dataset/kdtree4.pdf}
\caption{Iteration 4}
\label{fig:iteration-5}
\end{subfigure}
\caption{K-d Tree Subdivision}
\label{fig:k-d-tree-subdivision}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=\textwidth]{assets/contribution/dataset/kdtree_deform1.pdf}
\caption{K-d Tree Subdivision of Stretched Rectangle}
\label{fig:k-d-tree-subdivision-of-stretched-rectangle}
\end{figure}

\begin{figure}
\centering
\begin{subfigure}{.36\textwidth}
\centering
\includegraphics[width=0.96\linewidth]{assets/contribution/dataset/kdtree_deform2.pdf}
\label{fig:irregular-k-d-tree-1}
\end{subfigure}%
\begin{subfigure}{.64\textwidth}
\centering
\includegraphics[width=0.94\linewidth]{assets/contribution/dataset/kdtree_deform3.pdf}
\label{fig:irregular-k-d-tree-2}
\end{subfigure}
\caption{K-d Tree Subdivision of Irregular Polygons}
\label{fig:k-d-tree-subdivision-of-irregular-polygons}
\end{figure}

\paragraph{Voronoi Diagram}\label{par:voronoi-diagram}

The initial requirements for Voronoi diagram subdivision methods are similar to those for k-dimensional trees, since they also involve defining points on the initial shapes in advance. However, the body to be subdivided can be any \gls{convex} as well as concave polygon, allowing for greater diversity in layout generation. The points on the surface are first connected to each other and to the vertices of the base body by a Delaunay triangulation, so that each point is connected to another by an edge. Then the Voronoi diagram is computed so that the points become Voronoi seeds and all points that are closer to one seed than to any other point of the body are bounded by a \textit{Voronoi cell} (figure \ref{fig:voronoi-subdivision}). These cells are irregular polygons whose shape is defined by the distance of the triangulated point mesh only. This means that the spaces generated in this way tend to have irregular shapes, which is an architectural challenge, since in construction reality, although it is not impossible to build irregular spaces, a general convention is to design spaces that are as orthogonal as possible.

In order to deform the irregular Voronoi cells into polygons that are more regular, various post-processing algorithms can be applied to the generated design. For example, a \textit{\Gls{lloyd algorithm}} can iteratively shift the seed points to relax the Voronoi mesh, allowing initially highly deformed cells to approximate more compact geometric shapes. This works by iteratively computing a new Voronoi diagram based on the continuous shift of the Voronoi seed points towards the \gls{centroid} of each Voronoi cell. It should be noted, however, that this step-based algorithm is more resource intensive in terms of time and computing power. Another way to generate more regular Voronoi cells is to use a \textit{semi-orthogonal Voronoi diagram}, where each cell has at least one orthogonal edge or completely orthogonal cells computed by the \textit{orthogonal Voronoi treemap algorithm}\footcites{wang2019orthogonal}{chatzikonstantinou20143}. Another drawback of the Voronoi method is the lack of control over the resulting area dimensions of each cell, as the method is point-based like the k-dimensional tree method. To overcome this problem, weights can be assigned to each point in advance, so that by varying them the radius of each cell can be precisely controlled; these diagrams are then called power diagrams or Laguerre-Voronoi\footcite{anuradha2008voronoi} (figure \ref{fig:weighted-laguerre-voronoi-diagram}).

\begin{figure}
\centering
\begin{subfigure}{.33\textwidth}
\centering
\includegraphics[width=.97\linewidth]{assets/contribution/dataset/voronoi1.pdf}
\caption{5 Cells}
\label{fig:5-cells}
\end{subfigure}%
\begin{subfigure}{.33\textwidth}
\centering
\includegraphics[width=.97\linewidth]{assets/contribution/dataset/voronoi2.pdf}
\caption{20 Cells}
\label{fig:20-cells}
\end{subfigure}%
\begin{subfigure}{.33\textwidth}
\centering
\includegraphics[width=.97\linewidth]{assets/contribution/dataset/voronoi3.pdf}
\caption{50 Cells}
\label{fig:50-cells}
\end{subfigure}
\caption{Voronoi Subdivision}
\label{fig:voronoi-subdivision}
\end{figure}

\begin{figure}
\centering
\begin{subfigure}{.5\textwidth}
\centering
\includegraphics[width=\linewidth]{assets/contribution/dataset/voronoi3d1.pdf}
\label{fig:3d-voronoi-1}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
\centering
\includegraphics[width=\linewidth]{assets/contribution/dataset/voronoi3d2.pdf}
\label{fig:3d-voronoi-2}
\end{subfigure}
\caption{Three-Dimensional Voronoi Division}
\label{fig:three-dimensional-voronoi-division}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=\textwidth]{assets/contribution/dataset/power_voronoi.pdf}
\caption{Laguerre-Voronoi Diagram}
\label{fig:weighted-laguerre-voronoi-diagram}
\end{figure}

\subsubsection{Grid Planning}\label{subsubsec:grid-planning}

The basis of grid planning algorithms is the placement of a grid on the surface to be subdivided. This basic requirement in itself introduces some issues and difficulties, since the act of grid placement is in itself a subdivision of space. The simplest way of placing a grid is to project a pre-established mesh onto the base to be subdivided, matching the dimensions of the grid to the dimensions of the surface in two dimensions. This only gives satisfactory results if the base is orthogonal and leads to unwanted subdivisions as soon as \gls{convex} or concave polygons are involved. To satisfactorily subdivide irregular polygons by grids, it is necessary to apply the subdivision methods described above. In particular, the quad-subdivision method (section \ref{par:recursive-subdivision}) and the Voronoi subdivision (section \ref{par:voronoi-diagram}) are suitable for applying a grid to the desired shape, since both have a high adaptability to the morphology of the basic shape. Thus, it is easily possible to divide a \gls{convex} irregular polygon by a grid; however, the generated meshes of the grid have an increased probability of inconsistency if the irregularity of the basic shape is higher. This problem can be partially remedied by applying relaxation methods such as the \Gls{lloyd algorithm}, but complete regularity of the grid cannot be guaranteed.

\paragraph{Random Assignment}\label{par:random-assignment}

The basic principle of the grid planning method is to place programs connected by a certain weighted graph (figure \ref{fig:weighted-graph-1}) in the given grid in such a way that the most optimal arrangement of their spaces is guaranteed according to the programs. Therefore, this is an abstract rather than a geometric partitioning of space, since the basic shape as an initial condition has already been divided into subspaces through the placement of the grid, and it is only a matter of assigning the programs to the individual spaces in such a way that the weighting of their interconnection is respected in the best possible way.

The simplest implementation of this method involves randomly assigning the programs (figure \ref{fig:iteration-6}) to each of the spaces defined by the grid (figure \ref{fig:grid-1}). This method can work iteratively, since after each random assignment an evaluation result can be computed by solving the function to be optimised. Thus, the randomisation is repeated until a defined satisfaction threshold (figure \ref{fig:final-placement-1}) or even the maximum number of iterations has been exceeded. Obviously, this method is purely \gls{stochastic} and thus represents the least optimised method of grid planning theory.

\begin{figure}
\centering
\begin{subfigure}{.25\textwidth}
\centering
\includegraphics[width=\linewidth]{assets/contribution/dataset/random_grid_graph.pdf}
\caption{Weighted Graph}
\label{fig:weighted-graph-1}
\end{subfigure}%
\begin{subfigure}{.25\textwidth}
\centering
\includegraphics[width=\linewidth]{assets/contribution/dataset/random_grid.pdf}
\caption{Grid}
\label{fig:grid-1}
\end{subfigure}%
\begin{subfigure}{.25\textwidth}
\centering
\includegraphics[width=\linewidth]{assets/contribution/dataset/random_grid_assign1.pdf}
\caption{Iteration}
\label{fig:iteration-6}
\end{subfigure}%
\begin{subfigure}{.25\textwidth}
\centering
\includegraphics[width=\linewidth]{assets/contribution/dataset/random_grid_assign2.pdf}
\caption{Final Placement}
\label{fig:final-placement-1}
\end{subfigure}
\caption{Random Grid Assignment}
\label{fig:random-grid-assignment}
\end{figure}

\paragraph{Iterative Assignment}\label{par:iterative-assignment}

A more optimised approach than the random assignment method is the iterative placement of programs\footcite{egor2020computer}. Here, a random program is first assigned to an equally randomly selected slot. Then the program with the greatest weight associated with the placed element is selected. This is then positioned as close as possible to the already placed program in the grid (figure \ref{fig:iteration-8}). The procedure is \gls{deterministic} because as long as the same basic conditions are given, such as the same programs, their connection weights and the initial program, the same program order will always be generated. Although this method achieves more optimised results than random positioning (figure \ref{fig:final-placement-2}), its \gls{deterministic}, iterative structure carries a high risk of generating suboptimal arrangements, since the algorithm does not use methods that consider a variation of all optimisation function variables at the same time.

\begin{figure}
\centering
\begin{subfigure}{.25\textwidth}
\centering
\includegraphics[width=\linewidth]{assets/contribution/dataset/iterative_grid_graph.pdf}
\caption{Weighted Graph}
\label{fig:weighted-graph-2}
\end{subfigure}%
\begin{subfigure}{.25\textwidth}
\centering
\includegraphics[width=\linewidth]{assets/contribution/dataset/iterative_grid.pdf}
\caption{Grid}
\label{fig:grid-2}
\end{subfigure}%
\begin{subfigure}{.25\textwidth}
\centering
\includegraphics[width=\linewidth]{assets/contribution/dataset/iterative_grid_assign1.pdf}
\caption{Iteration}
\label{fig:iteration-8}
\end{subfigure}%
\begin{subfigure}{.25\textwidth}
\centering
\includegraphics[width=\linewidth]{assets/contribution/dataset/iterative_grid_assign2.pdf}
\caption{Final Placement}
\label{fig:final-placement-2}
\end{subfigure}
\caption{Iterative Grid Assignment}
\label{fig:iterative-grid-assignment}
\end{figure}

\paragraph{Random Swap}\label{par:random-swap}

The best method to solve the arrangement problem without using established optimisation algorithms is the random swap method. Its operation is relatively simple and starts with a random assignment (figure \ref{fig:assignment}) of all programs to the respective spaces defined by the grid. Then the optimisation function is solved and the result is stored. Then the position of two randomly chosen programs is swapped (figure \ref{fig:swap-1}) and the new solution of the optimisation function is compared with the stored result of the previous constellation. If this result is an improvement with respect to the defined target value, the current constellation is kept and the process starts over again (figure \ref{fig:swap-2}). These steps are repeated until the previously defined target threshold is exceeded and the final layout is generated. This method achieves more optimised results than the previously mentioned methods; however, finding an optimal solution is still not guaranteed and, due to its \gls{stochastic} nature, the computational and time requirements of the algorithm cannot be predicted. Therefore, the use of state of the art optimisation methods (sections \ref{subsec:optimisation} and \ref{par:optimisation-algorithms}) is recommended for a concrete application of the grid planning method.

\begin{figure}
\centering
\begin{subfigure}{.25\textwidth}
\centering
\includegraphics[width=\linewidth]{assets/contribution/dataset/swap_grid_graph.pdf}
\caption{Weighted Graph}
\label{fig:weighted-graph-3}
\end{subfigure}%
\begin{subfigure}{.25\textwidth}
\centering
\includegraphics[width=\linewidth]{assets/contribution/dataset/swap_grid_assign1.pdf}
\caption{Assignment}
\label{fig:assignment}
\end{subfigure}%
\begin{subfigure}{.25\textwidth}
\centering
\includegraphics[width=\linewidth]{assets/contribution/dataset/swap_grid_assign2.pdf}
\caption{Swap 1}
\label{fig:swap-1}
\end{subfigure}%
\begin{subfigure}{.25\textwidth}
\centering
\includegraphics[width=\linewidth]{assets/contribution/dataset/swap_grid_assign3.pdf}
\caption{Swap 2}
\label{fig:swap-2}
\end{subfigure}
\caption{Random Swap Grid Assignment}
\label{fig:random-swap-grid-assignment}
\end{figure}

\subsubsection{Aggregation}\label{subsubsec:aggregation}

Another method that differs significantly from the previous ones is the family of aggregation methods in automatic floor plan design. A key feature of these methods is the ability to generate floor plans of irregular shape (figure \ref{fig:aggregation-results}), such as balconies that extend beyond the rectangular boundary of the building's cross-section, or courtyards that split the floor plan by creating a void in the center of the layout. Aggregation methods do not require predefined outlines to initiate the generative process, but rely on the formal definition of individual spaces in advance, which are then combined into an overall plan through various methods. Apart from the increased formal variety that this method allows, the possibility of defining the formal aspects of the spaces to be placed in advance is also a significant advantage of aggregation methods. However, due to their specific logic, it is always necessary to evaluate a function to be optimised in order to aggregate the spaces according to certain conditions. This means that in most practical applications of these methods it becomes necessary to apply optimisation algorithms\footcite{nisztuk2019tool}. In general, the use of such algorithms inevitably means an increased demand for computing power and thus an increased runtime, which can lead to complications when trying to integrate this type of automatic space planning methods into a parametric dataset generation pipeline.

\begin{figure}
\centering
\begin{subfigure}{.35\textwidth}
\centering
\includegraphics[width=\linewidth]{assets/contribution/dataset/aggregation1.pdf}
\label{fig:aggregation-1}
\end{subfigure}%
\begin{subfigure}{.39\textwidth}
\centering
\includegraphics[width=\linewidth]{assets/contribution/dataset/aggregation2.pdf}
\label{fig:aggregation-2}
\end{subfigure}%
\begin{subfigure}{.26\textwidth}
\centering
\includegraphics[width=\linewidth]{assets/contribution/dataset/aggregation3.pdf}
\label{fig:aggregation-3}
\end{subfigure}
\caption{Aggregation Results}
\label{fig:aggregation-results}
\end{figure}

\paragraph{Optimisation Algorithms}\label{par:optimisation-algorithms}

In order to successfully perform an aggregation of the initially defined bodies, the individual space contours must be shifted on the same two-dimensional plane in such a way that they are arranged in the most optimal manner possible. But what does optimal mean and how can it be evaluated? Since rational mathematical methods are involved, it is essential to create an evaluation scale in order to be able to compare one generated layout with another in terms of its degree of optimisation. In other words, it is necessary to establish a universal optimisation evaluation formula that assigns a value to each individual layout which must be maximised or minimised in order to achieve an optimum in the design evaluation.

This in itself can be a great challenge, as some of the criteria are highly subjective, purely architectural and extremely variable both locally and over time. This difficulty in defining an optimal design in architecture is the core problem of automatic floor plan generation\footcite{nisztuk2019hybrid}. However, since the present experiment is not concerned with the generation of individually optimised plans, but rather with the generation of the broadest possible architectural dataset, it is possible to include mainly objective design criteria as variables in the function to be optimised. Specifically, this means that the formula evaluates how close the individual rooms are to each other, but without overlapping, since unwanted gaps in an architectural floor plan are just as much a design flaw as an overlap of defined rooms. Furthermore, weighted graphs, which formulate the desired local and topological relationships between spaces as a formula to be optimised, can be used to evaluate the implementation of programmatic requirements in the final design. The parameters to be varied are thus represented by the two-dimensional coordinates of each space in the function, and the evaluation of the layout is solved by computing the weighted or unweighted graph.

In order to find the optimum of the formula defined in this way, optimisation algorithms and methods are applied, which exist in a great variety\footcite{pena2021artificial} in mathematical, physical and computer science contexts. Among the better known methods for the successful optimisation of multivariable non-linear functions are the evolutionary algorithms\footcites{caetano2020computational}{grzesiak2021evolutionary}, which, based on the natural model, rely on concepts such as populations, natural selection, crossover and mutation. Or the Simulated Annealing Algorithm, which, as a probabilistic method, also provides strategies to avoid local maxima by continuously reducing the probability weighting to find a global maximum of the function to be optimised. Of course, machine learning methods such as reinforcement learning can also be used for function optimisation.

\paragraph{Agent Based}\label{par:agent-based}

A slightly different approach to the definition of a function to be globally optimised is the establishment of independently acting agents. Each of the predefined spatial elements (figure \ref{fig:initial-disposition}) is given a certain autonomy as well as an evaluation capacity, so that the individual agents can perform different local transformations (figure \ref{fig:iteration-7}) on a two-dimensional level without deforming the space and find their optimised position in the overall layout by solving an optimisation function corresponding to the individual variables. Possible transformations include variation of the X and Y coordinates of the shape-defining points and rotation of the whole space to match the rotation of another body by calculating their degree of alignment.

\begin{figure}
\centering
\begin{subfigure}{.31\textwidth}
\centering
\includegraphics[width=\linewidth]{assets/contribution/dataset/agent_based1.pdf}
\caption{Initial Position}
\label{fig:initial-disposition}
\end{subfigure}%
\begin{subfigure}{.32\textwidth}
\centering
\includegraphics[width=\linewidth]{assets/contribution/dataset/agent_based2.pdf}
\caption{Iteration}
\label{fig:iteration-7}
\end{subfigure}%
\begin{subfigure}{.34\textwidth}
\centering
\includegraphics[width=\linewidth]{assets/contribution/dataset/agent_based3.pdf}
\caption{Final Layout}
\label{fig:final-layout-1}
\end{subfigure}
\caption{Agent Based Aggregation}
\label{fig:agent-based-aggregation}
\end{figure}

\paragraph{Physics Solver}\label{par:physics-solver}

Furthermore, the optimisation formula can also follow physical laws, so that the problems to be solved can be defined and calculated as physical constraints. In the concrete application for the arrangement of defined spatial bodies to an optimal floor plan layout, this means that the weighting of the individual graph edges of the topological graph, which represents the relationship of the individual spaces to each other, is simulated by physical spring forces corresponding to the weighting of the edges (figure \ref{fig:physics-solver-aggregation}). The individual spaces in this physical simulation are defined as solid bodies, which are therefore not penetrable and whose motion is limited to two dimensions. To add some variety to the generation process, the \gls{deterministic} nature of such a simulation needs to be complemented by randomness. This is achieved by arbitrarily positioning the individual rooms in two-dimensional space as the starting position of the simulation or regulating the strength and intensity of the simulated forces.

\begin{figure}
\centering
\begin{subfigure}{.33\textwidth}
\centering
\includegraphics[width=\linewidth]{assets/contribution/dataset/physics_solver1.pdf}
\caption{Initial Position}
\label{fig:initial-disposition-1}
\end{subfigure}%
\begin{subfigure}{.33\textwidth}
\centering
\includegraphics[width=\linewidth]{assets/contribution/dataset/physics_solver2.pdf}
\caption{Final Layout I}
\label{fig:final-layout-i}
\end{subfigure}%
\begin{subfigure}{.33\textwidth}
\centering
\includegraphics[width=\linewidth]{assets/contribution/dataset/physics_solver3.pdf}
\caption{Final Layout II}
\label{fig:final-layout-ii}
\end{subfigure}
\caption{Spring Force Aggregation}
\label{fig:physics-solver-aggregation}
\end{figure}

\paragraph{Refinement Strategies}\label{par:refinement-strategies}

The different aggregation strategies share similar methodological advantages and disadvantages. As mentioned above, the arrangement of the individual subbodies, by virtue of their formal definition in advance, allows for a high degree of design diversity and a relatively accurate representation of architectural design practice. However, the fundamental drawbacks of this logic should also be noted. Due to the necessary application of optimisation algorithms or force simulation in the case of the physical solver method, any aggregation strategy requires a large number of iterative, relatively complex calculations, which are reflected in a high computing power requirement and consequently in a considerable time consumption.

However, the most serious drawback of such methods is the lack of a possibility to guarantee a gapless generation of the final floor plan. Since all methods consider the distance function as part of the formulas to be optimised, the distances will tend towards zero, but without a certainty of reaching exactly zero. This makes the application of \textit{gap-removal algorithms} after the generation process inevitable. A slightly modified version of the shape-packing algorithms\footcite{jabi2013potential} (figure \ref{fig:shape-packing-algorithm-results}) can be applied here, which consists of moving the individual spaces in such a way that these gaps are reduced. However, this is only a minimisation of the individual gaps and not a complete resolution of them.

\begin{figure}
\centering
\begin{subfigure}{.5\textwidth}
\centering
\includegraphics[width=0.97\linewidth]{assets/contribution/dataset/packing1.pdf}
\label{fig:shape-packing-1}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
\centering
\includegraphics[width=0.97\linewidth]{assets/contribution/dataset/packing2.pdf}
\label{fig:shape-packing-2}
\end{subfigure}
\caption{Shape Packing Algorithm}
\label{fig:shape-packing-algorithm-results}
\end{figure}

In a final step, it is therefore common to assign the individual gaps to the areas of the different rooms in a random or regularised manner. This allows for the complete elimination of free spaces, but has the consequence that the morphology of the spaces no longer exactly matches the previously defined shape, and thus problems such as lack of orthogonality may arise.

\subsubsection{Shape Grammar}\label{subsubsec:shape-grammar}

In contrast to subdivision or aggregation methods, the so-called shape grammar\footcite{hong2022five} (section \ref{subsec:shape-grammar}), which, by defining generic geometric or topological rules, can generate a large number of different design layouts that always follow the same rules. In the shape-grammar approach, the initial element can be a single room or the floor plan to be subdivided, providing a greater variety of starting conditions than the previously described methods.

Since the rules of this method can be geometric transformations, the positioning of new shapes or even the generation of new shapes by intersecting given bodies, both subdivision and aggregation processes can take place within the framework of shape grammar. It thus represents a promising methodology for the automatic generation of floor plans\footcite{ruiz2013design}. However, it can be observed that its application in architectural practice is limited to a few project prototypes and has not found a place in everyday design. The reason for this could be the high complexity of its structure, since a clear, precise and unambiguous formulation of the different rules requires a fundamental understanding of basic geometric operations and patterns. Therefore, no meaningful application of shape grammar could be found in this research, as scientific or documentary sources on this method are rare and of limited quality and depth.

The various implementations of shape grammar processing tools all require a digital component that translates the formulated rules into machine processes, the so-called \textit{interpreter}, which is thus an essential component of the method. There are several widely differing implementations of this form grammar interpreter, and relatively little insight into its function, making it difficult to use legitimately.

However, a renewed interest in shape grammar methods in recent years shows promise in new interpreter implementations\footcite{muslimin2023experience} which, in addition to the basic geometric rules, can potentially respect topological relations and thus certain programmatic requirements of the designer.

\section{Parametric Framework}\label{sec:parametric-framework}

The advantages and drawbacks of the individual methods and algorithms were compared using various criteria in order to make a final selection of the methods to be integrated into the synthetic dataset generation process. It was decided to use a modified recursive subdivision and a Voronoi-based subdivision due to their comparatively low complexity and computational requirements. Furthermore, an intermediate method of the two algorithms mentioned above is used, where the layout generated by the recursive subdivision is used as input for a Voronoi diagram subdivision\footcite{balzer2005voronoi}, so that the centers of the existing spaces become the Voronoi seed points. During the research process it was not possible to find aggregation methods that did not exceed the computational and thus the temporal capacities. For the same reasons, it was not possible to implement a reliable method based on shape grammar rules.

Now that the three methods belonging to the family of division algorithms had been selected, the data required for the correct execution of the algorithms could be clearly defined. First, a basic geometric shape was needed to represent the outline of the floor plan to be divided. The convex hull method for generating irregular polygons and an algebraic method for generating randomly sized rectangles and squares were combined with a method for generating slightly deformed rectangles in an outline generation method. Since the goal is to automatically generate as diverse a dataset as possible, the individual outline morphologies were assigned different probabilities of being selected as the initial shape generation method for the current generation step. In addition, the dimensions of the rectangles and squares, the placement of the convex hull points and the deformation of the sides of the rectangles were carefully given a certain degree of randomness, but without running the risk of taking architecturally unrealistic forms, such as very low aspect ratios of the rectangles or very sharp angles.

\begin{figure}
\centering
\begin{subfigure}{.33\textwidth}
\centering
\includegraphics[width=\linewidth]{assets/contribution/dataset/input.pdf}
\caption{Amount and Room Sizes}
\label{fig:amount-and-room-sizes}
\end{subfigure}%
\begin{subfigure}{.33\textwidth}
\centering
\includegraphics[width=\linewidth]{assets/contribution/dataset/binary_tree.pdf}
\caption{Constructed Binary Tree}
\label{fig:constructed-binary-tree}
\end{subfigure}%
\begin{subfigure}{.33\textwidth}
\centering
\includegraphics[width=\linewidth]{assets/contribution/dataset/outline.pdf}
\caption{Area of Outline}
\label{fig:area-of-outline}
\end{subfigure}
\caption{Binary Tree Computation with Percentages}
\label{fig:binary-tree-computation-with-percentages}
\end{figure}

Another important input to the floor plan generation methods was the pre-specification of the size ranges of the respective rooms in relation to the total number of rooms selected in the current step of the generation process (section \ref{subsec:input-parameter}). Thus, by randomly selecting items from this list, the generation algorithm was allowed to obtain information about the program of desired rooms and their respective maximum and minimum sizes (figure \ref{fig:amount-and-room-sizes}). The number of rooms to be placed is determined by the total area of the arbitrary initial shape (figure \ref{fig:area-of-outline}) and is equally communicated to the generation algorithm.

One challenge was to create an iterative subdivision strategy that would allow the walls to be placed in such a way that, in the final layout, the individual rooms would have their exact percentage size corresponding to the generated outline. To achieve this, it was first necessary to create \glspl{binary tree} (figure \ref{fig:constructed-binary-tree}), each of which contained the desired size percentage of the parent rooms as child leaves. A \textit{\gls{downhill simplex algorithm}} was then used to optimise the placement of the cutting planes and the weighting of each Voronoi cell so that the percentage subdivision (figure \ref{fig:space-division-by-percentages}) specified by the \gls{binary tree} was maintained as closely as possible.

\begin{figure}
\centering
\begin{subfigure}{.33\textwidth}
\centering
\includegraphics[width=.97\linewidth]{assets/contribution/dataset/iteration_1.pdf}
\caption{First Iteration}
\label{fig:first-iteration-1}
\end{subfigure}%
\begin{subfigure}{.33\textwidth}
\centering
\includegraphics[width=.97\linewidth]{assets/contribution/dataset/iteration_2.pdf}
\caption{Second Iteration}
\label{fig:second-iteration-2}
\end{subfigure}%
\begin{subfigure}{.33\textwidth}
\centering
\includegraphics[width=.97\linewidth]{assets/contribution/dataset/iteration_3.pdf}
\caption{Third Iteration}
\label{fig:third-iteration}
\end{subfigure}
\caption{Space Division by Percentages}
\label{fig:space-division-by-percentages}
\end{figure}

The final step in the geometry generation process was the transformation of the previously purely two-dimensional apartment floor plans into three-dimensional cell elements by topological operations through the extrusion of the individual surface elements along the Z axis, in order to generate a three-dimensional \acrshort{bim} model. This whole process was implemented in the programming language \Gls{python} and was essentially enabled by the use of certain libraries such as \textit{TopologicPy}, \textit{NumPy}\footcite{2020NumPy-Array}, \textit{SciPy}\footcite{2020SciPy-NMeth} and \textit{PyVoro}\footcite{rycroft2007multiscale}.

A particularly common method of parametric geometry generation in architecture is the use of visual scripting languages (figure \ref{fig:visual-scripting-language}), which allow direct visualisation of the results of the individual substeps of a parametric generation pipeline and are thus more intuitive to use than abstract programming languages. In this thesis a large amount of experimentation has been done in the visual scripting interface \textit{Sverchok}\footcite{sverchok2013}. This is an add-on written in \Gls{python} for the 3D manipulation software \textit{Blender}\footcite{blender}. It quickly became apparent that such an interface has strong advantages during the experimental phase, but is not suitable for the generation of large datasets due to a high memory load and a lower portability as well as stability. Therefore, a pure \Gls{python} implementation was preferred for the final generation. An overview of the whole dataset generation pipeline is shown in figure \ref{fig:flow-diagram-of-the-framework}.

\begin{figure}
\centering
\includegraphics[width=\textwidth]{assets/contribution/dataset/visual_scripting.pdf}
\caption{Visual Scripting Language}
\label{fig:visual-scripting-language}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=\textwidth]{assets/contribution/dataset/framework.pdf}
\caption{Structure of the Framework}
\label{fig:flow-diagram-of-the-framework}
\end{figure}

\section{Architectural Rules}\label{sec:architectural-rules}

Throughout the construction of the parametric structure, a great deal of attention has been paid to the constant verification and guarantee of the architectural feasibility of each floor plan, since the ambition was to create a dataset that claims to reflect, to a certain extent, the state of architectural reality. The first architecturally regulated instance consisted in defining the various architectural programs, such as living room, bathroom, bedroom, toilet and utility rooms, and assigning them to the corresponding apartment types (table \ref{tab:minimum-room-sizes-per-room-amount} and table \ref{tab:maximum-room-sizes-per-room-amount}). For example, a ten-room apartment has four bedrooms, a living room with a kitchen and two bathrooms, a toilet and two utility rooms. A three-room flat, on the other hand, only has a living room with a kitchen, a bedroom and a bathroom. This list was then used to determine the maximum and minimum room sizes for each individual apartment constellation and room.

In addition, architectural control functions were implemented in the process of generating the apartment outline to be subdivided, which made it possible to eliminate unrealistic or particularly atypical plan morphologies in advance. These functions are also used to control the maximum and minimum size of the total area of the residential object to be generated. Here the values $27.5 m^2$ have been set as the minimum acceptable size and $136.5 m^2$ as the maximum acceptable size. The minimum acceptable aspect ratio of these plans was set to 0.7. For the non-orthogonal plans, a topological compactness measure is applied to make an appropriate selection.

The next architectural control instance is represented by a function that analyses the generated apartment plan in terms of architectural criteria and decides whether it is an acceptable result (figure \ref{fig:good-layout}) or whether the generation process needs to be repeated for this iteration. For example, this function verifies whether there are exterior or interior walls in the design that are unrealistically short in relation to the size of the apartment, or whether there are rooms whose \textit{\gls{degree of compactness}} is below an acceptable value, namely too long or too deformed (figure \ref{fig:bad-layout-i} and \ref{fig:bad-layout-ii}). If, despite the implemented optimisation function, a floor plan generated per iteration produces rooms that are too small or too large in relation to the other rooms or even the size of the apartment, these plans are equally classified as architecturally unacceptable and sorted out. This architectural control, both before and after the application of the algorithms, makes it therefore possible to synthetically generate \acrshort{bim} models that represent the architectural reality in the best possible way.

\begin{figure}
\centering
\begin{subfigure}{.32\textwidth}
\centering
\includegraphics[width=0.97\linewidth]{assets/contribution/dataset/layout_bad1.pdf}
\caption{Low Aspect Ratio}
\label{fig:bad-layout-i}
\end{subfigure}%
\begin{subfigure}{.34\textwidth}
\centering
\includegraphics[width=0.97\linewidth]{assets/contribution/dataset/layout_bad2.pdf}
\caption{Sharp Angles}
\label{fig:bad-layout-ii}
\end{subfigure}%
\begin{subfigure}{.32\textwidth}
\centering
\includegraphics[width=0.97\linewidth]{assets/contribution/dataset/layout_good.pdf}
\caption{Good layout}
\label{fig:good-layout}
\end{subfigure}
\caption{Layout Evaluation}
\label{fig:layout-evaluation}
\end{figure}

\section{Post-Processing}\label{sec:post-processing}

The three-dimensional apartment models, which constitute the final result of the previous steps, consist of rooms with their respective programs and their arrangement in an apartment constellation. However, in order to be able to speak of true \acrshort{bim} models, important architectural elements are missing, such as doors, which represent the topological connection between the individual rooms, and also windows, which represent the connection to the surrounding medium. These elements can be summarised under the term \glspl{aperture} and are represented and stored in the geometry engine used as subsurfaces associated with their respective walls. In order to be able to generate them automatically, certain topological tools have to be applied so that a logical placement of the individual elements becomes possible.

Firstly, in order to place the door elements between individual rooms, a graph object is created which connects the individual rooms in such a way that all topologically adjacent rooms are connected by edges (figure \ref{fig:delaunay-triangulation}). The vertices of this network represent the individual spaces. Once this graph has been determined, a \acrlong{mst} of the respective network can be computed, which represents the subgraph of the original graph connecting all vertices without forming any cycles (figure \ref{fig:mst-computation}). Furthermore, this tree is computed in such a way that the sum of its edge weights is as small as possible.

The next step is to relate this graph to the basic geometry by topologically identifying the walls that connect two spaces in the \acrlong{mst}. Finally, each of the wall objects found in this way is assigned a door \gls{aperture} in order to create a more architecturally realistic three-dimensional model (figure \ref{fig:door-generation-process}).

\begin{figure}
\centering
\begin{subfigure}{.33\textwidth}
\centering
\includegraphics[width=\linewidth]{assets/contribution/dataset/window_generation1.pdf}
\caption{Delaunay Triangulation}
\label{fig:delaunay-triangulation}
\end{subfigure}%
\begin{subfigure}{.33\textwidth}
\centering
\includegraphics[width=\linewidth]{assets/contribution/dataset/window_generation2.pdf}
\caption{MST Computation}
\label{fig:mst-computation}
\end{subfigure}%
\begin{subfigure}{.33\textwidth}
\centering
\includegraphics[width=\linewidth]{assets/contribution/dataset/window_generation3.pdf}
\caption{Door Placement}
\label{fig:door-placement}
\end{subfigure}
\caption{Door Generation Process}
\label{fig:door-generation-process}
\end{figure}

Once the doors have been placed, the window \gls{aperture} generation can begin. First, however, to provide greater variety in the dataset, each generated apartment model is duplicated and rotated along the Z-axis in 90 degree increments to create four new models that differ only in their orientation. This allows the same apartment structure to be stored in the training dataset with an orientation in all four cardinal directions. Once this duplication and rotation operation is complete, four different window arrangements are generated for each rotated apartment model, which are then stored as final geometries in the dataset (figure \ref{fig:layout-variants-diagram}).

\begin{figure}
\centering
\includegraphics[width=\textwidth]{assets/contribution/dataset/rotation.pdf}
\caption{Layout Variation Process}
\label{fig:layout-variants-diagram}
\end{figure}

The method of generating the individual window \glspl{aperture} is relatively simple, as it is basically a matter of randomly calculating the \textit{\glspl{glazing percentage}} per wall. Since this is the preparation of models for an energy simulation, the shape of the windows is not critical; only their orientation and size play a significant role in the energy efficiency calculation. Similarly, each exterior wall is given a probability of not containing a window \gls{aperture} in order to remain faithful to architectural reality, since of course not every exterior wall of a building necessarily carries a window.

\subsection{Information Annotation}\label{subsec:information-annotation}

The final geometry model has thus been successfully generated, but it still requires the annotation of certain information in order to represent a valid \acrshort{bim} model and start the next iteration. This information is fundamental to the creation of this dataset, since the aim is the computation of \glspl{knowledge graph} that contain certain dimensional, positional and evaluative data. To understand how such information can be stored in the model, it is necessary to look at the structure of the geometry files. In the generation pipeline presented here, the data is stored using \textit{\acrfull{json}} file formats as the frame structure (section \ref{subsec:bim-model-storage}) and the geometry information is stored using \textit{\acrlong{brep} Strings} (\acrshort{brep}-Strings) (section \ref{subsec:geometry-storage}).

The \acrshort{json} structure is a text-based file format which, like the \acrshort{ifc} file format mentioned earlier (section \ref{subsec:graphs-in-bim}), allows information to be related in a hierarchical and structured way, so that specific key-value pairs can be assigned to individual geometry elements and stored and read in a structured way. First, the \acrshort{brep} string of the geometry is stored as a value of the 'brep' key at the top level of the structure, and an associated \textit{\gls{dictionary} element} is added containing information about the type of element, the number of rooms, the different room types and the \acrlong{id} number of the geometry. The individual rooms are then also assigned \gls{dictionary} elements by referencing selectors containing information about the element type, the total area in $m^2$, the respective program of the room and the \acrlong{id} number of the room. This structured form of data representation also makes it possible to store \gls{aperture} elements such as doors and windows as separate geometries, and to associate individual \gls{dictionary} objects with them, containing information about their element type, their area in $m^2$, their orientation, and their architectural role.

\subsection{Energy Performance Simulation}\label{subsec:energy-performance-simulation}

The generated and annotated \acrshort{bim} model now contains information about its geometry, topology, program and dimensions, but essential information about its energy efficiency is still missing. To obtain this data, careful energy simulations have to be performed for each individual apartment model. Furthermore, as the aim is to develop an automated data generation process, these simulations need to be parameterised and integrated into the general pipeline in order to automatically add the calculated energy-related information to the \acrshort{json} file of the \acrshort{bim} model. For this purpose, the \textit{Openstudio}\footcite{guglielmetti2011openstudio} software tool is integrated into the parametric framework, providing, among other functionalities, simplified access to the \textit{EnergyPlus}\footcite{crawley2001energyplus} energy simulation software.

In order to carry out a thorough energy simulation in \textit{EnergyPlus}, a variety of information is required, presented in different file formats. First, a so-called \textit{\acrfull{epw}} file is required, which provides general, historical and detailed information about the climatic conditions of the chosen environment for the simulation. A design day file is also required, which provides information about the specific \textit{\acrfull{ashrae}} design conditions. However, the most important component of the energy simulation is the creation of a \textit{\acrlong{osm}}, stored in a human readable text format called \acrshort{osm}. In the case of this work, this is a partial simulation model that contains basic information about the materials and composition of each element of the residential model (section \ref{subsec:energy-simulation-parameter}), as well as individually created operating and loading schedules that comply with the \acrshort{ashrae} standards. These schedules provide a variety of different information for each individual room program, namely the number of people over time, the technical equipment of the room, the heat emitting light sources and much more.

Finally, all these simulation conditions are merged with the generated \acrshort{bim} models and their stored information to initiate the energy simulations. The result of this simulation is presented in a common database form, \textit{\acrfull{sql}}, and can be easily and accurately queried. This now allows energy efficiency information to be added to the original \acrshort{bim} model. Many different energy simulation values can be included, providing information on a variety of different parameters and performances\footcite{aksin2021use}. However, in this work the initial focus was on the total value of energy consumption per area in $MJ/m^2$, which was then automatically added to each of the \acrshort{bim} models individually. The described energy simulation process is shown in figure \ref{fig:energy-simulation-process}.

\begin{figure}
\centering
\includegraphics[width=\textwidth]{assets/contribution/dataset/energy_simulation_diagram.pdf}
\caption{Energy Simulation Process}
\label{fig:energy-simulation-process}
\end{figure}

\subsection{Data Mapping}\label{subsec:data-mapping}

Although the individual \acrshort{bim} models thus generated are now complete, the stored information still needs to be partially processed and manipulated to produce a qualitative dataset. Each room and window element already carries numerical values about their respective programs, dimensions and orientations, corresponding to the cardinal points, but these must be converted into distinguishable classes, due to the way graph machine learning methods work. For this purpose, 91 classes are created, which describe the type of window or the respective program of the rooms, a relative size ranging from XXS to XXL and the respective cardinal direction of the windows from north to northwest in clockwise direction (table \ref{tab:node-label-calculation}). The cardinal direction and the type description are already available in the saved format, but the relative size calculation requires an additional step.

In order to classify the individual elements into the seven relative size categories, the \glspl{quantile} of the total representative size information of all generated geometries must be calculated (section \ref{subsec:node-label-calculation}). This means that, for example, a living room classified as XL is relatively large compared to the average living room size generated by the algorithm and controlled by the architectural function. It is also necessary to calculate relative energy classes per apartment layout based on energy consumption values. This is similarly done by considering the full range of simulated energy values and dividing them into the desired number of classes, five in the case of this experiment, by \glspl{quantile} representing the minimum and maximum values of each energy class (section \ref{subsec:energy-class-definition}).

This means that, after successful execution of the data mapping process, the \gls{dictionary} objects of the individual rooms and windows have now been extended by a 'label' key, which carries a category from 0 to 91 as a value and describes the essential relative information of the individual element, such as: window - medium - south-east or bathroom - extra small.

\subsection{Graph Retrieval}\label{subsec:graph-retrieval}

The basis for generating synthetic graph datasets is thus complete and functional; the only thing missing is the automatic translation from the geometric information model to the topological \gls{knowledge graph} (figure \ref{fig:final-layouts-with-corresponding-graphs}). To achieve this as smoothly as possible, the capabilities of the \Gls{python} library \textit{TopologicPy} were used, as it offers the possibility to generate different topological graphs based on geometric objects and to automatically assign information to each node and edge.

\begin{figure}
\centering
\begin{subfigure}[][][c]{.33\textwidth}
\centering
\includegraphics[width=.95\textwidth]{assets/contribution/dataset/apartment1.pdf}
\label{fig:apartment-1}
\end{subfigure}%
\begin{subfigure}[][][c]{.33\textwidth}
\centering
\includegraphics[width=.95\textwidth]{assets/contribution/dataset/apartment2.pdf}
\label{fig:apartment-2}
\end{subfigure}%
\begin{subfigure}[][][c]{.33\textwidth}
\centering
\includegraphics[width=.95\textwidth]{assets/contribution/dataset/apartment3.pdf}
\label{fig:apartment-3}
\end{subfigure}
\caption{Final Layouts with Corresponding Graphs}
\label{fig:final-layouts-with-corresponding-graphs}
\end{figure}

The particularity of the generated graphs is that the individual nodes not only represent the individual spaces and their topological relationship, but also, starting from the individual spaces, edges are drawn to the window \glspl{aperture} associated with the spaces. This means that the resulting \gls{knowledge graph} objects represent a topological description of the room constellation and window arrangement for the entire apartment. Furthermore, the topological graph generation method allows an automated and regularised information transfer between the \gls{dictionary} objects of the geometry and the elements of the graph. Thus, the individual nodes receive the previously generated label keys with the corresponding categories of the individual rooms and windows (table \ref{tab:node-data}). Similarly, the individual graph objects receive general dictionaries, which in this case contain the \textit{\gls{site energy consumption} per surface} value in $MJ/m^2$ and the respective energy class as well as an individual reference number (table \ref{tab:graph-data}).

The complete \gls{knowledge graph} objects are then stored as binary objects using the \Gls{python} \textit{\acrfull{dgl}}\footcite{wang2019dgl} to build the final dataset.

\section{Outcome}\label{sec:outcome-synthetic-data-generation}

\subsection{Results}\label{subsec:results}

Through the described construction of the generation pipeline, it was possible to generate a large dataset of 40 000 synthetic housing graphs in a relatively short time. Key aspects such as the selection of the best geometry partitioning methods, the construction of the parametric framework, the architectural control function, and the information retrieval and annotation were satisfactorily handled and allowed a broad insight into the respective tools and algorithms. An essential requirement of the dataset was the morphological and architectural versatility of the individual \acrshort{bim} models, which was made possible by the application of different partitioning methods, namely Voronoi diagram, optimised recursive subdivision and the combination of both.

The final dataset consists of two complementary parts, which are stored separately. The first dataset represents the totality of the generated housing geometries including the generated \glspl{aperture} and the stored information about the individual elements (figure \ref{fig:geometry-dataset} and section \ref{subsec:bim-model-storage}). The second part consists of the corresponding \glspl{knowledge graph} of the individual apartment geometries, which contain essential information such as the energy efficiency and topology of the individual apartment layouts (figure \ref{fig:corresponding-graph-dataset} and tables \ref{tab:graph-data}, \ref{tab:node-data} and \ref{tab:edge-data}).

Several methods were used to verify the quality of the generated dataset. Firstly, an arbitrary visualisation of the geometries and corresponding graphs was performed, with a focus on controlling for unwanted regularities. This revealed a high variance in the layout design, indicating a satisfactory result. However, due to the large amount of data involved, a statistical analysis of the datapoints and associated information was required.

Accordingly, the correlations of the three variables: opening area in $m^2$, total surface in $m^2$ and energy consumption in $MJ/m^2$ were first plotted in order to detect any unexpected patterns or relationships (figure \ref{fig:relation-between-opening-area-surface-and-energy-consumption}). This and the analysis of the correlations between the energy consumption in $MJ/m^2$, the id number as $\in N$ and the total surface in $m^2$ (figure \ref{fig:relation-between-energy-consumption-layout-id-and-surface}) showed a satisfactory distribution of the individual information variables. Some logical relationships could also be verified for the legitimacy of the dataset, such as the linear relationship between total surface and total window opening area, or the increased energy consumption for larger apartments.

The distribution of the values for total surface (figure \ref{fig:distribution-of-surface-value}), energy consumption (figure \ref{fig:distribution-of-energy-consumption-value}) and total window opening area (figure \ref{fig:distribution-of-opening-area-value}) was also examined to confirm the variance of the dataset. A satisfactory distribution of the total surface was found within the defined maximum and minimum values, which was likewise the case for the distribution of the total window opening area. The distribution of energy consumption values showed a concentration around the value of 660 $MJ/m^2$ with a relatively strong increase on the left side of the peak. Whereas the decrease on the right side of the graph shows a much less steep gradient and extends up to the values at 1100 $MJ/m^2$. Thus indicating that the energy consumption values in this dataset are non-symmetrically distributed.

\begin{figure}
\centering
\vspace{20pt}
\includegraphics[width=.2\textwidth]{assets/contribution/dataset/results/1.pdf}\hfill
\includegraphics[width=.2\textwidth]{assets/contribution/dataset/results/2.pdf}\hfill
\includegraphics[width=.2\textwidth]{assets/contribution/dataset/results/3.pdf}\hfill
\includegraphics[width=.2\textwidth]{assets/contribution/dataset/results/4.pdf}\hfill
\includegraphics[width=.2\textwidth]{assets/contribution/dataset/results/5.pdf}
\\[\smallskipamount]
\includegraphics[width=.2\textwidth]{assets/contribution/dataset/results/6.pdf}\hfill
\includegraphics[width=.2\textwidth]{assets/contribution/dataset/results/7.pdf}\hfill
\includegraphics[width=.2\textwidth]{assets/contribution/dataset/results/8.pdf}\hfill
\includegraphics[width=.2\textwidth]{assets/contribution/dataset/results/9.pdf}\hfill
\includegraphics[width=.2\textwidth]{assets/contribution/dataset/results/10.pdf}
\\[\smallskipamount]
\includegraphics[width=.2\textwidth]{assets/contribution/dataset/results/11.pdf}\hfill
\includegraphics[width=.2\textwidth]{assets/contribution/dataset/results/12.pdf}\hfill
\includegraphics[width=.2\textwidth]{assets/contribution/dataset/results/13.pdf}\hfill
\includegraphics[width=.2\textwidth]{assets/contribution/dataset/results/14.pdf}\hfill
\includegraphics[width=.2\textwidth]{assets/contribution/dataset/results/15.pdf}
\\[\smallskipamount]
\includegraphics[width=.2\textwidth]{assets/contribution/dataset/results/16.pdf}\hfill
\includegraphics[width=.2\textwidth]{assets/contribution/dataset/results/17.pdf}\hfill
\includegraphics[width=.2\textwidth]{assets/contribution/dataset/results/18.pdf}\hfill
\includegraphics[width=.2\textwidth]{assets/contribution/dataset/results/19.pdf}\hfill
\includegraphics[width=.2\textwidth]{assets/contribution/dataset/results/20.pdf}
\\[\smallskipamount]
\includegraphics[width=.2\textwidth]{assets/contribution/dataset/results/21.pdf}\hfill
\includegraphics[width=.2\textwidth]{assets/contribution/dataset/results/22.pdf}\hfill
\includegraphics[width=.2\textwidth]{assets/contribution/dataset/results/23.pdf}\hfill
\includegraphics[width=.2\textwidth]{assets/contribution/dataset/results/24.pdf}\hfill
\includegraphics[width=.2\textwidth]{assets/contribution/dataset/results/25.pdf}
\\[\smallskipamount]
\includegraphics[width=.2\textwidth]{assets/contribution/dataset/results/26.pdf}\hfill
\includegraphics[width=.2\textwidth]{assets/contribution/dataset/results/27.pdf}\hfill
\includegraphics[width=.2\textwidth]{assets/contribution/dataset/results/28.pdf}\hfill
\includegraphics[width=.2\textwidth]{assets/contribution/dataset/results/29.pdf}\hfill
\includegraphics[width=.2\textwidth]{assets/contribution/dataset/results/30.pdf}
\\[\smallskipamount]
\includegraphics[width=.2\textwidth]{assets/contribution/dataset/results/31.pdf}\hfill
\includegraphics[width=.2\textwidth]{assets/contribution/dataset/results/32.pdf}\hfill
\includegraphics[width=.2\textwidth]{assets/contribution/dataset/results/33.pdf}\hfill
\includegraphics[width=.2\textwidth]{assets/contribution/dataset/results/34.pdf}\hfill
\includegraphics[width=.2\textwidth]{assets/contribution/dataset/results/35.pdf}
\\[\smallskipamount]
\includegraphics[width=.2\textwidth]{assets/contribution/dataset/results/36.pdf}\hfill
\includegraphics[width=.2\textwidth]{assets/contribution/dataset/results/37.pdf}\hfill
\includegraphics[width=.2\textwidth]{assets/contribution/dataset/results/38.pdf}\hfill
\includegraphics[width=.2\textwidth]{assets/contribution/dataset/results/39.pdf}\hfill
\includegraphics[width=.2\textwidth]{assets/contribution/dataset/results/40.pdf}
\caption{Geometry Dataset}
\label{fig:geometry-dataset}
\end{figure}

\begin{figure}
\centering
\vspace{20pt}
\includegraphics[width=.2\textwidth]{assets/contribution/dataset/results_graphs/1.pdf}\hfill
\includegraphics[width=.2\textwidth]{assets/contribution/dataset/results_graphs/2.pdf}\hfill
\includegraphics[width=.2\textwidth]{assets/contribution/dataset/results_graphs/3.pdf}\hfill
\includegraphics[width=.2\textwidth]{assets/contribution/dataset/results_graphs/4.pdf}\hfill
\includegraphics[width=.2\textwidth]{assets/contribution/dataset/results_graphs/5.pdf}
\\[\smallskipamount]
\includegraphics[width=.2\textwidth]{assets/contribution/dataset/results_graphs/6.pdf}\hfill
\includegraphics[width=.2\textwidth]{assets/contribution/dataset/results_graphs/7.pdf}\hfill
\includegraphics[width=.2\textwidth]{assets/contribution/dataset/results_graphs/8.pdf}\hfill
\includegraphics[width=.2\textwidth]{assets/contribution/dataset/results_graphs/9.pdf}\hfill
\includegraphics[width=.2\textwidth]{assets/contribution/dataset/results_graphs/10.pdf}
\\[\smallskipamount]
\includegraphics[width=.2\textwidth]{assets/contribution/dataset/results_graphs/11.pdf}\hfill
\includegraphics[width=.2\textwidth]{assets/contribution/dataset/results_graphs/12.pdf}\hfill
\includegraphics[width=.2\textwidth]{assets/contribution/dataset/results_graphs/13.pdf}\hfill
\includegraphics[width=.2\textwidth]{assets/contribution/dataset/results_graphs/14.pdf}\hfill
\includegraphics[width=.2\textwidth]{assets/contribution/dataset/results_graphs/15.pdf}
\\[\smallskipamount]
\includegraphics[width=.2\textwidth]{assets/contribution/dataset/results_graphs/16.pdf}\hfill
\includegraphics[width=.2\textwidth]{assets/contribution/dataset/results_graphs/17.pdf}\hfill
\includegraphics[width=.2\textwidth]{assets/contribution/dataset/results_graphs/18.pdf}\hfill
\includegraphics[width=.2\textwidth]{assets/contribution/dataset/results_graphs/19.pdf}\hfill
\includegraphics[width=.2\textwidth]{assets/contribution/dataset/results_graphs/20.pdf}
\\[\smallskipamount]
\includegraphics[width=.2\textwidth]{assets/contribution/dataset/results_graphs/21.pdf}\hfill
\includegraphics[width=.2\textwidth]{assets/contribution/dataset/results_graphs/22.pdf}\hfill
\includegraphics[width=.2\textwidth]{assets/contribution/dataset/results_graphs/23.pdf}\hfill
\includegraphics[width=.2\textwidth]{assets/contribution/dataset/results_graphs/24.pdf}\hfill
\includegraphics[width=.2\textwidth]{assets/contribution/dataset/results_graphs/25.pdf}
\\[\smallskipamount]
\includegraphics[width=.2\textwidth]{assets/contribution/dataset/results_graphs/26.pdf}\hfill
\includegraphics[width=.2\textwidth]{assets/contribution/dataset/results_graphs/27.pdf}\hfill
\includegraphics[width=.2\textwidth]{assets/contribution/dataset/results_graphs/28.pdf}\hfill
\includegraphics[width=.2\textwidth]{assets/contribution/dataset/results_graphs/29.pdf}\hfill
\includegraphics[width=.2\textwidth]{assets/contribution/dataset/results_graphs/30.pdf}
\\[\smallskipamount]
\includegraphics[width=.2\textwidth]{assets/contribution/dataset/results_graphs/31.pdf}\hfill
\includegraphics[width=.2\textwidth]{assets/contribution/dataset/results_graphs/32.pdf}\hfill
\includegraphics[width=.2\textwidth]{assets/contribution/dataset/results_graphs/33.pdf}\hfill
\includegraphics[width=.2\textwidth]{assets/contribution/dataset/results_graphs/34.pdf}\hfill
\includegraphics[width=.2\textwidth]{assets/contribution/dataset/results_graphs/35.pdf}
\\[\smallskipamount]
\includegraphics[width=.2\textwidth]{assets/contribution/dataset/results_graphs/36.pdf}\hfill
\includegraphics[width=.2\textwidth]{assets/contribution/dataset/results_graphs/37.pdf}\hfill
\includegraphics[width=.2\textwidth]{assets/contribution/dataset/results_graphs/38.pdf}\hfill
\includegraphics[width=.2\textwidth]{assets/contribution/dataset/results_graphs/39.pdf}\hfill
\includegraphics[width=.2\textwidth]{assets/contribution/dataset/results_graphs/40.pdf}
\caption{Corresponding Graph Dataset}
\label{fig:corresponding-graph-dataset}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=\textwidth]{assets/contribution/dataset/data/surface.pdf}
\caption{Distribution of Surface Value}
\label{fig:distribution-of-surface-value}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=\textwidth]{assets/contribution/dataset/data/total_site_energy_consumption_per_surface_MJm2.pdf}
\caption{Distribution of Energy Consumption Value}
\label{fig:distribution-of-energy-consumption-value}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=\textwidth]{assets/contribution/dataset/data/total_window_opening_area_m2.pdf}
\caption{Distribution of Opening Area Value}
\label{fig:distribution-of-opening-area-value}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=\textwidth]{assets/contribution/dataset/data/plot0.pdf}
\caption{Relation Between Opening Area, Surface and Energy Consumption}
\label{fig:relation-between-opening-area-surface-and-energy-consumption}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=\textwidth]{assets/contribution/dataset/data/plot1.pdf}
\caption{Relation Between Energy Consumption, Layout ID and Surface}
\label{fig:relation-between-energy-consumption-layout-id-and-surface}
\end{figure}

\subsection{Conclusion}\label{subsec:conclusion}

In this thesis, the primary reason for the creation of the graph dataset is for its use in machine learning. However, the generation pipeline provides a basis for automatic floor plan generation that can also be used in the context of optimised floor plan generation processes. In this case, however, a modification of the described processes is necessary, as the ambition is then to generate an optimal design solution and not a search for the greatest possible variance in the design.

The whole generation workflow implemented in \Gls{python} shows a generally satisfactory harmony of the different stages. However, some could benefit from more customised logic and functionality. For example, for maintenance reasons, it would be advisable to replace the Voronoi generation with \textit{SciPy's} spatial Voronoi instead of the \Gls{python} library \textit{PyVoro}. It would also be advantageous to speed up certain aggregation methods by parallelising processes, thus allowing their integration into the pipeline. In fact, the variance of the dataset would benefit from such integration, as in the current implementation there is no way to generate floor plans with a central patio or external balconies.

\chapter{Graph Machine Learning}\label{chap:graph-machine-learning}

The second part of the methodology deals with the construction, training and evaluation of different graph machine learning models. The focus is on two special variants of graph deep learning. First, a model is developed that is trained by graph classification methods and thus represents a classification model. More specifically, this model is trained on the labelled graph objects and their energy class to be able to classify new, unseen and annotated graph structures into one of the five energy classes. The second model is also based on graph-wide prediction, but uses continuous energy consumption values instead of classes, and is therefore a regression model. In other words, for any architecture-based graph structure, this regression model allows the \gls{site energy consumption} per surface in $MJ/m^2$ of the underlying architectural objects to be predicted as accurately as possible.

The two models created belong to the family of \acrlong{dgcnn}\textit{s} (\acrshort{dgcnn}\textit{s})\footcite{velivckovic2023everything} and, as the name suggests, are based on a \textit{convolutional neural network structure}. Due to their special architecture, they allow the use of graphical datasets as a basis for the supervised learning process instead of conventional pictorial or tabular training datasets. Since \acrshort{dgcnn}\textit{s} are composed of several layers, each of which performs convolutional operations followed by \textit{\gls{activation} functions} and \textit{\gls{pooling} operations}, information can be reduced and abstracted so that classes or values corresponding to the entire graph can be predicted\footcite{wang2019dgl}. Another key feature of \acrshort{dgcnn}\textit{s} is their ability to handle graphs of variable size and structure. This means that these networks are able to process graphs with different numbers of nodes and edges without changing the network structure or training method, which has proven to be an essential feature in their application.

The entire training, testing and \textit{\gls{validation}} process, as well as the evaluation of the results, was implemented in \Gls{python}, as was the geometric generation pipeline, and is essentially based on several different \Gls{python} libraries such as \textit{TopologicPy}, \acrshort{dgl}, \textit{PyTorch}\footcite{NEURIPS2019_9015}, \textit{Scikit-learn}\footcite{scikit-learn}, \textit{NumPy} and \textit{Pandas}\footcite{mckinney-proc-scipy-2010}. The \acrshort{dgl} library is by no means the only \Gls{python} library that allows the creation and training of \acrshort{dgcnn}\textit{s}; However, it was preferred over other libraries, such as \textit{PyTorch Geometric}, because \acrshort{dgl} is an excellently documented library that can be easily combined with the \textit{TopologicPy} toolkit. In addition, \acrshort{dgl} offers interesting functions such as the mean node calculation or a wide range of \textit{\gls{sampling} methods}.

\section{Structure}\label{sec:structure}

In order to successfully train the respective \acrshort{dgcnn} models, several steps must be taken, beginning with the data preparation. With the help of the \acrshort{dgl} library, a \textit{DGL.Dataset} object is created, which allows the individual stored and annotated graph objects to be read in, converted into adjacency matrices and the respective training information to be stored at graph level. The thus created graph dataset object is perfectly suitable for continuing the training preparation.

The next step is to divide the dataset for the \textit{\gls{holdout}} training method into a training, \gls{validation} and test dataset, with the test and \gls{validation} datasets each representing ten percent of the total. The resulting \gls{validation} dataset is stored and used only after successful training. Because of the large amount of data involved, it is also necessary to create a so-called \textit{data loader}, which allows the graphs to be loaded in a bundled form per learning step. This is based on the creation of so-called \textit{graph batches}. A graph batch consists of several graph objects that are combined into a larger bundled graph without changing their structure or information. In this sense, a graph batch is a supergraph object to the individual subgraphs from the dataset. The data loader function also needs information about how to read the graphs from the dataset. In the present project, a \textit{\gls{subset random sampler}} was chosen to allow a random composition of the graph batches.

\begin{figure}
\centering
\includegraphics[width=\textwidth]{assets/contribution/machine_learning/classifier_schema.pdf}
\caption{Classification Model Structure}
\label{fig:classification-model-layer-size}
\end{figure}

At this point it is advisable to verify the balance of the created dataset, which in concrete application means comparing the number of energy classes occurring in the dataset. If these percentages are not sufficiently balanced, methods such as \textit{over-} or \textit{under\gls{sampling}} should be used to rebalance the dataset.

The data preparation and reading is now complete, and all that remains to be done is to define the model and its individual structure, the hyperparameters that influence training, and the choice of optimisation methods. Then the training cycle can begin, where \textit{\gls{epoch}} by \gls{epoch} the weights of each layer of the model are optimised by \textit{backpropagation} to improve the prediction accuracy on the training data. Finally, the trained model is tested on the test dataset to evaluate its prediction accuracy.

This framework is the same for both classification and regression models. The two methods differ only in their individual structure, input data and the way \textit{\gls{loss}} is calculated during the training process.

\begin{figure}
\centering
\includegraphics[width=\textwidth]{assets/contribution/machine_learning/regressor_schema.pdf}
\caption{Regression Model Structure}
\label{fig:regression-model-layer-size}
\end{figure}

The graph classification model (figure \ref{fig:classification-model-layer-size}) is based on the energy classes contained in the dataset at the graph level, ranging from zero to four. Its layers consist of the \textit{input layer} with a dimension corresponding to the number of node labels, a variable number of graph convolutional layers with different dimensions and \gls{activation} functions, a \gls{pooling} layer used to reduce the size of the graph and at the same time the information loss, and finally the \textit{output layer} corresponding to the dimension of the number of classes to be predicted and providing the final prediction about the class membership of the graph. The \gls{loss} calculation for the classification model can be computed by either \textit{\gls{cross-entropy}} or \textit{\gls{negative log-likelihood}}.

The structure of the regression model (figure \ref{fig:regression-model-layer-size}) is similar to that of the classifier; however, the final layer is a \textit{linear layer}, which is one-dimensional because the prediction of the regression model is, by definition, almost always a single value. Another difference is the \gls{loss} calculation, which is performed in the regression model by calculating the \textit{\acrlong{mse}}.

\section{Hyperparameter}\label{sec:hyperparameter}

\textit{Hyperparameters} are parameters that control the behaviour of the model, but are usually not chosen by the model itself. Instead, they have to be set manually by the developer. In this work, the main hyperparameters are the \textit{\gls{learning rate}}, the number and dimension of \textit{hidden layers}, the number of \glspl{epoch}, the \textit{batch size}, the \gls{pooling} strategy, the \gls{loss} function, the \textit{optimisation method} and the \gls{convolution} layer type. Since there are many different and interdependent parameters, certain methods such as \textit{random search} or \Gls{bayesian optimisation} can be used to find the optimal hyperparameters. This process is called \textit{hyperparameter tuning} in computer science.

The basic principle is to perform the training process with the parameters determined by the automatic tuning algorithms and then test the performance of the model on the \gls{validation} dataset. This process is repeated until the most optimal combination of each hyperparameter is found. The results of the random search tuning as well as the \Gls{bayesian optimisation} method (figure \ref{fig:regressor-hyperparameter-tuning} and \ref{fig:classifier-hyperparameter-tuning}) show that for the classification model and the regression model, the hyperparameters shown in the tables \ref{tab:regressor-hyperparameter} and \ref{tab:classifier-hyperparameter} lead to an optimisation of the prediction and classification accuracy.

\begin{table}
\begin{center}
\resizebox{\textwidth}{!}{%
\begin{tabular}{ c c c c c c c c }
Epochs & Batch Size & Learning Rate & Pooling & Hidden Layer & Layer Type & Optimiser & Loss Function\\ 
300 & 32 & 0.01 & Average & 32 & TagConv & Adam & MSE
\end{tabular}}
\end{center}
\caption{Regressor Hyperparameter}
\label{tab:regressor-hyperparameter}
\end{table}

\begin{figure}
\centering
\includegraphics[width=\textwidth]{assets/contribution/machine_learning/wandb_regressor.pdf}
\caption{Regressor Hyperparameter Tuning}
\label{fig:regressor-hyperparameter-tuning}
\end{figure}

\begin{table}
\begin{center}
\resizebox{\textwidth}{!}{%
\begin{tabular}{ c c c c c c c c }
Epochs & Batch Size & Learning Rate & Pooling & Hidden Layer & Layer Type & Optimiser & Loss Function\\ 
50 & 64 & 0.01 & Average & 16 | 16 & GraphConv & Adam & NLL
\end{tabular}}
\end{center}
\caption{Classifier Hyperparameter}
\label{tab:classifier-hyperparameter}
\end{table}

\begin{figure}
\centering
\includegraphics[width=\textwidth]{assets/contribution/machine_learning/wandb_classifier.pdf}
\caption{Classifier Hyperparameter Tuning}
\label{fig:classifier-hyperparameter-tuning}
\end{figure}

\section{Evaluation}\label{sec:evaluation}

There are several evaluation metrics that can be used to evaluate the trained \acrshort{dgcnn} models. The most common are \textit{accuracy} (table \ref{tab:classifier-performance}), \gls{precision}, \textit{\gls{recall}}, \textit{\gls{f1-score}} and \acrlong{mse} or \textit{\acrlong{rmse}} (table \ref{tab:regressor-performance}), although only the last two can be used as a strategy for evaluating regressors. These metrics measure different aspects of model performance, such as the overall \gls{precision} of the model, or even the ability to distinguish positive examples from negative ones in the case of classifications.

To ensure that the evaluation of the \acrshort{dgcnn} model is robust and reliable, a \textit{\gls{10-fold cross-validation}} is preferable to simple \gls{holdout} training. In this method, the dataset is divided into 10 subsets and the model is trained and tested accordingly 10 times, using each subset once as a test dataset. The results are then averaged to obtain a robust evaluation score of the two models.

\subsection{Classification Metrics}\label{subsec:classification-metrics}

Graph classification models are typically evaluated based on their accuracy in predicting graph classes. To do this, graphs are divided into training and test datasets. The training dataset is used to train the model, while the test dataset is used to evaluate the performance of the model. The accuracy of the model is then measured by the number of correct predictions on the test dataset. A commonly used metric for evaluating graph classification models is the accuracy metric, which is calculated as the ratio of correct predictions to the total number of predictions.

In addition to accuracy, other metrics can be used to evaluate the performance of graph classification models, such as \gls{precision}. This is the number of times that the model correctly predicts that a datapoint belongs to a particular class, as measured by all the predictions that the model has assigned to that class. It is calculated as the ratio of \textit{\acrlong{tp}} predictions (\acrshort{tp}) to the sum of \acrshort{tp} and \textit{\acrlong{fp}} predictions (\acrshort{fp}).

In addition, the \gls{recall} score can be used for evaluation, which indicates how often the model correctly predicts that a datapoint belongs to a particular class, measured against all actual datapoints of that class. \Gls{recall} is calculated as the ratio of true \acrshort{tp} predictions to the sum of \acrshort{tp} and \textit{\acrlong{fn}} predictions (\acrshort{fn}).

Finally, it is also common in research to calculate the \gls{f1-score} of a classification model, which is a combination of \gls{precision} and \gls{recall}. It indicates how well the model is both precise and comprehensive. This score represents the \textit{harmonic mean} of \gls{precision} and \gls{recall}.

It is also important to note that the performance of graph classification models is highly dependent on the quality of the data used. It is therefore necessary to carefully verify that the training and test datasets are sufficient and representative of the application scenario (section \ref{subsec:results}), as well as having the best possible balanced class distribution in the dataset to avoid possible \textit{\gls{overfitting}}.

\subsection{Regression Metrics}\label{subsec:regression-metrics}

Classification accuracy metrics such as \gls{precision}, \textit{accuracy}, \gls{recall} and \gls{f1-score} are not appropriate for regression models because they are designed to evaluate the performance of discrete class prediction models. In a regression problem, however, the goal is to predict a numerical target variable given a set of input variables. Because the target variable is continuous rather than discrete, accuracy metrics such as conventional classification accuracy cannot be used because they are unable to measure the accuracy of predicting continuous variables.

Instead, regression models are evaluated using metrics such as \acrfull{mse}, \acrfull{rmse} and \textit{\acrfull{mae}} to quantify the predictive performance with respect to the actual values of the target variable. The \acrshort{mse} indicates the degree of \acrlong{mse} between the model's predictions and the actual values of the target variable. It is calculated as the mean of the squared difference between the predictions and the actual values. The \acrshort{rmse} is the root of the \acrshort{mse} and thus indicates how large the average absolute error is between the model's predictions and the actual values of the target variable. The \acrshort{mae} on the other hand indicates the size of the average absolute error between the predictions of the model and the actual values of the target variable.

\subsection{Comparison}\label{subsec:comparison}

Finally, in order to adequately evaluate the performance of the two trained models, they are compared with other models that have similar functionalities and their respective prediction accuracies. Thus, the \acrshort{dgcnn} Classifier is compared with a number of different classification models such as the \textit{Random Forest Classifier}, the \textit{Gradient Boost Classifier}, the \textit{\Gls{decision tree} Classifier}, \textit{K-Nearest Neighbours Classifier}, \textit{Support Vector Classifier}, \textit{Logistic Regression} and \textit{\acrlong{mlp} Classifier} and their respective accuracy (figure \ref{fig:10-fold-cross-validation-accuracy-for-different-classifiers}).

The \acrshort{dgcnn} regression model was compared with models such as \textit{Random Forest Regressor}, \textit{Gradient Boosting Regressor}, \Gls{decision tree} Regressor, \textit{K-Nearest Neighbours Regressor}, \textit{Support Vector Regressor}, \textit{Linear Regression}, \textit{Ridge Regression}, \textit{Lasso Regression}, \textit{\acrlong{mlp} Regressor} and \textit{Elastic Net} (figure \ref{fig:10-fold-cross-validation-rmse-for-different-regressors}). However, it is important to understand that these are different machine learning approaches and therefore the underlying data is required in different formats. This means, for example, that the aforementioned comparative models were trained entirely on tabular data (section \ref{subsec:graphical-to-tabular-data-conversion}), as opposed to the graphical data used for the \acrshort{dgcnn} models. This has to be considered as an essential part of the evaluation.

\section{Outcome}\label{sec:outcome-machine-learning-for-energy-prediction}

\subsection{Machine Learning for Energy Prediction}\label{sec:machine-learning-for-energy-prediction}

Both classification and regression models have been trained on the synthetic graph dataset to predict the energy performance score of an apartment. The classification models predict the energy efficiency class of an apartment, while the regression models predict the actual energy consumption in $MJ/m^2$.

At this point, it is crucial to re-emphasise the advantages of the trained models. Contrary to the common assumption that machine learning and artificial intelligence in the field of \acrshort{aec} are mostly applied to the automatic generation of optimised architectural plans, in this experimentation the focus is purely on enabling optimised design feedback loops. In other words, the primary application of the models takes place during the conceptual phase of the design process, hand in hand with the architect or designer. The creative freedom of the architect's work is therefore in no way affected or constrained by the application of the machine learning models, but rather provides a continuous indication of the actual energy consumption of the current project. This allows the designer to easily test certain decisions, such as the placement, size or orientation of a window, in terms of changes in energy consumption, without having to continuously run costly energy simulations. More on the concrete architectural application of the results of this research can be found in the sections \ref{sec:added-value-for-architects} and \ref{sec:potential-application-and-usage}.

\subsection{Classification}\label{subsec:classification}

This section provides a comprehensive performance comparison between several classifiers. The models have been trained on information derived from the synthetic graph dataset, allowing their respective capabilities and limitations to be assessed and evaluated in the context of the task at hand. The use of the \gls{10-fold cross-validation} technique ensures the generalisation of the models and the reliability of their performance metrics. In order to compare the performance of the \acrshort{dgcnn} classifiers, several different models have been used, such as Logistic Regression, \acrshort{mlp}, Support Vector, Random Forest, Gradient Boosting, \Gls{decision tree} and K-Neighbors. They were evaluated based on their mean accuracy, which provides an overview of how well the classifiers performed on average. The results are presented graphically in figure \ref{fig:10-fold-cross-validation-accuracy-for-different-classifiers} and numerically in table \ref{tab:classifier-performance}. In addition, a detailed analysis of the performance of the \acrshort{dgcnn} Classifier is presented, including a mean confusion matrix which helps to understand the classification accuracy for each class individually.

\subsubsection{Results}\label{subsubsec:results-classification}

It can be seen that the \acrshort{dgcnn} Classifier has the highest mean accuracy (0.83), followed by the logistic regression (0.82) and the \acrshort{mlp} Classifier (0.79). On the other hand, the K-Neighbors Classifier has the lowest mean accuracy (0.52). The K-Neighbors Classifier is a model that relies on local patterns in the \textit{feature space} to make predictions. This approach works well when the input data is densely packed and can be easily separated into different classes based on these patterns. However, if the data has a more complex structure, such as graph-derived data, this approach may not be effective. The \acrshort{dgcnn} Classifier is designed to work with such graphical data and can learn the underlying features of the graph structure to make accurate classifications.

\begin{figure}
\centering
\includegraphics[width=\textwidth]{assets/contribution/machine_learning/comparison_10foldCV_accuracy_classifiers.pdf}
\caption{10-Fold Cross Validation Accuracy for Different Classifiers}
\label{fig:10-fold-cross-validation-accuracy-for-different-classifiers}
\end{figure}

\begin{table}
\centering
\begin{tabular}{ ccc }
\textbf{Classifier} & \textbf{Mean Accuracy} & \textbf{Standard Deviation} \\
\rowcolor[rgb]{0.93, 0.93, 0.93} DGCNN Classifier & 0.83 & 0.01 \\
Logistic Regression Classifier & 0.82 & 0.01 \\
MLP Classifier & 0.79 & 0.01 \\
Support Vector Classifier & 0.79 & 0.01 \\
Random Forest Classifier & 0.65 & 0.01 \\
Gradient Boosting Classifier & 0.61 & 0.02 \\
Decision Tree Classifier & 0.57 & 0.01 \\
K-Neighbors Classifier & 0.52 & 0.00 \\
\end{tabular}
\caption{Mean Accuracy and Standard Deviation of Different Classifiers}
\label{tab:classifier-performance}
\end{table}

The Random Forest Classifier has a mean accuracy of 0.65, which is relatively good compared to some of the other classification models. The Random Forest Classifier is an \gls{ensemble learning method} that relies on a collection of \glspl{decision tree} to make predictions. It is known for its robustness to \gls{overfitting} and its ability to handle high-dimensional data. However, the performance of the model can be sensitive to the choice of hyperparameters, such as the number of trees and the depth of such trees. The Gradient Boosting Classifier has a mean accuracy of 0.61, which is slightly lower than the Random Forest Classifier. This model is also an \gls{ensemble learning method} that uses a collection of \textit{weak learners} to make predictions. It is known for its ability to handle high-dimensional data and its ability to predict the shape of complex non-linear functions that describe the relationship between the features and the target variable. However, like the Random Forest classifier, the performance of the model can be sensitive to the choice of hyperparameters.

Not surprisingly, the \Gls{decision tree} Classifier has a mean accuracy of 0.57, which is lower than the Random Forest Classifier and the Gradient Boosting Classifier. This model is a simple tree-based learning method that makes predictions based on a set of if-then rules. It is known for its simplicity and interpretability, but it can be prone to \gls{overfitting} and may not perform well on such complex datasets.

The Support Vector Classifier has a mean accuracy of 0.79, which is relatively high in comparison. This model is a \textit{discriminative learning} method that divides the data into different classes by slicing planes in n-dimensional space according to the data features. Its main advantage is its ability to learn on tensors of higher dimensions and its robustness to noisy data. The \acrshort{mlp} Classifier has a mean accuracy of 0.79, which is similar to the Support Vector Classifier. This model is a neural network based learning method that uses multiple layers of interconnected nodes to learn the features of the input data. It is also capable of learning non-linear functions that map the features to the target variable, but the selection of hyperparameters must be well chosen.

Logistic regression has a mean accuracy of 0.82, which is surprisingly the second highest of all the classifiers. This model is a probabilistic learning method that models the probability of the target variable given the input data. It is known for its simplicity and interpretability and can perform well on a wide range of datasets. It is possible that the linear nature of the logistic regression model is beneficial for this particular task, allowing the classifier to be less prone to \gls{overfitting}. 

\begin{figure}
\centering
\includegraphics[width=0.9\textwidth]{assets/contribution/machine_learning/mean_confusion_matrix.pdf}
\caption{Mean Confusion Matrix for DGCNN Classifier}
\label{fig:mean-confusion-matrix-for-Classifier}
\end{figure}

Figure \ref{fig:mean-confusion-matrix-for-Classifier} shows the \textit{mean confusion matrix} for the \acrshort{dgcnn} Classifier. The confusion matrix represents the distribution of predicted and true labels for each class. The x-axis and y-axis of the matrix represent the predicted and true labels respectively. The colour intensity of each cell in the matrix represents the mean of the confusion matrix for the corresponding true and predicted labels. The values within each cell indicate the mean of the \gls{10-fold cross-validation} confusion matrices with the corresponding \textit{\gls{standard deviation}} value. Based on the mean and \gls{standard deviation} values, it appears that some classes are easier to predict than others using the \acrshort{dgcnn} Classifier. For example, the diagonal values for classes 0 and 4 are relatively high, indicating that these classes are easier to predict accurately. In contrast, the off-diagonal values for classes 1, 2 and 3 are likewise relatively high, indicating that these classes are harder to predict accurately. The \gls{standard deviation} values also show some variability in the confusion matrix, with the highest levels of variance observed in the off-diagonal values for classes 1 and 2.

\subsubsection{Conclusion}\label{subsubsec:conclusion-classification}

The comprehensive performance analysis presented in this section illustrates the comparative performance of several models predicting different energy consumption classes for unseen apartment layouts through \gls{10-fold cross-validation}. The results clearly indicate that the \acrshort{dgcnn} classifier outperforms the other models, with the highest mean accuracy of 0.83. This superior performance can be attributed to its ability to handle the complex structure of graph-derived data, effectively learning the underlying features of the node relationships for accurate classifications.

The mean confusion matrix for the \acrshort{dgcnn} Classifier further reveals that some classes are easier to predict than others. This highlights the importance of considering class distribution and potential class imbalances when evaluating model performance.

Overall, these results emphasise the crucial role of choosing the right classifier architecture depending on the nature of the dataset and the problem at hand. For the task described, which involves complex graphical data structures, more sophisticated models such as the \acrshort{dgcnn} Classifier are expected to offer superior performance. However, simplicity and interpretability should not be overlooked, as demonstrated by the relatively high performance of the logistic regression model. Future work could focus on optimising the hyperparameters of these models, further improving their performance on the dataset, and exploring other sophisticated models designed to handle complex graphical data structures.

\subsection{Regression}\label{subsec:regression}

The aim of this section is to compare the performance of a set of regression models trained on tabular data derived from the information contained in the graph dataset. The dataset consists of energy consumption values in $MJ/m^2$, which serve as labels for the regression task. Using the \acrfull{rmse} as an evaluation metric, it was possible to measure the respective performance of the models. The results are presented graphically in figure \ref{fig:10-fold-cross-validation-rmse-for-different-regressors} and numerically in table \ref{tab:regressor-performance}. These results show the best performing regressor, with details of the respective \gls{standard deviation} across multiple model outputs.

\subsubsection{Results}\label{subsubsec:results-regression}

A scatter plot of the actual and predicted values of the \acrshort{dgcnn} regression model is shown in figure \ref{fig:actual-and-predicted-values-of-the-regression-model}, where the x-axis describes the actual energy consumption values of the apartments in the test dataset in $MJ/m^2$ and the y-axis shows the corresponding predictions made by the trained \acrshort{dgcnn} model. In this plot, the scatter points should be as close as possible to the diagonal, as their orthogonal distance to this line represents the mean error of each prediction. This plot shows that higher energy consumption values are associated with a slightly increased \acrshort{rmse}, which translates into an elevated inaccuracy for these values. On the other hand it can be seen that from about 900 $MJ/m^2$ the number of datapoints representing these values becomes sparser, which was already visible in figure \ref{fig:distribution-of-energy-consumption-value}.

\begin{figure}
\centering
\includegraphics[width=0.9\textwidth]{assets/contribution/machine_learning/actual_predicted.pdf}
\caption{Actual and Predicted Values of the Regression Model}
\label{fig:actual-and-predicted-values-of-the-regression-model}
\end{figure}

When analysing the results, the \acrshort{mlp} Regressor stands out as the best performing model with a \acrshort{rmse} of 18.82. This demonstrates that the \acrshort{mlp} Regressor was the most accurate in predicting energy consumption values. The \acrshort{mlp} Regressor, a type of neural network, exploits its ability to learn complex, non-linear relationships, which is likely to have contributed to its high performance in this task.

The \acrshort{dgcnn} Regressor closely followed the \acrshort{mlp} Regressor, with a slightly higher \acrshort{rmse} of 19.33 $MJ/m^2$. Given that \acrshort{dgcnn} is inherently capable of handling complex structured data, such as graph data, it is not surprising that it performed well in this task. The high performance of \acrshort{dgcnn} and \acrshort{mlp} suggests that the underlying patterns present in the dataset may be complex, requiring sophisticated models to accurately predict the energy consumption values.

\begin{figure}
\centering
\includegraphics[width=\textwidth]{assets/contribution/machine_learning/comparison_10foldCV_rmse_regressors.pdf}
\caption{10-Fold Cross Validation RMSE for Different Regressors}
\label{fig:10-fold-cross-validation-rmse-for-different-regressors}
\end{figure}

\begin{table}
\centering
\begin{tabular}{ ccc }
\textbf{Regressor} & \textbf{Mean RMSE} & \textbf{Standard Deviation} \\
MLP Regressor & 18.82 & 0.53 \\
\rowcolor[rgb]{0.93, 0.93, 0.93} DGCNN Regressor & 19.33 & 0.63 \\
Linear Regression & 34.52 & 0.55 \\
Ridge Regressor & 34.52 & 0.55 \\
Gradient Boosting Regressor & 41.85 & 0.76 \\
Lasso Regressor & 43.29 & 0.76 \\
Random Forest Regressor & 46.73 & 1.17 \\
Support Vector Regressor & 47.41 & 1.13 \\
Decision Tree Regressor & 49.53 & 1.40 \\
K-Neighbors Regressor & 52.14 & 1.34 \\
Elastic Net Regressor & 70.18 & 0.96 \\
\end{tabular}
\caption{Mean RMSE and Standard Deviation of Different Regressors}
\label{tab:regressor-performance}
\end{table}

Linear Regression and the Ridge Regressor follow with a significant lead of about 15 units, yielding an \acrshort{rmse} of 34.52. As these two regressors are linear models, the difference in performance compared to \acrshort{mlp} and \acrshort{dgcnn} suggests that the relationship between the features and the energy consumption value is likely to be non-linear.

Interestingly, the performance of more complex models such as the Random Forest Regressor and Gradient Boosting Regressor is suboptimal, with \acrshort{rmse}\textit{s} of 46.73 and 41.85 respectively. These regressors typically perform well on many datasets due to their ability to handle high-dimensional data and capture non-linear relationships. However, in this case they underperform compared to simpler models such as Linear Regression and Ridge, suggesting that the features in the dataset may not be well suited to tree-based models.

Support Vector Regressor, Decision Tree Regressor and K-Neighbors Regressor have \acrshort{rmse}\textit{s} higher than 47, indicating their poor performance in this task. The low score of these models may be due to their difficulty in dealing with the high complexity of the dataset and the distribution of the target variable.

Finally, the Elastic Net Regressor has the highest \acrshort{rmse} of 70.18, reflecting the worst performance of all the models. This could be due to the model's \textit{l1 regularisation}, which, while useful in preventing \gls{overfitting}, may have led to \textit{\gls{underfitting}} in this case.

\subsubsection{Conclusion}\label{subsubsec:conclusion-regression}

Despite the good accuracy of the graph model regressor, the fact that the \acrshort{mlp} Regressor slightly outperforms the \acrshort{dgcnn} Regressor provides an important finding. It implies that, contrary to what was initially assumed, graph representation does not necessarily provide a substantial advantage over simple tabular representation in the case of this particular energy prediction task.

There are several factors that could contribute to this. It may be that the complexity of the graph-structured data does not provide additional benefits in predicting energy consumption values, especially compared to a well-designed tabular representation. The energy consumption of apartments may be primarily influenced by characteristics that are easily captured in a tabular format, such as apartment size, number of rooms or location, while the added topological information of the graph structure may not contribute significant predictive value.

In conclusion, the choice of data representation and machine learning model was found to have a significant impact on performance, highlighting the need for task-specific and data-driven model selection. While the \acrshort{dgcnn} Classifier excelled in the classification task, reinforcing the value of graph data for this particular problem, the \acrshort{mlp} Regressor outperformed the \acrshort{dgcnn} Regressor in the regression task, suggesting that tabular data may be equally or even more effective in data-driven energy performance prediction. This requires a detailed understanding of the benefits and limitations of different data representations and models according to the specific characteristics of the dataset and the nature of the prediction task. Future research can build on these findings by exploring other sophisticated models and data transformations to take advantage of the relational information contained in graphical architecture datasets.

\chapter{Conclusion and Future Perspectives}\label{chap:conclusion-and-future-perspectives}

The results of this study demonstrate the potential of applying topological \glspl{knowledge graph} and graph machine learning to the architectural design process. The synthetic dataset generation process, which included the collection of space partitioning algorithms, allowed the creation of an unbiased architectural dataset with significant variability. Both the classification and regression models achieved high accuracy in terms of energy prediction, indicating the potential for a useful implementation of the described methods in the relevant industry. However, when compared to other machine learning models, the application of a simple neural network (\acrshort{mlp}) trained on tabular rather than graphical data showed comparably good results. This suggests that the application of topological \glspl{knowledge graph} in the context of energy simulation is not particularly advantageous compared to simple tabular data. Nevertheless, further research is needed before the usefulness of graphical topological information in the field of energy consumption prediction can be determined.

Furthermore, the importance of graph theory in architectural design is demonstrated, questioning the current role and application of building simulations in architecture. Although the proposed application in this study is currently a proof of concept rather than a complete tool proposal, it demonstrates the possibility of improving the architectural workflow and integrating architectural graph representation into design, as well as optimising simulation and design interaction. The implications of this study are broad, ranging from the advancement of graph-theoretic applications in architectural research to the enhancement of creative, artistic and organisational aspects of the architectural profession.

However, there are limitations to the proposed methods, including the lack of complex architectural models with concave spaces or patios in the dataset and the high degree of generalisation in the energy simulations. The choice of partitioning algorithm was limiting but necessary due to resource limitations that made the application of complex iterative optimisation methods more restrictive. Lack of access to an unbiased large graph dataset, limited machine power, time constraints and the difficulty of establishing connections and communication with the \acrshort{aec} were also limiting factors.

Despite these limitations, this study opens new avenues for research in the field of architecture and demonstrates the potential of topological information coupled with \glspl{knowledge graph} in the context of automated architectural simulation and analysis methods. With further development and optimisation, this approach has the potential to find a valuable application in architectural design, allowing for more efficient building practices.

By presenting and discussing the various simulation and topological analysis methods, this thesis provides a basis for further research into graphical machine learning methods in the architectural context. Furthermore, the detailed documentation of the methodology, the software used, and the public availability of all data and code (section \ref{sec:summary-of-contribution}) allows for subsequent research based on this contribution.

\section{Summary of Contribution}\label{sec:summary-of-contribution}

One of the main contributions of this thesis is the development of a comprehensive framework for the synthetic generation of a complete graph dataset that accurately represents real-world building designs. This involved the gathering of topological analysis methods which were then used to extract \gls{knowledge graph} objects from the corresponding apartment designs. By exploring and comparing different algorithms, this research identified the most appropriate methods for creating diverse apartment designs with their topological organisation represented by graph networks. This synthetic dataset was then enriched with energy performance scores and values, allowing it to be used to train different machine learning algorithms to predict and classify the energy consumption of buildings based on their graph representations.

A detailed introduction and presentation of the concepts of graph theory, topology and simulation provides insights into the respective fields and their connection to architecture. Essential methods of each field are explained and described, such as the different graphical analysis methods, topological analysis concepts in concrete architectural applications, graph machine learning algorithms, and the most relevant simulation techniques. This summary thus provides a knowledge base for further research in the areas described.

The synthetic generation of the graph dataset involved several crucial steps, including the creation of a parametric generation pipeline and the integration of the identified space partitioning algorithms into the parametric framework for automatic floor plan generation. The defined rules and architectural requirements for the properties of each object in the dataset were also carefully considered and a post-processing phase was conducted to ensure the accuracy and reliability of the dataset. This synthetic dataset can thus serve as a useful tool for training graph machine learning algorithms to predict various parameters based on chosen architectural simulation metrics.

Another significant contribution of this thesis is the use of graph machine learning to predict the energy performance of buildings based on their graph representations. The trained regressor and classifier were able to accurately predict the energy performance of unseen apartment designs based on their topology, demonstrating the potential of integrating such methods into the architectural design process. This research has also shown the importance of feedback in the early design stages to optimise building performance, and that graphical information could be a beneficial alternative to traditional unstructured data for analytical applications.

This research has thus contributed to the field of architectural and conceptual design by demonstrating the integration of theoretical concepts such as graph theory, topology, synthetic data generation, automatic floor planning and graph machine learning in the design process. The generated dataset and the corresponding code will be made available under an \gls{open source} licence, thus being open for contribution while allowing any kind of use and modification. The dataset is hosted on Google's data science platform \textit{kaggle} (\url{https://kaggle.com/datasets/rabanohlhoff/architecture-graph-dataset}) and the code for the parametric framework as well as the machine learning procedure with the trained models is hosted on Microsoft's code sharing platform \textit{GitHub} (\url{https://github.com/Sinasta/thesis}).

\section{Reasearch Questions}\label{sec:research-questions-conclusion}

This work brings to light the findings and advances that can be achieved by integrating complex computational and mathematical methods into architectural design practices. In doing so, it opens up a range of new possibilities and tools for architectural design, but also presents new implications and challenges that need to be addressed. The following section proposes answers to the four key research questions that arise from the interplay between architecture, graph theory, topology, machine learning and synthetic dataset creation. These questions, previously announced in section \ref{sec:research-questions}, explore the potential benefits and challenges of these integrations, ultimately aiming to provide a holistic understanding of the opportunities and complexities they bring to the field of architecture. The ensuing discussion is intended to provide valuable insights into these research topics and to encourage further exploration and innovation in these areas.

\subsection{Graph and Topology in Architecture}\label{sec:graph-and-topology-in-architecture-conclusion}

The integration of \glspl{knowledge graph} and topological methods in architectural practice holds immense potential\footcite{alymani2023classifying}. Graphs provide a powerful tool for representing relational information between architectural entities, while topology can be used to understand and manipulate the spatial as well as non-spatial relationships between these entities. The meaningful application of these tools in project design can allow architects to derive insights from abstract relationships and potentially influence their design decisions in novel ways. However, such integration also presents challenges. Architects may need to acquire new skills and familiarise themselves with complex mathematical concepts, and a thoughtful approach to translating abstract relationships into concrete design decisions is required.

Successful integration into the everyday design process would bring significant programmatic, organisational and conceptual benefits to the architect, adding an essential tool to the conventional creative process. The ability to graphically represent abstract information, such as relational or hierarchical relationships between architectural elements, provides the basis for quantifying and analysing previously intangible design intentions.

\subsection{Feedback in Early Design Stages}\label{sec:feedback-in-early-design-stages-conclusion}

Feedback in the early stages of design can be invaluable in enhancing creativity and coherence between initial concepts and more detailed elaborations in later stages\footcite{paterson2013real}. By receiving indicative feedback early on, architects can take informed design decisions and ensure that their initial designs are well aligned with the requirements of later project stages. However, integrating such feedback can be challenging due to the ambiguous and exploratory nature of early design stages. Machine learning models can be leveraged to provide immediate, data-driven feedback to architects, helping them to iteratively improve their designs and optimise the development process.

In this work, the integration of energy performance feedback was explored using schematic apartment floor plans and their respective window positions. In a concrete application, this would allow the designer to receive immediate feedback on the energy profile of the drawn architectural object. It would also allow a comparative study of design performance by simply moving individual \glspl{aperture} or changing the dimensions and topology of the volumes without the need for complex simulations. This method of instant design feedback can be extended to any simulation parameter, making difficult to understand characteristics of the design object clear and assessable.

\begin{figure}
\centering
\includegraphics[width=0.95\textwidth]{assets/contribution/conclusion/design_for_manufacturing_principle.pdf}
\caption{Design for Manufacturing Principle}
\label{fig:design_for_manufacturing_principle}
\end{figure}

As illustrated in the \textit{design for manufacturing principle} (figure \ref{fig:design_for_manufacturing_principle}), it becomes increasingly costly to change the design as time progresses during the design process, while the impact of the change steadily decreases. This shows that the best time to make a design change is as early as possible in the project. Since simulation and other feedback operations in the traditional architecture process occur mostly in the later stages of the project, such changes are only possible on a small scale or in a very costly framework\footcite{as2018artificial}. This clearly demonstrates the essential need for early design feedback operations such as those demonstrated in this work.

\subsection{Machine Learning in Architecture}\label{sec:machine-learning-in-architecture-conclusion}

Machine learning models can play an important role in architectural design\footcite{kiavarz2021room}. In both practice and academic research, the application of deep learning methods can help to abstract geometric or topological information from complex architectural data in order to provide detailed insights. Trained models can therefore serve as valuable tools for design feedback iteration, transforming data into actionable recommendations for design improvement\footcite{paterson2013real}. In particular, the present study found that \acrlong{dgcnn}\textit{s}, which accept annotated graphs as input, are well suited to this task. However, a significant degree of technical skill and understanding of these models is required to adapt the training process in order to use them effectively in the architectural context. In this regard, important challenges related to bias in training data and interpretability of machine learning models need to be carefully considered and addressed.

The abstraction of \acrshort{bim} models to topological \glspl{knowledge graph} proved to be an advantageous method for dataset creation and the subsequent training process of graph machine learning algorithms. By a sophisticated selection of the node and graph labels of the respective datasets, promising results can be achieved in the architectural application.

\subsection{Synthetic Architecture Datasets}\label{sec:synthetic-architecture-datasets-conclusion}

The synthetic creation of architectural graph datasets is an innovative approach that can abstract complex relational information. Automatic floor plan generation can provide a rich source of diverse and adaptable design data\footcite{carta2021self}, providing a valuable resource for machine learning models. This approach can help mitigate \gls{origin bias} by providing a broader and more varied representation of architectural designs than a dataset based on the analysis of a limited number of architecture plans or styles. However, the automatic generation of such datasets also presents challenges. Ensuring the quality, realism and variability of the generated floor plans requires careful tuning of the generation algorithm. Evaluating the creative diversity and adaptability offered by synthetic datasets also poses methodological challenges, requiring metrics that capture both the quantitative and qualitative characteristics of architectural designs.

The dataset generation pipeline developed in this thesis provides an extensible framework for dataset synthesis. The technical decomposition into architectural control rules and extensible generation algorithms is thus ideally suited for adaptation to the specific requirements of the graph datasets to be generated.

\section{Learned lessons}\label{sec:learned-lessons}

Through the development of an end-to-end framework for the synthetic generation of a complete graph dataset and the use of machine learning algorithms to predict the energy performance of buildings based on their graph representations, this thesis has uncovered several important lessons for the field of \acrlong{cad}. These lessons have the potential to impact architectural research and design practice in terms of topological analysis methods through the application of graph machine learning models. In this section, these learned lessons will be explored in more detail, including the accuracy of the machine learning models, the importance of graphical information and retrieval, and the potential of \acrshort{dgcnn}\textit{s} in architectural design optimisation. These learned lessons provide valuable insights for researchers, architects and designers looking to integrate topological and graph-theoretic analysis methods into their research or design processes.

\begin{itemize}

\item By applying topological and graphical analysis methods to architecture, mathematical concepts can be used to automatically explore and evaluate possible design proposals, providing the designer with a useful tool in the creative process.

\item Graph machine learning algorithms can achieve good accuracy for both classification and regression tasks when predicting the energy performance of buildings based on their graph representations. This demonstrates the potential of using graph machine learning algorithms as a tool for optimising building designs and improving their energy efficiency. By accurately predicting the energy performance of buildings, it becomes possible to make more informed decisions and to design more efficient and sustainable building layouts.

\item The comparable accuracy of the \acrlong{dgcnn} and \acrlong{mlp} neural network developed in this work indicates that the role of the topological relationships between individual spaces and their respective \glspl{aperture} do not have a significant impact on the energy performance of individual apartments.

\item However, the relation between the annotated graph information, specifically node labels, and energy performance has been shown to be of critical importance. By carefully selecting and defining node labels in the graph representations of building designs, machine learning algorithms can predict their energy performance with high accuracy. This highlights the importance of selecting meaningful and informative node labels when generating graph datasets for architectural design.

\item The ability to retrieve the graphical representation of the \acrshort{bim} model throughout all phases of the project allows architects and designers to continuously verify and optimise their building designs based on different graph analysis methods. This highlights the significance of integrating graph theory tools into the traditional \acrshort{bim} workflow to optimise the design and organisation of building topologies.

\end{itemize}

\section{Future Perspective}\label{sec:future-perspective}

Looking forward, this thesis has identified several important open issues and implications for the future of the use of graph representations and machine learning algorithms in architectural design. Addressing these open issues and considering their implications will be crucial for architectural research in this area in order to fully exploit the potential of such tools in order to produce more optimised and efficient building designs.

An important open issue is the identification of other values beyond energy performance that can be used as labels for graphs and predictions. These could include other building performance parameters such as light simulation, fire simulation, evacuation simulation or spatial syntax evaluation. In addition, it will be important to expand the dataset to include more features, such as different locations, materials, heights and contexts, to fully explore the potential of graph machine learning algorithms in the architectural design context.

Further research into model optimisation is another important open question. While this work has demonstrated the use of \acrlong{dgcnn}, further improvements in the structure of the model could be made. Another open issue is the implementation of a continuous back and forth between the graph representation and the \acrshort{bim} model, in particular using the \acrshort{ifc} file format\footcite{isaac2013analyzing}, which is hierarchical and can therefore be queried in a structured manner.

An important implication of this research is that machine learning algorithms should not be used primarily as recommendation tools, but rather as useful feedback indicators for architects and designers. In addition, the use of these tools should not be limited to the architectural side, but should be extended to the engineering side of building design. Furthermore, it is important to note that the synthetic dataset used in this research should be adapted or enhanced for specific use cases to ensure its effectiveness.

To address these open issues and implications, several recommendations can be made. Adapting the synthetic dataset generation algorithms to represent more real-world variety in terms of geometry, adding variations for composite materials such as walls and windows, and simulating variations for different locations are important steps. Exploring the benefits of the \acrshort{ifc} file format for graph retrieval\footcite{nahar2017applying} is also recommended, as it could provide a simple and potentially widely adaptable method for integrating topological graph analysis into the conventional design workflow.

\section{Added Value for Architects}\label{sec:added-value-for-architects}

The development and application of a comprehensive framework for the synthetic generation of a complete graph dataset and the use of machine learning algorithms to predict the simulated performance of buildings based on their graph representations provides significant added value to architects and designers. By integrating machine learning algorithms and graph representations into their design processes, architects can create more efficient and optimised building designs. The use of machine learning algorithms to predict the energy performance of buildings, as explored in this work, can help architects and designers in making more educated choices and optimise their building designs to reduce energy consumption and increase energy efficiency. This can lead to significant cost savings over the lifecycle of the building, as well as improved environmental performance.

In addition, the use of graph representations can provide architects and designers with a powerful tool for abstracting and visualising the complex relationships and dependencies within building designs. In fact, graph representations can be used to visualise and analyse building designs at different scales and to identify patterns and relationships that are not immediately apparent in conventional design workflows. This can help architects to better understand the implications of their design decisions and to make choices about the materials, systems and technologies used in their building designs.

The use of machine learning algorithms and graph representations can also provide architects with a valuable tool for early design optimisation. By using predictive models to analyse different design options, designers can quickly and efficiently evaluate the potential performance of different three-dimensional layout proposals and modifications. This can help reduce the time and cost associated with traditional feedback methods and ensure that the final design is optimised in terms of the chosen performance parameter.

\section{Concluding Remarks}\label{sec:concluding-remarks}

This thesis has demonstrated the potential of applying topological graphs in the context of architectural design. Throughout the process, from conceptual clarifications to the synthetic generation of a graph dataset to the training of machine learning models, essential foundations have been laid for research into the application of graphical-topological analysis methods to architectural objects. Several implications, open questions, perspectives and benefits have been identified in the course of this research.

Although a concrete implementation regarding the energy consumption of individual apartments was developed in the contribution section, this work is primarily a proof of concept to lay the foundations for further research in this field. In this sense, the individual stages of the experiment can now be further refined by applying the concepts explained, and subsequently compared and evaluated using the established evaluation metrics. This can be done both at the level of data generation, by varying the geometric methods, simulation strategies or information annotation, and at the level of model training, by modifying and combining different \acrlong{gnns}.

The potential of graph theory and topology to represent abstract relational information in architectural designs has been established, while at the same time highlighting the need for their meaningful interpretation in the context of project design. While these methods open up new ways of conceptualising and analysing architectural design\footcites{boguslawski2016two}{nauata2020house}{wang2021room}, they also pose the challenge of deciphering the abstract representations and making them practically relevant.

The early integration of indicative simulation variables in the design process is a way to promote creativity and coherence in architectural projects. However, it requires the development of effective methods to manage and optimise this process, a task that has been explored and discussed in this thesis.

Machine learning, a field that has seen tremendous growth and advancement in recent years, has shown immense potential in the field of architecture. As has been explored, machine learning models can not only provide nuanced feedback in design iterations, but can also effectively process complex inputs such as annotated graphs. The exploration of \acrshort{dgcnn} as a novel model for graph-based machine learning has presented an innovative approach to architectural design. However, the task of selecting the right model, fine-tuning the hyperparameters, and interpreting the outputs remains a complex and challenging process.

While the creation of synthetic datasets offers a promising solution to address \gls{origin bias} and increase creative diversity in architectural design, it comes with its own challenges. From ensuring the validity of the generated data to dealing with the complexities of automatic floor plan generation, many facets of this promising yet challenging field have been traversed.

The interdisciplinary approach of this work, between mathematical and computer science methods and those of conceptual architecture, aims to enrich the creative design process with valuable tools, thus bridging the gap between the individual disciplines. Since effective progress would hardly be possible without collaboration and public availability of research and results, an essential ambition of this work is the publication of all code sources as well as data and information (section \ref{sec:summary-of-contribution}).

\printbibheading[heading=bibintoc, title={Bibliography}]
\printbibliography[heading=none]

\newpage
\AddToHookNext{shipout/background}{\put (-8pt,-\paperheight-4pt){
\begin{tikzpicture}\node(a){\includegraphics[scale=1]{assets/cover/appendix_cover_1.pdf}};
\node at(a.center)[draw, fill=white,line width=1.2pt,circle, minimum height=200pt, yshift=80pt]{};
\end{tikzpicture}}}

\part{\appendixname}\label{part:appendix}

\newpage \ \thispagestyle{empty}
\AddToHookNext{shipout/background}{\put (0,-\paperheight){\includegraphics[scale=1]{assets/cover/appendix_cover_2.pdf}}}
\newpage\clearpage

\appendix

\chapter{Additional Content}\label{chap:additional-content}

\section{Further Readings}\label{sec:further-readings}

The state of the art (chapter \ref{chap:state-of-the-art}) listed the main literature that provided fundamental information for the development of the experiments described in this study. However, the preliminary resource analysis revealed a much larger number of publications, research and experiments on the topics covered. For reasons of space and structure, those that did not contribute significantly to the development of the experiment, but still contained valuable and relevant information, were collected and summarised in the further reading section.

\subsection*{Graph Theory in Architecture}\label{subsec:graph-theory-in-architecture-fr}

\paragraph{\cite{west2001introduction}} is an in-depth study of graph theory with additional emphasis on the detailed explanation of essential graph-theoretic algorithms and special cases such as \textit{perfect graphs}, \textit{matroids}, \textit{random graphs} and \textit{eigenvalues}.

\paragraph{\cite{napong2004graph}} documents the creation of graphical networks to analyse the geometric properties of architectural or urban structures. The \textit{minimum path graph} is computed based on the distance between each node to provide designers with insight into the geometric potential of the object through analysis of optimal node \gls{centrality} and \textit{edge passage capacity}.

\paragraph{\cite{vandromme2009interactive}} demonstrates the calculation of adjacency graphs based on apartment and studio layouts using different methods to evaluate the subgraphs generated by \textit{Bayesian causal methods}. It synthesises a set of optimal floor plan layout graph variants. This work demonstrates the potential of adjacency graphs in the context of creative design practices at a conceptual level.

\paragraph{\cite{lakshmi2017graph}} presents different ways of applying graph-based analysis approaches at both architectural and urban levels. Concepts such as \textit{pseudo-graphs}, \textit{disconnected graphs}, \textit{degree and eccentricity based graphs}, \textit{semi-graphs} and \textit{cluster analysis} are discussed.

\paragraph{\cite{nahar2017applying}} describes and develops a potential workflow from a \acrshort{ifc} \acrshort{bim} model to two informative and distinct graph networks. The \textit{meta-graph} aims to describe the relational information of the model, while the \textit{object-graph} captures the physical relationships. The methodology described aims to manage, visualise and analyse the information within the \acrshort{bim} model.

\paragraph{\cite{abualdenien2021pbg}} focuses on the application of so-called \textit{parametric building graphs}, which capture constructive \acrshort{aec} detail patterns and automatically integrate them into new projects through \textit{graph rewriting systems}, provided a pattern match is given. The application of graph theory in this work is thus at a physically constructive level, involving detailed investigation, where the nodes and edges of the graph represent the respective elements and connections of the detail patterns.

\subsection*{Topology and Space Syntax}\label{subsec:topology-in-architecture-fr}

\paragraph{\cite{alexander1977pattern}} is a fundamental book of architectural theory which identifies a multitude of so-called patterns, each of which describes a specific problem and its solution. Together, these patterns form a \textit{pattern language} that encompasses not only constructive but also social, psychological, political and urban aspects.

\paragraph{\cite{kantor2005tale}} focuses on the mathematical concept of topology and makes it tangible through examples. It clarifies essential features such as the topological understanding of space as opposed to the geometric one.

\paragraph{\cite{varoudis2014beyond}} takes a closer look at the \acrfull{vga} method of space syntax. It then develops a method for extending \acrshort{vga} to allow the integration of three-dimensional information into the analysis. This allows for a comprehensive understanding of spatial configuration in two and three dimensions.

\paragraph{\cite{lee2017measuring}} explores the relationship between spatial configurations and cultural and social backgrounds using aged care facilities as an example. Several facilities from different countries and cultures are compared using \gls{isovist} and \acrlong{vga} to draw conclusions about their differences and influences.

\paragraph{\cite{lojanica12018topological}} looks at contemporary architecture through the lens of mathematical topology. Concepts of continuous deformation of geometric forms, such as \textit{deformability}, \textit{openness} and \textit{continuity}, are illustrated with examples and their significance for architectural design is discussed.

\paragraph{\cite{wardhana2019spatial}} explores the benefits of non-\gls{manifold} topologies for spatial reasoning and presents their implementation in the '\textit{Topologic}' library. An example of automatic path finding within a topology model derived from a \acrshort{bim} model and its \textit{dual graphs} is documented.

\paragraph{\cite{bielski2020topological}} studies the topology of different school buildings by analysing adjacency, accessibility, depth and flow. This information is transformed into graph objects, together with semantic data about the buildings and spatial functions, and analysed using machine learning methods to identify specific patterns and draw architectural conclusions.

\subsection*{Decisionmaking and Feedback-Tools}\label{subsec:decisionmaking-and-feedback-tools-fr}

\paragraph{\cite{bao2013generating}} addresses the problem of defining a good building layout, which presents a significant challenge in the field of \acrshort{cad}. This task is achieved by defining specific evaluation metrics that allow users to make design decisions within the \textit{local shape space} based on a \textit{portal graph}.

\paragraph{\cite{das2016space}} explores the automated generation of design variants based on architectural requirements, similar to the previously mentioned publications. Different layouts are calculated using an evolutionary algorithm-based floor plan generator, taking into account certain user-defined constraints such as site outline, number of floors, total area and adjacencies.

\paragraph{\cite{thurow2016assisting}} creates a searchable architectural database using graph-based \textit{semantic building fingerprints} to enable designers to explore multiple similar typologies in the early stages of the design process.

\paragraph{\cite{nagy2017project}} includes the automated generation of layout variants based on user input. What makes it different, however, is the evaluation matrix developed, which evaluates a variety of architectural characteristics of the generated plans. A multi-objective genetic algorithm is then used to filter out the best optimised variant.

\paragraph{\cite{eisenstadt2019generation}} This publication documents a methodology for generating numerous possible design variations that could represent the current design using graphically programmable information and a \textit{generative adversarial network}. The motivation for this research was the common need for extensive formal research during the initial design phases and the time intensity that this task brings.

\paragraph{\cite{shekhawat2020gplan}} explains the developed software '\textit{GPLAN}', which is able to generate a series of rectangular floor plans according to the chosen parameters by specifying a desired programmatic adjacency graph. In addition, the research presented allows the generation of orthogonal floor plans, which allows for greater diversity. The second part of the paper discusses another method for the automatic dimensioning of drawn floor plans.

\paragraph{\cite{son2021framework}} is primarily concerned with the generation and quantification evaluation of multivariate design data from floor plans. In this context, an automated generation pipeline for the application of the evaluation method was also developed. Key assessment parameters include the number and function of individual rooms, perimeter, room shape, adjacency and connectivity. However, other elements such as walls and doors and their respective positions and dimensions are missing from the evaluation method.

\subsection*{Optimisation in Early Design-Stages}\label{subsec:optimisation-in-early-design-stages-fr}

\paragraph{\cite{jabi2013potential}} explores the beneficial application of evolutionary algorithms in conjunction with shape-packing algorithms in the design process. These automatic optimisation methods in two-dimensional space have proven their effectiveness in examples such as shading facade patterns and urban residential layouts. The key benefits of this method are the automation and optimisation of multiple design parameters that influence each other.

\paragraph{\cite{boon2015optimizing}} experiments with the application of evolutionary parametric optimisation methods to improve architectural layouts by defining desired parameters using a visual scripting language and genetic solver extensions. However, the chosen fitness criteria still pose a problem due to the lack of a uniform evaluation syntax.

\paragraph{\cite{grzesiak2021evolutionary}} shows a novel approach to define optimal solutions. In fact, fitness criteria can take different forms in architectural optimisation applications. In this example, the genotypes represent two-dimensional vectors analogous to wall junctions in geometric space. Thus, within a defined framework, the placement of walls corresponding to desired room sizes and total area can be automatically adjusted to achieve the most optimal layout.

\subsection*{Performance-Based Design}\label{subsec:performance-based-design-fr}

\paragraph{\cite{ibrahim2011computing}} conducts an experiment on several architectural pavilions and their topological shapes to optimise the use of space. Graphical parameters, such as the topological relationship of each space, are defined as an optimisation function to generate an adapted morphology. The formal freedom given to the structure in this experiment is interesting; however, it makes the developed method less applicable in the specific domain of residential architecture.

\paragraph{\cite{aksin2021use}} shows an example of performance optimisation through the definition of multiple evaluation metrics. The application of conventional genetic algorithms has attempted to optimise daylighting and thermal performance while minimising energy consumption. However, the continuous simulation of individual building morphologies proves to be resource and time intensive, making it unsuitable as a supporting tool in the initial design phase.

\subsection*{Impact of Machine Learning}\label{subsec:impact-of-machine-learning-fr}

\paragraph{\cite{belem2019impact}} examines the role of machine learning in its respective application areas and then explores its adaptation in the \acrshort{aec} industry. Possible applications in different architectural areas such as conceptualisation, algorithmisation, modelling and optimisation tasks are discussed and their significance for the future of the industry is explained and evaluated.

\paragraph{\cite{pena2021artificial}} represents a collection of projects and applications of artificial intelligence in architecture, focusing on its beneficial use in the experimental design phase. Of interest is the emphasis on the creative capacities of trained models and computer science algorithms, allowing the discovery of innovative and original forms.

\subsection*{Design and Optimisation Applications}\label{subsec:design-and-optimisation-applications-fr}

\paragraph{\cite{harding2011associative}} explores the organisation of an exhibition space on a two-dimensional level using \textit{self-organising maps} and a \textit{growing neural network}. The neural network is used to optimally distribute changing input representing exhibiting individuals within the architectural space. Experimental approaches to three-dimensional spatial organisation using Voronoi diagram algorithms are also included.

\paragraph{\cite{newton2019deep}} investigates the use of \acrshort{gans} for the synthesis of architectural floor plans. Of note is the specific learning of certain architectural styles, such as that of the architect Le Corbusier, and the small size of the dataset, which is addressed by noise augmentation methods. However, this involves the generation of pixel-based plans, which have their own limitations.

\paragraph{\cite{sebestyen2020machine}} presents a smaller-scale experiment using neural networks to predict sunlight hours and radiation levels based on the formal aspects of a designed building facade. The entire pipeline has been implemented using a visual scripting language, providing a simplified understanding of the framework for non-expert designers.

\paragraph{\cite{eisenstadt2020student}} delves through a compilation of work into another area of intelligent spatial configuration in the early stages of architectural design. It presents various extensions to the existing methodology, focusing on concepts such as \textit{explicable AI}, \textit{game theory}, \textit{natural language processing and generation}. The common thread of these research efforts is the interaction with the designer and the usability of the developed model.

\paragraph{\cite{alammar2021predicting}} documents the training of two different machine learning methods, \textit{\acrfull{anns}} and \glspl{decision tree}, on a synthetic parametrically generated dataset for solar radiation of office buildings. The labelled data from the environmental simulation tool suite \textit{Ladybug} served as the training base, allowing a comparison of the accuracy of the two trained machine learning models. The \gls{decision tree} classification model showed higher accuracy, indicating the categorical nature of the data.

\paragraph{\cite{zidong2021topological}} introduces a generative design method that abstracts complex topological data into spatial layouts, enabling the organisation and structuring of spatial networks according to user requirements. The research explores the application of \acrlong{rnns} in the generative design process through experimentation.

\subsection*{Graph Machine Learning in Architecture}\label{subsec:graph-machine-learning-in-architecture-fr}

\paragraph{\cite{as2018artificial}} describes the process of encoding different buildings as \glspl{knowledge graph}, and using graph machine learning methods to identify specific patterns and subgraphs. These learned structures and distinctive building blocks of the designs were then used as input to modified \acrshort{gans}. Using graph-based neural networks, the tool generated high-quality organisational graph objects based on desired criteria such as \textit{livability} and \textit{sleepability}.

\paragraph{\cite{hu2020graph2plan}} analyses the generation of new floor plans with optimised programs based on user input about the desired layout graphs and building boundaries. \textit{Graph2plan}, a trained graph neural network, has learned the adjacency information of the graphs in relation to their outlines. This tool provides suggestions for the internal functional structure during the design process.

\paragraph{\cite{nauata2020house}, \citetitle{nauata2021house}} document the use of actual architectural floor plans and their generated layout graphs describing room adjacency and function to create a GAN capable of generating plausible room configurations based on new bubble diagrams. The machine learning model, \textit{houseGAN}, was later extended to handle non-rectangular room shapes, doors, entrances and functional graphs instead of adjacency graphs using a new vector-based dataset.

\paragraph{\cite{sanchez2021gentle}} aims to illustrate the basic concepts and main application areas of deep learning methods with graphical data through visual examples. It presents essential concepts related to different graphical information and the different challenges related to graph components in machine learning. Finally, it visualises and explains terminology such as \gls{pooling}, \textit{message passing}, \textit{edge and global representations}, \gls{sampling} and \textit{batching}.

\paragraph{\cite{velivckovic2023everything}} emphasises the importance of graph structures in our natural environment in terms of physical and biological phenomena, as well as cultural and social elements such as language, text, information and social networks. The authors also explain the basic principles of \acrlong{gnns} and their application in various classification and regression tasks.

\paragraph{\cite{ali2023architectural}} describes and documents the creation of a pipeline for generating graph datasets, starting with pixel-based actual architectural floor plans. Using spatial recognition methods similar to \cite{kalervo2019cubicasa5k}, the plans were vectorised to extract topological graphs based on the \textit{robust attributed adjacency graph extraction method} described in \cite{chen2022ro}.

\subsection*{Parametric Design and Algorithms}\label{subsec:parametric-design-and-algorithms-fr}

\paragraph{\cite{coates2005generating}} examines the use of Voronoi diagrams and Delaunay triangulations in the context of automatic space partitioning. The two methods offer a number of advantages due to their flexibility and controllability by seed points, which are evaluated in terms of spatial configuration generation. Topological concepts such as \textit{pathfinding} are also investigated.

\paragraph{\cite{chatzikonstantinou20143}} discusses and explains the \textit{rectangular Voronoi subdivision} method in an architectural context. A disadvantage of regular Voronoi subdivision is that the resulting regions always have an irregular polygonal shape. With the developed algorithm, the resulting regions retain rectangular shapes, making it more suitable for traditional architectural applications, but it also has drawbacks such as gaps between individual spatial elements.

\paragraph{\cite{aguiar2017algorithmic}} presents an algorithm that allows the generation of a twin model in parallel with the conventional modelling process. The twin model has a simplified structure that is well suited for analytical purposes. The parametric algorithm is implemented in a visual scripting language that allows for continuous visualisation of the process.

\paragraph{\cite{postle2019pattern}} introduces a software tool capable of automatically generating three-dimensional \acrshort{bim} models. The tool's functionality is based on the optimisation of patterns defined by \citeauthor{alexander1977pattern} and their solutions by evolutionary algorithms. It combines the language of architectural patterns with design patterns from computer science to generate optimised architectural objects.

\paragraph{\cite{caetano2020computational}} discusses the state of the art of computational design in the architectural context through a meta-analysis. The study explains different terminologies and their use and clarifies key concepts and synonyms such as parametric, generative, algorithmic, adaptive and evolutionary.

\subsection*{Automatic Floor Plan Generation}\label{subsec:automatic-floor-plan-generation-fr}

\paragraph{\cite{lopes2010constrained}} provides a method for automated space planning through a relatively simple algorithm that simulates and generates room allocations based on user-defined functional constraints. The growing structures resemble a combination of Voronoi diagram computation with a grid-based approach, but not exactly slicing or aggregation methods.

\paragraph{\cite{shekhawat2014algorithm}} presents a method for space allocation that is not significantly different from those mentioned above. However, it introduces an element called '\textit{extra space}', which can take various forms such as corridors, terraces, balconies or storage space.

\paragraph{\cite{calixto2015literature}} categorises, analyses and compares space planning algorithms supported by evolutionary algorithms developed since the 1990s. It delves into different approaches to space partitioning, such as \textit{half plans}, K-3D trees, shape grammars, \textit{blocking}, assignment and slicing trees.

\paragraph{\cite{guo2017evolutionary}} studies the combination of multi-agent topology search systems and evolutionary solution methods. The algorithm belongs to the family of agent-based aggregation methods that use topological analysis metrics as evaluation strategies.

\paragraph{\cite{nisztuk2019hybrid}, \citetitle{nisztuk2019tool}} compares greedy-based algorithms and \textit{hybrid evolutionary algorithms} in their ability to automate space allocation and planning, and presents an implementation of a resulting tool. User-defined optimisation parameters include room area, location and topological connectivity.

\paragraph{\cite{carta2021self}} presents and compares a collection of \acrshort{anns} and \acrshort{gans} applications in automated space planning methodologies. The meta-analysis raises questions about the acquisition and quality of architectural datasets, as these have a significant impact on the performance and influence of trained models.

\subsection*{Architectural Datasets}\label{subsec:architectural-datasets-fr}

\paragraph{\cite{fedorova2021synthetic}} develops a generation pipeline to create a synthetic three-dimensional architectural dataset. While datasets providing two-dimensional floor plan information are available in various forms, geometric deep learning tasks may require volumetric information. During the course of these experiments, individual architectural objects were augmented with additional information such as building components, material textures and building classes.

\paragraph{\cite{eisenstadt2021comparative}, \citetitle{eisenstadt2021exploring}} test, compare and evaluate different tensor-based encoding methods of architectural and topological information in conjunction with deep learning methods. The tensor formats tested include \textit{multilayer maps}, \textit{textual maps} and \textit{\gls{one-hot-encoded} maps}, with the latter method showing the highest accuracy with 98\%.

\section{Potential Application and Usage}\label{sec:potential-application-and-usage}

The scope of this work does not allow for a concrete application of the developed methods due to time constraints. Nevertheless, a presentation of a possible application workflow is helpful for a solid understanding of the usefulness of the models trained in the experimental phase. In the following sections, the possible software interaction is first presented and explained using visual examples. Then technical details of a possible implementation are given, and finally considerations for community-based improvement and learning methods are listed.

\subsection{Workflow Concept}\label{subsec:workflow-concept}

The implementation of the model-based feedback provider should be achieved through visual representations of the predicted values that are as intuitive as possible. Therefore, by using the \acrshort{api} extension of the commercially available architecture software packages, an interaction with the user interface for the visualisation of the results can be ensured within the applied software. Thus, the application of the feedback method does not require any adaptation on the part of the user and the need for a large number of different software packages is avoided.

As shown in figure \ref{fig:mock-up-of-potential-implementation}, a small floating button is displayed in the center of the left half of the screen, continuously indicating the predicted energy consumption in $MJ/m^2$ and the corresponding energy class of the current design. In addition, an intuitive colour code can be used here, which could adopt a green-yellow-red gradient to effectively and conveniently convey good, mediocre or poor results.

\begin{figure}
\centering
\begin{subfigure}{\textwidth}
\centering
\includegraphics[width=\linewidth]{assets/appendix/mockup_1.pdf}
\caption{Feedback for 2D Floor Plan}
\label{fig:feedback-for-2d-floor-plan}
\end{subfigure}
\begin{subfigure}{\textwidth}
\centering
\includegraphics[width=\linewidth]{assets/appendix/mockup_2.pdf}
\caption{Feedback for 3D BIM Model}
\label{fig:feedback-for-3d-bim-model}
\end{subfigure}
\caption{Mock-up of Potential Implementation}
\label{fig:mock-up-of-potential-implementation}
\end{figure}

The fact that the dataset, and thus the input to the trained model, is based on graphical information is extremely valuable at this point, as the level of detail and dimensionality of the developed design does not pose any problem or difference in the prediction process. The only requirement is to extract the topological information from the two dimensional plan or three dimensional building and compute the topological graph of the design based on this data. This can easily be done using the \textit{TopologicPy} library, as long as a uniform \gls{aperture} and space designation is given. In effect, this means that the prediction of simulation values can be performed in the background at all design stages without any effort on the part of the user (figure \ref{fig:immediate-design-feedback}), thus saving considerable cost and time in design practice.

\begin{figure}
\centering
\begin{subfigure}{.49\textwidth}
\centering
\includegraphics[width=\linewidth]{assets/appendix/mockup_3.pdf}
\end{subfigure}
\begin{subfigure}{.49\textwidth}
\centering
\includegraphics[width=\linewidth]{assets/appendix/mockup_4.pdf}
\end{subfigure}
\caption{Immediate Design Feedback}
\label{fig:immediate-design-feedback}
\end{figure}

The key functionality of the developed method becomes apparent as soon as changes are made to the created design. By clicking on the box displaying the results, 'save points' can be created that store the performance of the design at that point in time. This allows detailed comparisons to be made between design variations, whether these are minor changes such as the size or position of a window, or major changes such as the orientation of the building or completely different building typologies. These comparisons are displayed in a pop-up window and visualised by graphs (figure \ref{fig:pop-up-graph-showing-previous-design-variants-and-performance}). By clicking on the individual datapoints, it is easy to go back to a previous design, which can be useful if the previous version was found to be more energy efficient than the current layout.

\begin{figure}
\centering
\includegraphics[width=.7\linewidth]{assets/appendix/mockup_5.pdf}
\caption{Pop-Up Graph Showing Previous Design Variants and Performance}
\label{fig:pop-up-graph-showing-previous-design-variants-and-performance}
\end{figure}

\subsection{Technical Implementation}\label{subsec:technical-implementation}

The presented functionality entails some prerequisites and requirements on a technical level. First of all, the interface between the user and the machine learning model has to be defined, since user-friendliness is an important criterion in the architectural application. On the one hand, one could develop an independent software that builds its interface through libraries such as GTK and uses \gls{open source} kernels such as Open CASCADE as a geometry processor, or the development of an add-on that could be split into a software-independent processing part and a \acrshort{api} for the dominant architecture tools, allowing easy integration into conventional design workflows.

As the proposed machine learning model is relatively small and therefore fast and resource efficient and could be distributed pre-trained, no special hardware requirements are necessary. Furthermore, an essential part of the technical implementation would be the development of a general design language for intuitive feedback visualisation that has uniform semantics across all software. Last but not least, the choice between a centralised server back-end or a decentralised local model interaction would have to be discussed.

\subsection{Crowd Sourcing}\label{subsec:crowd-sourcing}

During the research for the experiments and analyses presented in this paper, subjects around the degree of openness and democratisation of architecture were identified that offered a variety of interesting and pioneering ideas. The concept of \gls{open source} architecture\footcite{postle2019pattern}, both in construction and in the design process, is about making the acquired knowledge of individuals publicly available, in order to offer more inexperienced or non-specialist individuals the opportunity and access to the respective corpus of knowledge. The publication of the plans and rights of several projects by Pritzker prize winner Alejandro Aravena or the \gls{open source} design initiative \textit{OpenStructures}, which makes its designs available online free of rights, has sparked a general interest in the political-social movement in the \acrshort{aec} industry.

Another concept of knowledge sharing in the field of architecture is \textit{urban mining}, in which urban areas and their buildings are understood as a kind of warehouse of building components, and thus a database of available constructive and decorative elements can be created and searched by anyone. This would make it theoretically possible to design an entire project based on a list of available components, thus addressing essential issues of today's society such as component recycling, carbon emissions, life cycle and energy consumption in the production of materials.

In the context of this master's thesis, the crowdsourcing of architectural designs, for example, could lead to a significant increase in the variety, complexity and reality of datasets such as the developed graph dataset, as personal designs are progressively fed in. It would be possible to increase the number and quality of datapoints in the dataset and to improve the labelling of the data through sensor measurements. This type of data collection through continuous sensor-based real-world measurement in an urban context is often referred to as \textit{smart city} or \textit{\acrfull{iot}}.

\section{Open Source Software and Knowledge}\label{sec:open-source-software-and-knowledge}

The term \textit{\acrfull{oss}} refers to the type of software whose source code is publicly available and published under certain permissive licences. Terms such as free software broaden the conceptual framework to include essential notions such as the freedom to use the program without restriction, to modify and redistribute the program as desired, and to publish improvements. In the early days of program development, these concepts were implicitly the norm and contributed significantly to software development. However, the diametrically opposed concept of proprietary software was introduced shortly afterwards for economic reasons and has since become the dominant approach to software development. As a result, various institutions such as the \textit{\acrfull{osi}} or the \textit{GNU Project} were founded to protect, regulate and centralise the values and standards of free software.

\subsection{Benefits of Open Source}\label{subsec:benefits-of-open-source}

\Gls{open source} and free software offer significant benefits that have widely changed research and software development and continue to drive technological progress\footcite{chaillou2022artificial}. They promote public accessibility and inclusion, enabling individuals around the world to benefit from advanced tools. Source code transparency enhances security and encourages community collaboration, leading to rapid innovation. The use of \acrlong{oss} reduces vendor lock-in and ensures longevity. In addition, \gls{open source} file standards such as \acrshort{ifc} and \acrshort{dxf} in the \acrshort{aec} industry enable easy interoperability between different software packages and stakeholders. The use of these concepts is also consistent with the ethical principles of knowledge sharing and collective progress. In practice, however, large software vendors often resist the adoption of \gls{open source} standards, as demonstrated by the public letter\footcite{ali2023architectural} addressed to \textit{Autodesk} and signed by 324 influential \acrshort{aec} industry professionals.

\subsection{AEC Software and Efforts}\label{subsec:aec-software-and-efforts}

In addition to established \acrshort{oss} such as \textit{NumPy}, \textit{SciPy}, \textit{PyTorch}, \textit{Scikit-learn}, \acrshort{dgl} and \textit{Pandas}, a number of libraries and software have been tested and studied in the course of this work. In particular, simulation software corresponding to the main simulation topics (section \ref{sec:simulation}) was collected. Due to their accessibility, \Gls{python}-based or \Gls{python} \acrshort{api} were preferred. In the following subsections, the most important \acrshort{oss} related to the topics of this thesis can be found in a categorised list.

\subsubsection{BIM Tools}\label{subsec:bim-tools}

\begin{itemize}

\item \href{https://github.com/IfcOpenShell/IfcOpenShell/tree/v0.7.0/src/blenderbim}{\textbf{BlenderBim}} is a sophisticated \acrfull{bim} tool that integrates with \textit{Blender} and uses the \href{https://github.com/IfcOpenShell/IfcOpenShell}{\textbf{ifcOpenShell}} library to allow efficient visualisation and modification of \acrfull{ifc} data, providing a complete \acrshort{bim} workflow.

\item \href{https://github.com/FreeCAD/FreeCAD}{\textbf{FreeCAD}} is a versatile parametric 3D modelling engine that provides a \href{https://github.com/yorikvanhavre/BIM_Workbench}{\textbf{BIM Workbench}}. In combination with this add-on, FreeCAD becomes a proper \acrshort{bim} tool, providing architectural design, modelling and analysis capabilities.

\item \href{https://github.com/nortikin/sverchok}{\textbf{Sverchok}} is a visual scripting tool for parametric modelling within \textit{Blender}. While not exclusively a \acrshort{aec} tool, it can be used to create rule-based geometries and automate specific tasks in architectural projects such as performance analysis.

\item \href{https://github.com/wassimj/topologicpy}{\textbf{TopologicPy}} is a library for topological operations on point sets with integrated graph functionalities based on non-\gls{manifold} elements. It can also be integrated into sverchok's visual programming engine and provides useful tools for graph and geometry analysis.

\item \href{https://github.com/brunopostle/homemaker-addon}{\textbf{Homemaker Add-On}} is a \textit{Blender} add-on that simplifies architectural design by converting spatial configurations into \acrshort{ifc} building models through a user-friendly approach that combines evolutionary algorithms with Christopher Alexander's \citetitle{alexander1977pattern}.

\item \href{https://github.com/tpaviot/oce}{\textbf{Open CASCADE}} is a powerful 3D modelling and numerical simulation engine that provides essential components for \acrshort{cad} software.

\end{itemize}

\subsubsection{Open Formats}\label{subsec:open-formats}

\begin{itemize}

\item \href{https://github.com/buildingSMART/IFC4.3.x-development}{\textbf{IFC}} is a file format used to store and exchange \acrshort{bim} data between different software applications. It facilitates interoperability throughout the project development cycle.

\item \href{https://www.autodesk.com/developer-network/platform-technologies/autocad-dxf-archive}{\textbf{DXF}} or Drawing Exchange Format is a file format developed by Autodesk and commonly used to exchange 2D and 3D drawings between different \acrshort{cad} software applications.

\item \href{https://www.gbxml.org/}{\textbf{gbXML}} or Green Building XML is a file format that represents the environmental characteristics of a building. Its primary use is in the simulation and analysis of the energy performance of building designs.

\end{itemize}

\subsubsection{Structural, Thermal and CFD Analysis}\label{subsec:structural-thermal-and-cfd-analysis}

\begin{itemize}

\item \href{https://github.com/Krande/adapy}{\textbf{ADA-Py}} is a \Gls{python} library used in \textit{\acrfull{fea}}. It provides structural analysis tools that allow advanced simulations of the mechanical performance of building designs.

\item \href{http://www.calculix.de/}{\textbf{CalculiX}} is another \acrshort{fea} solver that provides various methods for structural and thermal analysis. It can calculate complex mechanical scenarios and simulate a variety of different material behaviours.

\item \href{https://code-aster.org/V2/spip.php?rubrique2}{\textbf{Code Aster}} is a powerful structural and thermo-mechanical analysis software. Developed and maintained by EDF (Electricité de France), it has found wide application in the \acrshort{aec} industry.

\item \href{https://github.com/firemodels/cfast}{\textbf{CFAST}} or Consolidated Fire and Smoke Transport is a simulation engine for fire propagation and smoke analysis in buildings. It can be used to assess fire safety and verify evacuation systems.

\item \href{https://github.com/ElmerCSC/elmerfem}{\textbf{Elmer}} is a multiphysics simulation software that provides different types of analysis such as thermal, structural or electromagnetic simulations.

\item \href{https://github.com/firemodels/fds}{\textbf{FDS}} or Fire Dynamics Simulator is a \textit{\acrlong{cfd}} software used to analyse fire and smoke spread in buildings. It can help to understand fire behaviour and develop fire safety measures.

\item \href{https://github.com/OpenFOAM/OpenFOAM-dev}{\textbf{OpenFOAM}} is another \acrshort{cfd} software package that provides a wide range of numerical modelling capabilities for fluid flow and heat transfer analysis.

\item \href{https://github.com/OpenSees/OpenSees}{\textbf{OpenSEES}} or Open System for Earthquake Engineering Simulation offers a framework for simulating the seismic behaviour and resistance of structures. It is mainly used in earthquake analysis and research.

\end{itemize}

\subsubsection{Environmental Analysis}\label{subsec:environmental-analysis}

\begin{itemize}

\item \href{https://github.com/ladybug-tools}{\textbf{Ladybug Tools}} is a collection of environmental analysis plug-ins for various 3D modelling software. It allows architects and engineers to perform various environmental simulations such as daylighting, solar radiation and energy analysis by providing multiple access points.

\item \href{https://github.com/GreenDelta/olca-app}{\textbf{OpenLCA}} is a tool for \textit{\acrfull{lca}} that allows users to analyse the environmental impact of construction processes, making it a suitable feedback tool for sustainable design and decision making.

\item \href{https://github.com/DavidVeld/CarboLifeCalc}{\textbf{CarboLifeCalc}} is software that focuses on calculating the carbon footprint of building materials and projects. It helps to quantify the environmental impact of construction practices and enables environmentally conscious design choices.

\item \href{https://github.com/LBNL-ETA/Radiance/tree/master}{\textbf{Radiance}} is a suite of lighting simulation and ray tracing tools. It is most commonly used for daylight analysis and shadow simulation in architectural space.

\item \href{https://github.com/rgsouthall/vi-suite07}{\textbf{Vi-Suite}} is an add-on for \textit{Blender}, originally designed for contextual and performative building analysis. Over time it has expanded to include dynamic functionality such as parametric lighting, shadows and building energy analysis through the integration of Radiance, EnegyPlus and OpenFOAM.

\item \href{https://github.com/architecture-building-systems/CityEnergyAnalyst}{\textbf{CEA}} or Comprehensive Environmental Assessment is a software tool used to assess and subsequently optimise the environmental performance of buildings. It considers multivariate factors such as energy use, carbon emissions and indoor environmental quality.

\item \href{https://github.com/NREL/EnergyPlus}{\textbf{EnergyPlus}} is a widely used energy simulation software from the \textit{\acrfull{nrel}} that models the energy consumption of buildings in combination with \acrshort{hvac} systems. It enables the assessment of building energy performance and provides metrics that can be used to develop energy efficient designs.

\item \href{https://github.com/NREL/OpenStudio}{\textbf{OpenStudio}} is a platform used to perform energy modelling and simulation for buildings. It provides an interface to EnergyPlus and additional tools for advanced building performance analysis.

\item \href{https://github.com/NREL/SAM}{\textbf{SAM}} or System Advisor Model is another toolkit developed by the \acrshort{nrel} for the design of systems integrating renewable energy concepts. Its main application is to assess the economic and environmental impact of renewable energy sources such as wind, solar and others.

\end{itemize}

\subsubsection{Traffic and Pedestrian Analysis}\label{subsec:traffic-and-pedestrian-analysis}

\begin{itemize}

\item \href{https://github.com/gama-platform/gama}{\textbf{GAMA}} is a platform for modelling and simulation of complex systems such as traffic and pedestrian dynamics. It can be used to study the behaviour of agents in different environments, such as urban areas or public buildings.

\item \href{https://github.com/eclipse/sumo}{\textbf{SUMO}} or Simulation of Urban Mobility is a traffic simulation software used to model individual vehicles and pedestrians to analyse traffic flow, congestion and transport.

\item \href{https://github.com/PedestrianDynamics/jupedsim}{\textbf{JuPedSim}} is another pedestrian simulation software designed to study crowd dynamics and pedestrian behaviour. It provides tools to help design public spaces and optimise pedestrian flow within building complexes.

\end{itemize}

\subsubsection{Acoustic Simulation}\label{subsec:acoustic-simulation}

\begin{itemize}

\item \href{https://github.com/Universite-Gustave-Eiffel/I-Simpa}{\textbf{I-Simpa}} is an acoustic simulation software capable of analysing and calculating sound propagation in different media and environments. Its capabilities enable architects to evaluate and optimise the acoustic performance of buildings and spaces.

\item \href{https://github.com/Universite-Gustave-Eiffel/NoiseModelling}{\textbf{NoiseModelling}} is a tool that allows users to create and analyse noise maps and models. It is mainly used to reduce noise pollution in urban areas.

\end{itemize}

\subsubsection{Urban Analysis}\label{subsec:urban-analysis}

\begin{itemize}

\item \href{https://github.com/UDST/urbansim}{\textbf{UrbanSim}} is another \acrlong{oss} with a focus on urban simulation and land use modelling. It can help planners understand urban growth patterns, infrastructure and the impact of urban regulations.

\item \href{https://github.com/urbanopt}{\textbf{URBANopt}} is an urban energy modelling platform that combines energy and urban simulation to analyse the energy performance of buildings and their interactions with the surrounding environment.

\item \href{https://github.com/qgis/QGIS}{\textbf{QGIS}} or Quantum \acrshort{gis} is a \textit{\acrfull{gis}} software that allows users to analyse, visualise and interpret a variety of different spatial data, providing an important tool for urban planning.

\item \href{https://github.com/EL-BID/urbanpy}{\textbf{UrbanPy}} is a \Gls{python} library that facilitates urban data analysis and simulation by providing tools to work with geospatial data and perform various urban analyses and simulations automatically.

\item \href{https://github.com/gboeing/osmnx}{\textbf{OSMnx}} is a \Gls{python} library used to retrieve and analyse OpenStreetMap data. It is capable of generating urban road networks, visualising urban patterns and performing network-based analysis.

\item \href{https://github.com/victorcalixto/mega-polis}{\textbf{Mega-Polis}} is a data-focused urban add-on for \textit{Blender} that provides a range of capabilities for collecting, analysing, generating and visualising urban data. It uses \Gls{python} libraries such as GeoPandas, NetworkX, OpenCV and Shapely to enhance its functionality.

\end{itemize}

\subsubsection{Resources and Datasets}\label{subsec:resources-and-datasets}

\begin{itemize}

\item \href{https://community.osarch.org/}{\textbf{OSArch Community}} is a platform for sharing and discussing \gls{open source} architectural design and \acrshort{bim} tools. It provides resources, forums and channels for the exchange of information in the \acrshort{aec} industry.

\item \href{https://github.com/BrickSchema/brick}{\textbf{Brick}} is a data model for representing buildings and their systems. It helps standardise and access building data for many purposes, such as energy modelling or performance analysis.

\item \href{http://www.materialsdb.org/}{\textbf{MaterialsDB}} is a database of building materials and their properties. It provides information for architectural and engineering simulations, allowing the selection of appropriate materials based on selected performance criteria.

\item \href{https://www.oekobaudat.de/}{\textbf{ÖkobauDat}} is a German database that catalogues ecological building materials and products, making it an important resource for sustainable and environmentally friendly building practices.

\item \href{https://github.com/buds-lab/building-data-genome-project-2}{\textbf{BDG2}} or Building Data Genome 2 dataset includes 3 053 energy meters in 1 636 buildings, providing two years of hourly measurements. The data includes electricity, heating, cooling water, steam and irrigation meters.

\item \href{https://github.com/CubiCasa/CubiCasa5k}{\textbf{CubiCasa5k}} is a dataset of 5 000 floor plans of residential buildings. It could be used as a basis for various machine learning tasks in the field of architectural automation.

\item \href{https://github.com/zzilch/RPLAN-Toolbox}{\textbf{RPLAN}} is an extensive dataset of annotated and cleaned floor plans extracted from actual residential building layouts.

\end{itemize}

\subsection{Application in this Thesis}\label{subsec:application-in-this-thesis}

This work is largely based on \gls{open source} knowledge and software and would certainly not have been possible without it. Furthermore, the data, information, models and results of this work will be made publicly available and openly accessible, downloadable and modifiable on online portals appropriate to the type of data (section \ref{sec:summary-of-contribution}). During the development of this work, a partial ambition was to use exclusively \gls{open source} programs to conduct a feasibility study on the state of the art of \acrshort{oss} in the \acrshort{aec} industry. Despite the primarily academic nature of the work, a complete \acrshort{oss} workflow was developed without much difficulty, thus proving the feasibility of such an approach.

In addition to basic software such as \Gls{python}, Linux, \textit{Blender} and \textit{Pytorch}, programs such as \textit{Sverchok}, \textit{TopologicPy} and \acrshort{dgl} proved particularly helpful in their application. The most important aspect of their implementation, however, was the exchange with the respective developers about the functionality, use and operation of the individual libraries. For example, contact with Professor Wassim Jabi\footcite{jabi2013parametric}, the principal developer of the \textit{TopologicPy} library, proved invaluable. The opportunity to study the source code allowed a deep understanding of the functionality and, after repeated exchanges, it was possible to develop new functions and integrate them into the software source code as so-called pull requests. In this way, functions and methods for dataset balancing, energy simulation optimisations and the implementation of graph regression training and evaluation were contributed.

This shows that an equally important part of the \acrshort{oss} community is the sharing and publication of knowledge and results, in addition to the disclosure of source code. In this context, media such as the \textit{OSArch Community}\footcite{osarch2020} and its associated wiki have proven to be extremely helpful in process development and in the search for inspiration. Publicly available project and code repositories such as \textit{GitHub}, \textit{GitLab} and \textit{kaggle} also provided essential resources, mostly through detailed introductory texts and easy contact with developers and interested contributors. Last but not least, the publication of scientific papers can also be seen as a source of academic collaboration and contribution.

\chapter{Raw Data}\label{chap:raw-data}

The experimental part of this work is a mostly data-based approach, especially the machine learning part. Thus, large amounts of data have been stored, reviewed and modified during the process in human-readable tabular, graphical or geometric visual forms. However, since the purpose is to demonstrate the process rather than to guide the reproduction of the results, the excerpts of this data are found in the appendix rather than in the body of this document. Nevertheless, the form of this information played an essential role in the parametric generation and training process. The samples of data presented in the following paragraphs have been applied in different areas and are broadly divided into geometric, graphical and informative data. The geometric part mainly refers to information that was needed, generated or defined as an initial rule during the geometric generation step. The graphical data represents the storage of the \glspl{knowledge graph} in machine readable form as input to the deep learning section, and the informational data deals with the storage in \acrshort{json} format, node label and \gls{quantile} computation, and graph label computation.

\section{Geometry Data}\label{sec:geometry-data}

\subsection{Input Parameter}\label{subsec:input-parameter}

One of the initial requirements of the geometric generation pipeline to ensure architectural coherence was the definition of maximum and minimum room sizes according to the respective room types. Furthermore, this size definition also determined which individual rooms the different apartments contained in their program, according to their number of rooms. For example, a three-room apartment has only a living room, a bathroom and a bedroom, whereas a ten-room apartment has three utility rooms and four bedrooms, as well as a toilet, a bathroom and a living room.

The tables \ref{tab:minimum-room-sizes-per-room-amount} and \ref{tab:maximum-room-sizes-per-room-amount} show the minimum and maximum room sizes respectively. The amount of rooms is shown per column and the room types are represented by individual rows. The last line shows the value of the sum of all living areas of the respective apartment type, thus describing the maximum and minimum total apartment sizes to be generated per amount of rooms.

\begin{table}
\centering
\begin{tabular}{ ccccccccc }
\textbf{Room amount} & \textbf{3} & \textbf{4} & \textbf{5} & \textbf{6} & \textbf{7} & \textbf{8} & \textbf{9} & \textbf{10} \\
Living & 28 & 26 & 28 & 29 & 28 & 30 & 33 & 36 \\
Bedroom & 8 & 9 & 10 & 13 & 12 & 10 & 12 & 14 \\
Bedroom & & & & & 9 & 8 & 9 & 11 \\
Bedroom & & & & & & 6 & 7 & 8 \\
Bedroom & & & & & & & & 6 \\
Bathroom & 3 & 4 & 4 & 4 & 4 & 4 & 6 & 7 \\
Toilet & & & 1 & 1 & 1 & 1 & 1 & 2 \\
Utility & & 3 & 5 & 3 & 3 & 1 & 3 & 4 \\
Utility & & & & 12 & 8 & 8 & 12 & 12 \\
Utility & & & & & & & 4 & 6 \\
\textbf{total} & 39 & 42 & 48 & 62 & 65 & 68 & 87 & 106 \\
\end{tabular}
\caption{Minimum Room Sizes per Room Amount}
\label{tab:minimum-room-sizes-per-room-amount}
\end{table}

\begin{table}
\centering
\begin{tabular}{ ccccccccc }
\textbf{Room amount} & \textbf{3} & \textbf{4} & \textbf{5} & \textbf{6} & \textbf{7} & \textbf{8} & \textbf{9} & \textbf{10} \\
Living & 32 & 30 & 32 & 33 & 32 & 34 & 37 & 40 \\
Bedroom & 12 & 13 & 14 & 17 & 16 & 14 & 16 & 18 \\
Bedroom & & & & & 13 & 12 & 13 & 15 \\
Bedroom & & & & & & 10 & 11 & 12 \\
Bedroom & & & & & & & & 10 \\
Bathroom & 7 & 8 & 8 & 8 & 8 & 8 & 10 & 11 \\
Toilet & & & 5 & 5 & 5 & 5 & 5 & 6 \\
Utility & & 7 & 9 & 7 & 7 & 5 & 7 & 8 \\
Utility & & & & 16 & 12 & 12 & 16 & 16 \\
Utility & & & & & & & 8 & 10 \\
\textbf{total} & 51 & 58 & 68 & 86 & 93 & 100 & 123 & 146 \\
\end{tabular}
\caption{Maximum Room Sizes per Room Amount}
\label{tab:maximum-room-sizes-per-room-amount}
\end{table}

\subsection{Geometry Storage}\label{subsec:geometry-storage}

Geometric bodies can be stored in a machine-interpretable form in a number of ways. In this work, the use of the geometry kernel \textit{OpenCASCADE}\footcite{slyadnev2017cad} led to the decision in favour of \acrlong{brep}, as this allowed seamless integration with \textit{TopologicPy}. The \acrshort{brep} files are hierarchically structured and are therefore similar to the functionality of the \Gls{python} library used, since elements of higher order such as shells, solids and compounds are composed and described by smaller elements such as vertices, edges and faces.

The text-based file format (listing \ref{lst:brep-file-structure}) defines and stores different elements by \Gls{cartesian coordinates}, which are stored through three-dimensional matrices or single coordinates with respect to the origin. The bodies described in this way are first specified by their position in three-dimensional space and then defined by three-dimensional points according to their respective geometry. The geometric section of the \acrshort{brep} file is therefore made up of the following elements: \textit{2D curves}, \textit{3D curves}, \textit{3D polygons}, \textit{polygons on triangulations}, \textit{surfaces} and \textit{triangulations}. In addition to its seamless integration into the geometry generation pipeline, this file format is also suitable because it provides the geometry description privileged by the \acrshort{ifc} format.

\begin{lstlisting}[language=python, caption={Brep File Structure}, label={lst:brep-file-structure}, captionpos=b]
            Curve2ds 4
            1 0.4 0.88 -1 0 
            1 0.4 -0.88 0 1 
            1 -0.4 0.88 0 -1 
            1 -0.4 -0.88 1 0 
            
            Curves 4
            1 4.7 1 1.6 1.1 0 -1 
            1 4.7 2.8 1.6 0 -1 0 
            1 4.7 1 0.8 0 1 0 
            1 4.7 2.8 0.8 -1.1 0 1 
            
            Surfaces 1
            1 4.7 1.9 1.25 1 0 0 -0 0 1 0 -1 0
            
                TShapes 10
                Ve
                1e-07
                4.7 1 1.6
                0 0
             
                ...
\end{lstlisting}

\subsection{Energy Simulation Parameter}\label{subsec:energy-simulation-parameter}

An essential part of the dataset generation was the implementation of energy performance simulations of each \acrshort{bim} model. As described in section \ref{subsec:energy-performance-simulation} and figure \ref{fig:energy-simulation-process}, a successful energy simulation using the \textit{EnergyPlus} and \textit{Openstudio} software required the definition of several parameters. The main initial values chosen for the experiment are shown in the table \ref{tab:energy-simulation-values}.

The definition of the location (in this case Berlin) is essential, as the latitude and the associated climate difference have a significant influence on the results of the energy balance. In order to generalise the climate, a number of additional datapoints would have to be added to the dataset, differing only in the location of the \acrfull{epw} file. In addition, the \ref{tab:energy-simulation-values} table shows that the materials of the individual apartment walls and ceilings, including the insulation, have a generic composition with concrete or gypsum depending on their function. Again, greater versatility in the training process is possible by extending the dataset and graph labels.

\begin{table}
\centering
\begin{tabular}{ cccccc }
\textbf{Location} & \textbf{Windows} & \textbf{Int. Walls} & \textbf{Ext. Walls} & \textbf{Type} & \textbf{Insulation} \\
Berlin & Double Glazed & Gypsum & Concrete & Midrise Apt. & 12cm \\
\end{tabular}
\caption{Energy Simulation Values}
\label{tab:energy-simulation-values}
\end{table}

\section{Graph data}\label{sec:graph-data}

The human-readable graph objects produced by \acrshort{dgl} are stored in a specific text-based format that allows the generated graphs to be reviewed, modified, or integrated into various data science processes. However, to save space, large graph databases are typically stored as binary files using the standard \Gls{python} library \textit{Pickle}.

The resulting files consist of three separate text files corresponding to the graph components: node, edge and graph. The information shown in the table \ref{tab:graph-data} is an excerpt from the graph-wide text file, which first defines in tabular form a graph \acrlong{id} number per object in the dataset. For performance reasons, the number of nodes contained in the graph is also stored here. This information is already sufficient to describe a complete graph. However, to create a meaningful dataset it is also necessary to store the nodes and edges in their individual text files and to add one or more labels to the graph object. These labels are, in this case, the energy class and the energy consumption value, which allow the machine learning model to learn the relationships between the graph structure and the energy performance.

\begin{table}
\centering
\begin{tabular}{ cccc }
\textbf{Graph ID} & \textbf{Energy Class} & \textbf{Energy Consumption} & \textbf{Node Amount} \\
8 & 3 & 758.59 & 14 \\
\end{tabular}
\caption{Graph Data}
\label{tab:graph-data}
\end{table}

According to the number of nodes, the individual vertices are defined in the node file (table \ref{tab:node-data}) for each graph of the dataset and assigned to the individual graphs with the graph \acrlong{id} number. The nodes are also given an individual \acrlong{id} number, which allows each node of the whole dataset to be referenced precisely and is essential for defining the edges of the graphs. The most important part of the node file, however, is the node label column, where each node can be assigned an arbitrary label, which, together with the graph labels, will be used as input for the machine learning process. In the procedure demonstrated in this thesis, the node labels consist of encoded information about the element type, its size and its orientation. More detailed information about the node label calculation can be found in section \ref{sec:information-data} and table \ref{tab:node-label-calculation}.

\begin{table}
\centering
\begin{tabular}{ rcccccccccccccc }
\textbf{Graph ID} & 8 & 8 & 8 & 8 & 8 & 8 & 8 & 8 & 8 & 8 & 8 & 8 & 8 & 8 \\
\textbf{Node ID} & 0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 & 11 & 12 & 13 \\
\textbf{Label} & 89 & 60 & 65 & 80 & 2 & 78 & 72 & 69 & 83 & 26 & 78 & 10 & 66 & 33 \\
\end{tabular}
\caption{Node Data}
\label{tab:node-data}
\end{table}

Now that the general graphical data and node specific information has been defined, only the edges between the nodes need to be described and stored. This is likewise achieved by a tabular text file (table \ref{tab:edge-data}) that summarises the total number of edges in the dataset. Each individual edge is described by a column of the table and is first assigned to the respective graphs by the graph \acrlong{id} number, as in the case of the nodes. Furthermore, each node is defined by its \acrfull{src} and \acrfull{dst} node points, which are referenced by the node \acrlong{id} numbers. The edge file thus represents a classical edge list notation. In the case of this work, we are dealing with undirected graphs, which means that an edge with (u v) also appears as (v u) in the edge list (table \ref{tab:edge-data-undirected}).

\begin{table}
\centering
\begin{tabular}{ rccccccccccccccccccccc }
\textbf{ID} & 8 & 8 & 8 & 8 & 8 & 8 & 8 & 8 & 8 & 8 & 8 & 8 & 8 & 8 & 8 & 8 & 8 & 8 & 8 & 8 & 8 \\
\textbf{SRC} & 0 & 0 & 0 & 1 & 1 & 1 & 1 & 1 & 1 & 3 & 3 & 5 & 5 & 7 & 7 & 7 & 8 & 8 & 10 & 10 & 12 \\
\textbf{DST} & 3 & 2 & 1 & 7 & 3 & 5 & 8 & 6 & 2 & 8 & 4 & 7 & 6 & 8 & 10 & 12 & 9 & 10 & 11 & 12 & 13 \\
\end{tabular}
\caption{Edge Data}
\label{tab:edge-data}
\end{table}

\begin{table}
\centering
\begin{tabular}{ rccccccccccccccccccccc }
\textbf{ID} & 8 & 8 & 8 & 8 & 8 & 8 & 8 & 8 & 8 & 8 & 8 & 8 & 8 & 8 & 8 & 8 & 8 & 8 & 8 & 8 & 8 \\
\textbf{SRC} & 3 & 2 & 1 & 7 & 3 & 5 & 8 & 6 & 2 & 8 & 4 & 7 & 6 & 8 & 10 & 12 & 9 & 10 & 11 & 12 & 13 \\
\textbf{DST} & 0 & 0 & 0 & 1 & 1 & 1 & 1 & 1 & 1 & 3 & 3 & 5 & 5 & 7 & 7 & 7 & 8 & 8 & 10 & 10 & 12 \\
\end{tabular}
\caption{Edge Data Undirected}
\label{tab:edge-data-undirected}
\end{table}

\section{Information Data}\label{sec:information-data}

\subsection{BIM Model Storage}\label{subsec:bim-model-storage}

The values generated during the information annotation process had to be correlated using specific file formats in order to avoid an unnecessarily complex data generation process. The key-value pair based text format \acrshort{json} was chosen as the main format for data storage. This choice was made not least because the applied geometry and analysis library \textit{TopologicPy} is equally based on \acrshort{json} files for importing and exporting the individual topological geometries.

This allows all the information about the individual \acrshort{bim} elements to be stored in a single file for each apartment. In addition to the \acrshort{json} format, the \acrshort{ifc} file type was also considered, but in the end the simplicity of the \acrshort{json} structure was preferred. The \ref{lst:json-file-structure} listing shows a simplified example file of annotated apartment geometry, consisting of information about the geometry as \acrshort{brep} strings and values at different element levels.

First, the \acrshort{brep} of the apartment and the individual rooms are listed with their respective types, surface sizes and labels. Then the apartment level information is listed, such as number of rooms, room types and total area. Also stored at this level are any values that provide information about the energy balance of the architectural object, such as heating and cooling energy, general energy consumption, energy class, window-wall ratio and window opening per orientation. Although in the end only the \gls{site energy consumption} per square meter and the energy class were used for training, it was important to test the information storage capabilities of the file format for experimental purposes. Finally, under the key: 'faceApertures', the windows of the model are noted with their respective geometry as brep strings, area, orientation and label.

\begin{lstlisting}[language=python,caption={JSON File Structure},captionpos=b,label={lst:json-file-structure},showstringspaces=false]
[
    {
        'geometry': 'brep',
        'cellDictionaries': [
            {
                'dictionary': {
                    'area': 3.11,
                    'element': 'room',
                    'label': 84,
                    'type': 'bathroom'
                }
            },
            {
                'dictionary': {
                    'area': 8.1,
                    'element': 'room',
                    'label': 77,
                    'type': 'bedroom'
                }
            },
            {
                'dictionary': {
                    'area': 28.54,
                    'element': 'room',
                    'label': 56,
                    'type': 'livingroom'
                }
            }
        ],
        'dictionary': {
            'element': 'apartment',
            'room amount': 3,
            'area': 39.75,
            'room types': [
                'bathroom',
                'bedroom',
                'livingroom'
            ],
            'site energy cooling GJ': 0.61,
            'site energy cooling MJ/m2': 19.0,
            'site energy heating GJ': 18.18,
            'site energy heating MJ/m2': 567.51,
            'total exterior wall amount': 4,
            'total site energy consumption GJ': 24.73,
            'total site energy consumption MJ/m2': 771.98,
            'total source energy consumption GJ': 85.16,
            'total source energy consumption MJ/m2': 2658.05,
            'energy class': 3,
            'total wall area m2': 61.13,
            'total window-wall ratio %': 8.52,
            'total window amount': 3,
            'total window opening area m2': 5.21,
            'wall area east m2': 15.28,
            'wall area north m2': 15.28,
            'wall area south m2': 15.28,
            'wall area west m2': 15.28,
            'window/wall ratio east %': 0.0,
            'window/wall ratio north %': 34.09,
            'window/wall ratio south %': 0.0,
            'window/wall ratio west %': 0.0,
            'window opening area east m2': 0.0,
            'window opening area north m2': 5.21,
            'window opening area south m2': 0.0,
            'window opening area west m2': 0.0
        },
        'faceApertures': [
            {
                'geometry': 'brep',
                'dictionary': {
                    'area': 1.35,
                    'element': 'window',
                    'label': 24,
                    'orientation': 'N'
                }
            },
            {
                'geometry': 'brep',
                'dictionary': {
                    'area': 3.35,
                    'element': 'window',
                    'label': 32,
                    'orientation': 'N'
                }
            },
            {
                'geometry': 'brep',
                'dictionary': {
                    'area': 0.51,
                    'element': 'window',
                    'label': 0,
                    'orientation': 'N'
                }
            }
        ]
    }
]
\end{lstlisting}

\subsection{Node Label Calculation}\label{subsec:node-label-calculation}

In order to calculate the node labels, the element types living room, bedroom, toilet, bathroom and window must be divided into relative size classes. These seven classes range from XXS to XXL and describe the size of the item in relation to the total set of sizes. A division into so-called \glspl{quantile} of the distribution was therefore necessary, but could only be carried out after the entirety of the \acrshort{bim} models had been generated. The \gls{quantile} values calculated in this way can be found in the table \ref{tab:room-size-calculation-quantiles} and describe the threshold value that separates two classes.

\begin{table}
\centering
\begin{tabular}{ ccccccc }
& \textbf{XXS | XS} & \textbf{XS | S} & \textbf{S | M} & \textbf{M | L} & \textbf{L | XL} & \textbf{XL | XXL} \\
\textbf{Window} & 0.503 & 0.823 & 1.304 & 2.214 & 3.538 & 5.327 \\
\textbf{Livingroom} & 27.397 & 29.685 & 31.376 & 33.409 & 35.349 & 37.953 \\
\textbf{Bedroom} & 8.137 & 9.255 & 10.261 & 11.322 & 12.491 & 14.218 \\
\textbf{Toilet} & 2.419 & 3.288 & 3.853 & 4.303 & 4.814 & 5.463 \\
\textbf{Bathroom} & 4.759 & 5.863 & 6.662 & 7.367 & 8.121 & 9.147 \\
\textbf{Utility} & 5.171 & 6.186 & 7.007 & 8.206 & 10.361 & 13.114 \\
\end{tabular}
\caption{Room Size Quantile Calculation}
\label{tab:room-size-calculation-quantiles}
\end{table}

The final step in calculating the labels shown in the listing \ref{lst:json-file-structure} was to encode the categories of each element into integer labels ranging from 1 to 91 (figure \ref{tab:node-label-calculation}). This is a simple type of encoding where the three categories: type, size and, in the case of windows, orientation, are listed and then simply labelled by their index in the list. This method allows the individual elements to be categorised and distinguished from each other. Other types of encoding were also considered, such as feature-based encoding, where the described procedure is applied to each feature individually, thus generating two labels for rooms and three for windows. Nevertheless, the first described method was chosen for this experiment. For the node labels, however, it is important to note that the integer labels are \textit{\gls{one-hot-encoded}} anew when the \acrshort{dgl} dataset creation methods are applied.

\begin{table}
\centering
\begin{tabular}{ cccc }
\textbf{Label} & \textbf{Type} & \textbf{Size} & \textbf{Orientation} \\
1 & window & XXS & N \\
2 & window & XXS & NE \\
3 & window & XXS & E \\
4 & window & XXS & SE \\
5 & window & XXS & S \\
6 & window & XXS & SW \\
7 & window & XXS & W \\
8 & window & XXS & NW \\
9 & window & XS & N \\
.. & ...... & ... & ... \\
57 & livingroom & XS & \\
61 & livingroom & XL & \\
64 & utility & XS & \\
68 & utility & XL & \\
71 & toilet & XS & \\
75 & toilet & XL & \\
78 & bedroom & XS & \\
82 & bedroom & XL & \\
85 & bathroom & XS & \\
89 & bathroom & XL & \\
.. & ........ & ... & \\
\end{tabular}
\caption{Node Label Calculation}
\label{tab:node-label-calculation}
\end{table}

\subsection{Energy Class Definition}\label{subsec:energy-class-definition}

Since the classification task involves the prediction of distinct classes, the \gls{site energy consumption} values first had to be divided into energy classes. Similarly to the method described in table \ref{tab:room-size-calculation-quantiles}, all the energy simulations were first performed in order to divide the general distribution (figure \ref{fig:distribution-of-energy-consumption-value}) of the energy values into \glspl{quantile} according to the desired number of classes. Table \ref{tab:energy-class-calculation-quantiles} shows the \gls{quantile} thresholds generated in this way for five energy classes. It is important to note that these classes, unlike the node labels, are hierarchically related. This means that an apartment with an energy class of zero will perform better in terms of energy efficiency than an apartment with a class of one, and so on.

\begin{table}
\centering
\begin{tabular}{ ccccc }
& \textbf{0 | 1} & \textbf{1 | 2} & \textbf{2 | 3} & \textbf{3 | 4} \\
\textbf{Energy Class} & 617.19 & 659.78 & 702.98 & 777.72 \\
\end{tabular}
\caption{Energy Class Quantile Calculation}
\label{tab:energy-class-calculation-quantiles}
\end{table}

\subsection{Graphical to Tabular Data Conversion}\label{subsec:graphical-to-tabular-data-conversion}

In order to perform the comparisons described in section \ref{subsec:comparison} with different machine learning models, the initially purely graphical information had to be converted into conventional tabular form, since \acrshort{dgcnn} is the only one of the models that can be trained on data presented in the form of graphs. To achieve this conversion, first had to be considered what information was contained in the graphical dataset (section \ref{sec:graph-data}). So the categories: Space and number of windows, window area by cardinal direction and previous node labels were identified. As a target variable, according to the prediction task, the energy class or the energy consumption value was used in the same way as in the graphical dataset. Table \ref{tab:graph-derived-data} shows an extract of this tabular dataset for the datapoints: 18, 45, 56, 76, 92, 109, 125 and 141. For spatial reasons, the nodelabel list of each datapoint is shown in the table \ref{tab:graph-derived-data-labels}, but in reality these datapoints are stored in a single database. The resulting table, not least because of its tabular form, contains different information to the graphical dataset, which should be taken into account when evaluating the performance comparison of the different models.

\begin{table}
\centering
\begin{tabular}{ ccccccccc }
& \textbf{18} & \textbf{45} & \textbf{56} & \textbf{76} & \textbf{92} & \textbf{109} & \textbf{125} & \textbf{141} \\
Rooms & 8 & 3 & 9 & 8 & 10 & 7 & 5 & 4 \\
Windows & 8 & 8 & 9 & 8 & 10 & 11 & 9 & 9 \\
Surface & 84.95 & 32.04 & 100.18 & 92.15 & 131.11 & 79.87 & 59.48 & 49.14 \\
Consumption & 704.06 & 878.09 & 667.81 & 666.68 & 634.52 & 791.09 & 818.39 & 845.12 \\
Energy Class & 3 & 4 & 2 & 2 & 1 & 4 & 4 & 4 \\
Window $m^2$ N & 9.06 & 6.57 & 0 & 0 & 0 & 4.46 & 10.36 & 7.46 \\
Window $m^2$ E & 8.34 & 1.31 & 6.62 & 4.86 & 2.48 & 11.27 & 0 & 9.3 \\
Window $m^2$ S & 0 & 0.65 & 6.13 & 7.35 & 14.57 & 10.09 & 11.3 & 1.56 \\
Window $m^2$ W & 4.83 & 2.54 & 11.17 & 8.73 & 10.69 & 5.89 & 1.74 & 1.5 \\
\end{tabular}
\caption{Graph Derived Data}
\label{tab:graph-derived-data}
\end{table}

\begin{table}
\centering
\begin{tabular}{ r l }
\textbf{18 :} & 86, 78, 59, 80, 79, 63, 71, 67, 47, 48, 50, 2, 16, 2, 24, 26 \\
\textbf{45 :} & 84, 77, 56, 14, 32, 6, 32, 12, 30, 0, 26 \\
\textbf{56 :} & 76, 60, 80, 68, 63, 78, 66, 90, 77, 46, 54, 2, 2, 42, 20, 6, 44, 26 \\
\textbf{76 :} & 75, 77, 89, 60, 64, 67, 78, 82, 18, 2, 52, 34, 20, 4, 54, 38 \\
\textbf{92 :} & 76, 78, 79, 62, 81, 82, 68, 68, 88, 65, 52, 12, 44, 14, 46, 10, 36, 10, 46, 18 \\
\textbf{109 :} & 68, 83, 81, 86, 58, 63, 71, 18, 52, 20, 34, 4, 47, 15, 40, 34, 42, 8 \\
\textbf{125 :} & 75, 57, 66, 87, 80, 19, 51, 22, 27, 0, 40, 19, 14, 48 \\
\textbf{141 :} & 64, 86, 79, 57, 8, 42, 4, 42, 4, 24, 20, 30, 40 \\
\end{tabular}
\caption{Graph Derived Data Labels}
\label{tab:graph-derived-data-labels}
\end{table}

\newpage
\vspace*{\fill}
\begin{flushright}
Copyright \textcopyright\\
Raban Ohlhoff\\
r.ohlhoff@gmail.com\\
\href{https://raban-ohlhoff.com}{raban-ohlhoff.com}\\~\\

Faculty of Architecture La Cambre Horta \\
Université Libre de Bruxelles \\
2023\\
\end{flushright}
\thispagestyle{empty}
\clearpage

\clearpage
\newpage \ \thispagestyle{empty}
\AddToHookNext{shipout/background}{\put (0,-\paperheight){\includegraphics[scale=1]{assets/cover/cover_back.pdf}}}
\newpage\clearpage

\end{document}